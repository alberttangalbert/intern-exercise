{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff4ae34",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f9a86",
   "metadata": {},
   "source": [
    "Load raw trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97784b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"data/raw_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e73a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69802af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                 |   0 |\n",
      "|:--------------------------------|----:|\n",
      "| title                           |   0 |\n",
      "| objective                       |   3 |\n",
      "| outcome_details                 | 146 |\n",
      "| phase                           |   0 |\n",
      "| primary_completion_date         |  61 |\n",
      "| primary_endpoints_reported_date | 161 |\n",
      "| prior_concurrent_therapy        | 184 |\n",
      "| start_date                      |  45 |\n",
      "| study_design                    |  16 |\n",
      "| treatment_plan                  |   1 |\n",
      "| record_type                     |   0 |\n",
      "| patients_per_site_per_month     | 119 |\n",
      "| primary_endpoint_json           |   0 |\n",
      "| other_endpoint_json             |   0 |\n",
      "| associated_cro_json             |   0 |\n",
      "| notes_json                      |   0 |\n",
      "| outcomes_json                   |   0 |\n",
      "| patient_dispositions_json       |   0 |\n",
      "| results_json                    |   0 |\n",
      "| study_keywords_json             |   0 |\n",
      "| tags_json                       |   0 |\n",
      "| primary_drugs_tested_json       |   0 |\n",
      "| other_drugs_tested_json         |   0 |\n",
      "| therapeutic_areas_json          |   0 |\n",
      "| bmt_other_drugs_tested_json     |   0 |\n",
      "| bmt_primary_drugs_tested_json   |   0 |\n",
      "| ct_gov_listed_locations_json    |   0 |\n",
      "| ct_gov_mesh_terms_json          |   0 |\n",
      "Shape: (184, 28)\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum().to_markdown())\n",
    "print(\"Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d278e",
   "metadata": {},
   "source": [
    "Generate unique hash per trial since trial id is missing\n",
    "- i.e. \"tid_0e8fa21079f928135dfc6164a15285f8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6897de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating raw_trials_with_hash.csv ...\n",
      "Index(['trial_hash', 'title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n",
      "(184, 29)\n",
      "✅ Saved to cache/raw_trials_with_hash.csv\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_PATH = Path(\"cache/raw_trials_with_hash.csv\")\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# If file already exists → skip generation\n",
    "# ---------------------------------------------------------\n",
    "if OUTPUT_PATH.exists():\n",
    "    print(f\"⚠️ {OUTPUT_PATH} already exists — skipping hash generation.\")\n",
    "else:\n",
    "    print(\"Generating raw_trials_with_hash.csv ...\")\n",
    "\n",
    "    def make_trial_hash(row):\n",
    "        \"\"\"\n",
    "        Deterministic hash for a trial based on stable fields.\n",
    "        You can add/remove fields if needed.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"start_date\": row.get(\"start_date\", \"\"),\n",
    "            \"phase\": row.get(\"phase\", \"\"),\n",
    "        }\n",
    "        raw = json.dumps(payload, sort_keys=True, ensure_ascii=False)\n",
    "        return \"tid_\" + hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # Create trial_hash column\n",
    "    data[\"trial_hash\"] = data.apply(make_trial_hash, axis=1)\n",
    "\n",
    "    # Move trial_hash to first column\n",
    "    cols = [\"trial_hash\"] + [c for c in data.columns if c != \"trial_hash\"]\n",
    "    data = data[cols]\n",
    "\n",
    "    print(data.columns)\n",
    "    print(data.shape)\n",
    "\n",
    "    # Export\n",
    "    data.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"✅ Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14351eb6",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "Using a chatbot, identify all interventions from each trial. For each intervention...\n",
    "- label as the investigational product, active comparator, or placebo\n",
    "- list all of the alternative names\n",
    "- identify the molecular target \n",
    "- identify the mechanism of action\n",
    "- for investigational products\n",
    "    - trial trove drug id\n",
    "    - biomedtracker drug id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb4f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 184 trials from cache/raw_trials_with_hash.csv\n",
      "Progress: processed 50 trials...\n",
      "Progress: processed 100 trials...\n",
      "Progress: processed 150 trials...\n",
      "✅ Trial drug-role mapping complete. processed=184, skipped=0, llm_error=0, parse_error=0\n",
      "Roles directory: cache/trial_drug_roles\n",
      "Log directory:   cache/trial_drug_roles_log\n",
      "Master roles:    cache/trial_drug_roles_master.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV = Path(\"cache/raw_trials_with_hash.csv\")\n",
    "\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"trial_drug_roles\"\n",
    "DRUG_ROLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DRUG_ROLE_LOG_DIR = BASE_DIR / \"trial_drug_roles_log\"\n",
    "DRUG_ROLE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_ROLES_PATH = BASE_DIR / \"trial_drug_roles_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "# Columns to feed into the chatbot\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def build_prompt(trial_payload: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM to:\n",
    "    - Extract drug names\n",
    "    - Canonicalize names by removing company/manufacturer/location qualifiers\n",
    "    - Deduplicate synonymous names\n",
    "    - For each canonical drug, return a dict with:\n",
    "        * role (Investigational Product / Placebo / Active Comparator / Standard of Care)\n",
    "        * alternative_names (list)\n",
    "        * molecular_target\n",
    "        * mechanism\n",
    "        * tt_drug_id (TrialTrove/PharmaProjects drugId as string)\n",
    "        * bmt_drug_id (BioMedTracker bmtDrugId as string)\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and interpretation expert.\n",
    "\n",
    "You are given structured information about a clinical trial, including:\n",
    "- Title and objective\n",
    "- Study design and treatment plan\n",
    "- JSON fields listing drugs tested in the study:\n",
    "  - primary_drugs_tested_json\n",
    "  - other_drugs_tested_json\n",
    "  - bmt_other_drugs_tested_json\n",
    "  - bmt_primary_drugs_tested_json\n",
    "- These JSON fields may also contain metadata such as\n",
    "  drugApprovalStatus (Approved / Unapproved), mechanisms, etc.\n",
    "- In the TrialTrove/PharmaProjects JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"drugId\".\n",
    "- In the BioMedTracker JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"bmtDrugId\".\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Identify all DISTINCT physical drug entities explicitly used in the study.\n",
    "   - Strings in the *_drugs_tested_json fields are drug-name candidates.\n",
    "   - If these fields contain structured JSON, infer names from keys such as\n",
    "     \"name\", \"drug_name\", \"drugName\", \"drugPrimaryName\", \"preferred_name\", \"label\", etc.\n",
    "\n",
    "2. Canonicalize each drug name:\n",
    "   Remove company names, manufacturer qualifiers, geographic qualifiers,\n",
    "   dosage-form qualifiers, or parenthetical descriptors that do NOT change\n",
    "   the name of the underlying drug.\n",
    "   Examples of correct canonicalization:\n",
    "   - \"AlphaBlocker (CompanyX)\" → \"AlphaBlocker\"\n",
    "   - \"Recombinant Growth Factor (rgf)\" → \"Recombinant Growth Factor\"\n",
    "   - \"DrugX citrate (RegionY)\" → \"DrugX citrate\"\n",
    "   - \"BrandName (compound-42, MakerCorp)\" → \"BrandName\"\n",
    "\n",
    "   Keep only the essential drug or brand name as the canonical key.\n",
    "\n",
    "3. Deduplicate synonymous names referring to the SAME drug.\n",
    "   - If multiple variations refer to one physical drug, keep ONE canonical key.\n",
    "   - Prefer the simplest, clean name.\n",
    "   - Collect all other variations in alternative_names.\n",
    "\n",
    "4. For EACH distinct drug, build an object with SIX fields:\n",
    "\n",
    "   - \"role\": one of:\n",
    "       * \"Investigational Product\"\n",
    "       * \"Placebo\"\n",
    "       * \"Active Comparator\"\n",
    "       * \"Standard of Care\"\n",
    "\n",
    "   ROLE ASSIGNMENT GUIDANCE:\n",
    "\n",
    "   A. \"Investigational Product\"\n",
    "      - Use ONLY for the sponsor's proprietary or novel product.\n",
    "      - Clues: unapproved, new mechanism, highlighted in title/objective.\n",
    "      - Do NOT label common chemotherapy or widely used drugs this way.\n",
    "\n",
    "   B. \"Standard of Care\"\n",
    "      - Use for established backbone therapies, such as common chemotherapies\n",
    "        or widely used drugs in the disease area.\n",
    "      - Examples (fictional): DrugX, Chemo-A, Cytotoxin-7, etc.\n",
    "\n",
    "   C. \"Active Comparator\"\n",
    "      - Use when a non-placebo drug is explicitly the control arm.\n",
    "      - Clues: terms like \"versus\", \"comparator\", \"control regimen\".\n",
    "\n",
    "   D. \"Placebo\"\n",
    "      - Use for inert or sham treatments.\n",
    "\n",
    "   SUMMARY:\n",
    "   - Proprietary or novel study drug → \"Investigational Product\".\n",
    "   - Classical or widely used therapy → \"Standard of Care\".\n",
    "   - Control regimen (non-placebo) → \"Active Comparator\".\n",
    "   - Inert control → \"Placebo\".\n",
    "\n",
    "   - \"alternative_names\": list of synonymous or variant names.\n",
    "     Examples:\n",
    "     * ABC-123 → [\"Compound-ABC\", \"ABC123\"]\n",
    "     * BrandX → [\"generic compound name\"]\n",
    "\n",
    "   - \"molecular_target\": e.g., \"CD20\", \"Kinase-A\", \"Receptor-Z\".\n",
    "     If unknown, use \"\".\n",
    "\n",
    "   - \"mechanism\": e.g., \"monoclonal antibody\", \"kinase inhibitor\",\n",
    "     \"fusion protein\", \"PD-1/LAG-3 bispecific antibody\", etc.\n",
    "     If not inferable, use \"\".\n",
    "\n",
    "   - \"tt_drug_id\": the TrialTrove/PharmaProjects drugId (from fields like\n",
    "     primary_drugs_tested_json / other_drugs_tested_json) for this drug, as a STRING.\n",
    "     If no matching ID can be determined, set this to \"\".\n",
    "\n",
    "   - \"bmt_drug_id\": the BioMedTracker bmtDrugId (from fields like\n",
    "     bmt_primary_drugs_tested_json / bmt_other_drugs_tested_json) for this drug,\n",
    "     as a STRING. If no matching ID can be determined, set this to \"\".\n",
    "\n",
    "   ID ASSIGNMENT GUIDANCE:\n",
    "   - Only assign non-empty \"tt_drug_id\" and \"bmt_drug_id\" when you can confidently\n",
    "     match the canonical drug name to the corresponding JSON object.\n",
    "   - It is especially important to assign these IDs for drugs whose \"role\"\n",
    "     is \"Investigational Product\".\n",
    "   - When converting numeric IDs to strings, do NOT pad or modify them:\n",
    "     e.g., drugId 170544 → \"170544\", bmtDrugId 42756 → \"42756\".\n",
    "\n",
    "Important rules:\n",
    "- \"role\" MUST use only the allowed strings.\n",
    "- No invented drugs.\n",
    "- Combination therapies: classify EACH component using the rules above.\n",
    "- Every drug object MUST contain ALL of the following keys:\n",
    "  \"role\", \"alternative_names\", \"molecular_target\", \"mechanism\",\n",
    "  \"tt_drug_id\", and \"bmt_drug_id\".\n",
    "- If you cannot determine a value, use the empty string \"\" (for IDs,\n",
    "  molecular_target, mechanism) or an empty list [] (for alternative_names).\n",
    "\n",
    "Input JSON:\n",
    "{payload_json}\n",
    "\n",
    "Output format (IMPORTANT):\n",
    "- Return ONLY a valid JSON object with:\n",
    "    - keys   = canonical drug names\n",
    "    - values = objects with EXACTLY:\n",
    "        * \"role\"\n",
    "        * \"alternative_names\"\n",
    "        * \"molecular_target\"\n",
    "        * \"mechanism\"\n",
    "        * \"tt_drug_id\"\n",
    "        * \"bmt_drug_id\"\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "  \"ABC-123\": {{\n",
    "    \"role\": \"Investigational Product\",\n",
    "    \"alternative_names\": [\"ABC123\", \"Compound-ABC\"],\n",
    "    \"molecular_target\": \"Receptor-Z\",\n",
    "    \"mechanism\": \"Bispecific antibody\",\n",
    "    \"tt_drug_id\": \"123456\",\n",
    "    \"bmt_drug_id\": \"78901\"\n",
    "  }},\n",
    "  \"DrugX\": {{\n",
    "    \"role\": \"Standard of Care\",\n",
    "    \"alternative_names\": [\"GenericX\", \"ChemX\"],\n",
    "    \"molecular_target\": \"Enzyme-A\",\n",
    "    \"mechanism\": \"Antimetabolite\",\n",
    "    \"tt_drug_id\": \"\",\n",
    "    \"bmt_drug_id\": \"\"\n",
    "  }},\n",
    "  \"Placebo\": {{\n",
    "    \"role\": \"Placebo\",\n",
    "    \"alternative_names\": [],\n",
    "    \"molecular_target\": \"\",\n",
    "    \"mechanism\": \"Inert comparator\",\n",
    "    \"tt_drug_id\": \"\",\n",
    "    \"bmt_drug_id\": \"\"\n",
    "  }}\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Shared counters & master mapping\n",
    "counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "master_roles: dict[str, dict] = {}\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_trial(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one trial: prompt LLM, save output & log (only if valid).\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = DRUG_ROLE_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # Build payload from selected columns\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_prompt(trial_payload)\n",
    "\n",
    "    token = trial_hash\n",
    "    hash_id = trial_hash\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for trial_hash={trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    drug_roles = extract_json_object(text_response)\n",
    "\n",
    "    # Treat non-dict OR empty dict as invalid → do NOT save anything\n",
    "    if not isinstance(drug_roles, dict) or not drug_roles:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse/validity error trial_hash={trial_hash}, raw={text_response!r}\")\n",
    "        with counter_lock:\n",
    "            counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"title\": row.get(\"title\"),\n",
    "        \"drug_roles\": drug_roles,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    # Save per-trial roles JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (DRUG_ROLE_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master roles\n",
    "    with master_lock:\n",
    "        master_roles[trial_hash] = mapped\n",
    "        MASTER_ROLES_PATH.write_text(\n",
    "            json.dumps(master_roles, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        counter[\"processed\"] += 1\n",
    "        if counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {counter['processed']} trials...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY\n",
    "# -------------------------------------------------\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "rows = df_trials.to_dict(orient=\"records\")\n",
    "total_trials = len(rows)\n",
    "print(f\"Loaded {total_trials} trials from {TRIALS_WITH_HASH_CSV}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_trial, row, idx, total_trials): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        th = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error trial_hash={th}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"✅ Trial drug-role mapping complete. \"\n",
    "    f\"processed={counter['processed']}, \"\n",
    "    f\"skipped={counter['skipped_existing']}, \"\n",
    "    f\"llm_error={counter['llm_error']}, \"\n",
    "    f\"parse_error={counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Roles directory: {DRUG_ROLE_DIR}\")\n",
    "print(f\"Log directory:   {DRUG_ROLE_LOG_DIR}\")\n",
    "print(f\"Master roles:    {MASTER_ROLES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce191b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LLM COST SUMMARY ==========\n",
      "Total LLM cost:             $4.3453\n",
      "Number of logged trials:     184\n",
      "Average cost per trial:      $0.0236\n",
      "\n",
      "Top 10 most expensive trials:\n",
      "  tid_261f0233308ca080d1c60e3fda61ca85.json: $0.0692\n",
      "  tid_1158b3369546dc4b16dc21c8c026b619.json: $0.0606\n",
      "  tid_e0a77c4ecf93cf781f04cc467c974511.json: $0.0522\n",
      "  tid_94883aa2d583afced004e22a7991ef3e.json: $0.0519\n",
      "  tid_196721abc2d5ee98883da9bfcf5bb255.json: $0.0486\n",
      "  tid_e9e01f51b6680ba4f467ac191bb307c5.json: $0.0467\n",
      "  tid_8b4d60a5fddc078962af34399d7e342c.json: $0.0452\n",
      "  tid_763e3011bc90e46c88c7a2953a39ed2a.json: $0.0447\n",
      "  tid_837737698a5271d314ea8208addb2d72.json: $0.0440\n",
      "  tid_7e80effdd579ba535ef686ac50dcc4bc.json: $0.0431\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/trial_drug_roles_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e313110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trial product breakdown to cache/trial_product_breakdown.csv\n",
      "|     | trial_hash                           | investigational_products                      | investigational_products_alternative_names                                                                                                                                                 | investigational_products_molecular_target   | investigational_products_mechanism                                    | investigational_products_tt_drug_id   | investigational_products_bmt_drug_id   | active_comparators                       | active_comparators_alternative_names                                                                                                                                                                                            | active_comparators_molecular_target   | active_comparators_mechanism                                 | active_comparators_tt_drug_id   | active_comparators_bmt_drug_id   | placebos   | placebos_alternative_names   | placebos_molecular_target   | placebos_mechanism   | standard_of_care   | standard_of_care_alternative_names   | standard_of_care_molecular_target   | standard_of_care_mechanism   | standard_of_care_tt_drug_id   | standard_of_care_bmt_drug_id   |\n",
      "|----:|:-------------------------------------|:----------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:----------------------------------------------------------------------|:--------------------------------------|:---------------------------------------|:-----------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------|:-------------------------------------------------------------|:--------------------------------|:---------------------------------|:-----------|:-----------------------------|:----------------------------|:---------------------|:-------------------|:-------------------------------------|:------------------------------------|:-----------------------------|:------------------------------|:-------------------------------|\n",
      "|  94 | tid_0541995757b10e613a42173d6b8ddc09 | ['cinacalcet hydrochloride (test)']           | [['cinacalcet HCl, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet, Zhejiang Wansheng Pharmaceutical Co.']]           | ['Calcium-sensing receptor (CaSR)']         | ['Calcimimetic; calcium-sensing receptor agonist']                    | ['194454']                            | ['']                                   | ['cinacalcet hydrochloride (reference)'] | [['cinacalcet hydrochloride tablets produced by Kyowa Kirin Co., Ltd.', 'cinacalcet HCl (Kyowa Kirin)']]                                                                                                                        | ['Calcium-sensing receptor (CaSR)']   | ['Calcimimetic; calcium-sensing receptor agonist']           | ['']                            | ['']                             | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "|  64 | tid_0da20e863cfc5f3e369868462bff74e0 | ['NuPIAO']                                    | [['nupiao', 'rESP', 'SSS-06', 'SSS 06', 'SSS06', 'NuPIAO (iv)', 'recombinant erythropoietin stimulating protein', 'recombinant erythropoiesis-stimulating protein injection (CHO cells)']] | ['Erythropoietin receptor']                 | ['Erythropoietin receptor agonist']                                   | ['40640']                             | ['19694']                              | []                                       | []                                                                                                                                                                                                                              | []                                    | []                                                           | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| 144 | tid_0e8fa21079f928135dfc6164a15285f8 | ['SSS-17']                                    | [['SSS17', 'SSS 17', '[14C]SSS17', '[14C] SSS17', '[14C]-SSS17', '[¹⁴C]SSS17', '[¹⁴C] SSS17', '[¹⁴C]-SSS17', 'HIF-117', 'HIF 117', 'HIF117', '[14C]HIF-117']]                              | ['Hypoxia-inducible factor (HIF)']          | ['Hypoxia-inducible factor antagonist']                               | ['130313']                            | ['']                                   | []                                       | []                                                                                                                                                                                                                              | []                                    | []                                                           | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "|  19 | tid_0f04ddb3d522d528d083d7d5c43d1e18 | ['Metformin hydrochloride sustained-release'] | [['Metformin Hydrochloride Sustained-release Tablets', 'metformin hydrochloride, Zhejiang Sunshine Mandi Pharmaceutical Co.', 'metformin hydrochloride extended-release', 'metformin XR']] | ['']                                        | ['Biguanide; gluconeogenesis inhibitor; insulin sensitizer']          | ['290388']                            | ['']                                   | ['Glucophage XR']                        | [['Glucophage', '格华止', 'metformin XR', 'metformin hydrochloride, once-daily, BMS', 'metformin HCl, once-daily, BMS', 'Diabex', 'Diabex XR', 'Dabex XR', 'Glifage XR', 'Metgluco', 'Stagid', 'SMP 862', 'SMP-862', 'SMP862']] | ['']                                  | ['Biguanide; gluconeogenesis inhibitor; insulin sensitizer'] | ['24060']                       | ['']                             | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "|  17 | tid_10562c0430b8b9bae93c94cadfb0a129 | ['RD-01']                                     | [['RD001', 'RD-001', 'RD 001', 'RD01', 'RD-01 Long-acting rhEPO', 'Peg-EPO']]                                                                                                              | ['Erythropoietin receptor (EPOR)']          | ['PEGylated recombinant human erythropoietin (EPO) receptor agonist'] | ['144350']                            | ['']                                   | []                                       | []                                                                                                                                                                                                                              | []                                    | []                                                           | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Build trial_product_breakdown.csv\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Base directory for cache + input/output\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "# Directory that contains per-trial drug-role JSONs\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"trial_drug_roles\"\n",
    "\n",
    "# Output CSV path\n",
    "OUT_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fp in DRUG_ROLE_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    drug_roles = obj.get(\"drug_roles\") or {}\n",
    "    if not isinstance(drug_roles, dict):\n",
    "        print(f\"⚠️ drug_roles not dict in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Containers\n",
    "    inv_names = []\n",
    "    inv_alt_names = []          # list of lists\n",
    "    inv_targets = []\n",
    "    inv_mechanisms = []\n",
    "    inv_tt_ids = []\n",
    "    inv_bmt_ids = []\n",
    "\n",
    "    ac_names = []\n",
    "    ac_alt_names = []           # list of lists\n",
    "    ac_targets = []\n",
    "    ac_mechanisms = []\n",
    "    ac_tt_ids = []\n",
    "    ac_bmt_ids = []\n",
    "\n",
    "    plc_names = []\n",
    "    plc_alt_names = []          # list of lists\n",
    "    plc_targets = []\n",
    "    plc_mechanisms = []\n",
    "\n",
    "    soc_names = []\n",
    "    soc_alt_names = []          # list of lists\n",
    "    soc_targets = []\n",
    "    soc_mechanisms = []\n",
    "    soc_tt_ids = []\n",
    "    soc_bmt_ids = []\n",
    "\n",
    "    for drug_name, meta in drug_roles.items():\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "\n",
    "        role = (meta.get(\"role\") or \"\").strip()\n",
    "        role_norm = role.lower()\n",
    "\n",
    "        alt_names = meta.get(\"alternative_names\") or []\n",
    "        if not isinstance(alt_names, list):\n",
    "            alt_names = [str(alt_names)]\n",
    "\n",
    "        molecular_target = meta.get(\"molecular_target\") or \"\"\n",
    "        mechanism = meta.get(\"mechanism\") or \"\"\n",
    "\n",
    "        # IDs are always stored as strings in the LLM output, but be defensive\n",
    "        tt_id = str(meta.get(\"tt_drug_id\") or \"\")\n",
    "        bmt_id = str(meta.get(\"bmt_drug_id\") or \"\")\n",
    "\n",
    "        if role_norm == \"investigational product\":\n",
    "            inv_names.append(drug_name)\n",
    "            inv_alt_names.append(alt_names)\n",
    "            inv_targets.append(molecular_target)\n",
    "            inv_mechanisms.append(mechanism)\n",
    "            inv_tt_ids.append(tt_id)\n",
    "            inv_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"active comparator\":\n",
    "            ac_names.append(drug_name)\n",
    "            ac_alt_names.append(alt_names)\n",
    "            ac_targets.append(molecular_target)\n",
    "            ac_mechanisms.append(mechanism)\n",
    "            ac_tt_ids.append(tt_id)\n",
    "            ac_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"placebo\":\n",
    "            plc_names.append(drug_name)\n",
    "            plc_alt_names.append(alt_names)\n",
    "            plc_targets.append(molecular_target)\n",
    "            plc_mechanisms.append(mechanism)\n",
    "\n",
    "        elif role_norm == \"standard of care\":\n",
    "            soc_names.append(drug_name)\n",
    "            soc_alt_names.append(alt_names)\n",
    "            soc_targets.append(molecular_target)\n",
    "            soc_mechanisms.append(mechanism)\n",
    "            soc_tt_ids.append(tt_id)\n",
    "            soc_bmt_ids.append(bmt_id)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "\n",
    "            \"investigational_products\": inv_names,\n",
    "            \"investigational_products_alternative_names\": inv_alt_names,\n",
    "            \"investigational_products_molecular_target\": inv_targets,\n",
    "            \"investigational_products_mechanism\": inv_mechanisms,\n",
    "            \"investigational_products_tt_drug_id\": inv_tt_ids,\n",
    "            \"investigational_products_bmt_drug_id\": inv_bmt_ids,\n",
    "\n",
    "            \"active_comparators\": ac_names,\n",
    "            \"active_comparators_alternative_names\": ac_alt_names,\n",
    "            \"active_comparators_molecular_target\": ac_targets,\n",
    "            \"active_comparators_mechanism\": ac_mechanisms,\n",
    "            \"active_comparators_tt_drug_id\": ac_tt_ids,\n",
    "            \"active_comparators_bmt_drug_id\": ac_bmt_ids,\n",
    "\n",
    "            \"placebos\": plc_names,\n",
    "            \"placebos_alternative_names\": plc_alt_names,\n",
    "            \"placebos_molecular_target\": plc_targets,\n",
    "            \"placebos_mechanism\": plc_mechanisms,\n",
    "\n",
    "            \"standard_of_care\": soc_names,\n",
    "            \"standard_of_care_alternative_names\": soc_alt_names,\n",
    "            \"standard_of_care_molecular_target\": soc_targets,\n",
    "            \"standard_of_care_mechanism\": soc_mechanisms,\n",
    "            \"standard_of_care_tt_drug_id\": soc_tt_ids,\n",
    "            \"standard_of_care_bmt_drug_id\": soc_bmt_ids,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved trial product breakdown to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e0e24",
   "metadata": {},
   "source": [
    "Manually check the rows with no investigational products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca77eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NO investigational products: 4 / 184\n",
      "| trial_hash                           | investigational_products   |\n",
      "|:-------------------------------------|:---------------------------|\n",
      "| tid_4c45730f6411aa1e5a38bb1223d66988 | []                         |\n",
      "| tid_67de51bf9728e056a6fb42c76e4b0212 | []                         |\n",
      "| tid_8cab7b7177fcb0d10255bced8b0633ee | []                         |\n",
      "| tid_bb1e0571142dde8a49976632c349593c | []                         |\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "IN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "def parse_listish(s: str):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Common empty-list cases\n",
    "    if s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        # If it's something else, treat as a single non-empty token\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        # Fallback: treat non-empty string as a single element\n",
    "        return [s]\n",
    "\n",
    "# Parse the investigational_products column into real lists\n",
    "df[\"investigational_products_parsed\"] = df[\"investigational_products\"].apply(parse_listish)\n",
    "\n",
    "# Flag rows with no investigational products\n",
    "no_inv_mask = df[\"investigational_products_parsed\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "num_no_inv = int(no_inv_mask.sum())\n",
    "total = len(df)\n",
    "\n",
    "print(f\"Rows with NO investigational products: {num_no_inv} / {total}\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\n",
    "    df.loc[no_inv_mask, [\"trial_hash\", \"investigational_products\"]]\n",
    "      .head(20)\n",
    "      .to_markdown(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ab97f",
   "metadata": {},
   "source": [
    "Manual checks \n",
    "- tid_4c45730f6411aa1e5a38bb1223d66988\n",
    "    - This trial is combining three standard-of-care agents into a regimen “DCF”\n",
    "- tid_67de51bf9728e056a6fb42c76e4b0212\n",
    "    - Even though they administer Yisaipu in a structured way, it is an approved drug and not being tested for regulatory approval.\n",
    "- tid_8cab7b7177fcb0d10255bced8b0633ee\n",
    "    - The trial is studying treatment strategies, regimens, algorithms, imaging-guided regimen selection, or dosing, using only approved standard therapies.\n",
    "- tid_bb1e0571142dde8a49976632c349593c\n",
    "    - The trial's focus is on optimizing regimen selection (e.g., TIPy or TCbIPy) via imaging, rather than testing a new drug entity.\n",
    "\n",
    "these are all confirmed generics biosimilars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3463d",
   "metadata": {},
   "source": [
    "Identify and group trials by unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebdc7bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated by tt_drug_id (first 10 rows):\n",
      "|   tt_drug_id | drug_names                           | alternative_names                                                                                                                                                     | molecular_targets                                                                    | product_mechanisms                                                                                                                                                  | trial_hashes                                                                                                                                                                                                                                     |\n",
      "|-------------:|:-------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|       104351 | ['recombinant human thrombopoietin'] | []                                                                                                                                                                    | ['MPL (TPO receptor)']                                                               | ['Thrombopoietin receptor agonist; recombinant cytokine']                                                                                                           | ['tid_58a84007ff09f957ed7e2275b22a07e6']                                                                                                                                                                                                         |\n",
      "|       104513 | ['rhTPO']                            | ['rHuTPO', 'recombinant human thrombopoietin']                                                                                                                        | ['MPL']                                                                              | ['Thrombopoietin receptor agonist']                                                                                                                                 | ['tid_dbcfbd78eefaa77a4d740900bcb30c51']                                                                                                                                                                                                         |\n",
      "|       113881 | ['SB8']                              | ['615', 'Aybintio', 'Onbevzi', 'SB 8', 'SB-8', 'bevacizumab']                                                                                                         | ['VEGF-A']                                                                           | ['Anti-VEGF humanized monoclonal antibody']                                                                                                                         | ['tid_89d6847422b8926a855e677c6434b263']                                                                                                                                                                                                         |\n",
      "|        11947 | ['Methotrexate', 'methotrexate']     | ['Emthexate', 'Lantarel', 'Ledertrexate', 'MTX', 'Novatrex', 'Rheumatrex', 'Trexall', 'methotrexate (oral)']                                                          | ['DHFR', 'Dihydrofolate reductase (DHFR)']                                           | ['Antimetabolite (dihydrofolate reductase inhibitor)', 'Dihydrofolate reductase inhibitor', 'Dihydrofolate reductase inhibitor (antimetabolite immunosuppressant)'] | ['tid_8cab7b7177fcb0d10255bced8b0633ee', 'tid_9dc4ace9308864c9f8a619d6abe32011', 'tid_fc93655913a5e1b233a8077e9fc758c6']                                                                                                                         |\n",
      "|       122198 | ['Bevacizumab']                      | ['601 t', '601-t', '601t', 'bevacizumab, 3SBio', 'bevacizumab, Shanghai CP Guojian']                                                                                  | ['VEGF-A']                                                                           | ['Humanized monoclonal antibody; angiogenesis inhibitor (anti-VEGF)']                                                                                               | ['tid_27f709d68d54479e8ac19e27a5b5a300']                                                                                                                                                                                                         |\n",
      "|       122500 | ['Fexofenadine hydrochloride']       | ['fexofenadine', 'fexofenadine HCl', 'fexofenadine hydrochloride, Zhejiang Wansheng']                                                                                 | ['Histamine H1 receptor']                                                            | ['H1 receptor antagonist (antihistamine)', 'Histamine H1 receptor antagonist']                                                                                      | ['tid_7cbe7f647635abbe0ccccf9233a6e375', 'tid_8c76966ccc6c54153d53dc85c07f1d4b']                                                                                                                                                                 |\n",
      "|        12359 | ['Mycophenolate mofetil']            | ['CellCept', 'ME-MPA', 'MMF', 'Munoloc', 'R-99', 'R99', 'RS-61443', 'Renodapt']                                                                                       | ['IMPDH']                                                                            | ['Inosine monophosphate dehydrogenase inhibitor']                                                                                                                   | ['tid_94883aa2d583afced004e22a7991ef3e']                                                                                                                                                                                                         |\n",
      "|       125796 | ['Apremilast', 'apremilast']         | ['AP-506', 'AP506', 'apremilast, 3SBio']                                                                                                                              | ['PDE4']                                                                             | ['Phosphodiesterase 4 inhibitor']                                                                                                                                   | ['tid_857d006a53745f50ac861416a33e8960', 'tid_888909319365ceead4be875847493bb0']                                                                                                                                                                 |\n",
      "|       126134 | ['toripalimab']                      | ['JS-001', 'JS001', 'Loqtorzi', 'TAB-001', 'TAB001', 'Tuoyi', 'toripalimab-tpzi']                                                                                     | ['PD-1']                                                                             | ['Monoclonal antibody (PD-1 inhibitor)']                                                                                                                            | ['tid_e9e01f51b6680ba4f467ac191bb307c5']                                                                                                                                                                                                         |\n",
      "|       130313 | ['SSS-17', 'SSS17']                  | ['HIF 117', 'HIF-117', 'HIF117', 'SSS 17', 'SSS-17', 'SSS17', '[14C] SSS17', '[14C]-SSS17', '[14C]HIF-117', '[14C]SSS17', '[¹⁴C] SSS17', '[¹⁴C]-SSS17', '[¹⁴C]SSS17'] | ['HIF', 'HIF prolyl hydroxylase (PHD)', 'HIF-PH1', 'Hypoxia-inducible factor (HIF)'] | ['HIF-prolyl hydroxylase inhibitor', 'Hypoxia-inducible factor antagonist']                                                                                         | ['tid_0e8fa21079f928135dfc6164a15285f8', 'tid_232704536c29e97119c451c906d641f0', 'tid_394f064db860c7cebcdd82c40cad1d9d', 'tid_6dc666a780607d3dcb99598f22cb5210', 'tid_b31c0cb67a9379f28e0393dc632f56d2', 'tid_e7a7c80dc675e3ddaf7b4f73690f0015'] |\n",
      "Saved aggregated tt_drug_id table → cache/product_id_master_by_tt.csv\n",
      "Rows missing molecular_targets or product_mechanisms:\n",
      "|   tt_drug_id | drug_names                  | alternative_names                                         | molecular_targets   | product_mechanisms   | trial_hashes                             |\n",
      "|-------------:|:----------------------------|:----------------------------------------------------------|:--------------------|:---------------------|:-----------------------------------------|\n",
      "|       131499 | ['SSS24']                   | ['SSS 24', 'SSS-24']                                      | []                  | []                   | ['tid_a70cae5bfe1598f1d1f9d7e6d2fef4f7'] |\n",
      "|       182230 | ['narfurine hydrochloride'] | ['narfurine hydrochloride orally disintegrating tablets'] | []                  | []                   | ['tid_302158af0c6b2664f2a682e42b6de1e8'] |\n",
      "Saved → cache/product_id_missing_targets_or_mechs.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "IN_CSV   = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load\n",
    "# ---------------------------\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# We'll aggregate everything keyed by tt_drug_id\n",
    "agg = {}  # tt_id -> {\"names\": set(), \"alt_names\": set(), \"targets\": set(), \"mechs\": set(), \"trials\": set()}\n",
    "\n",
    "ROLE_PAIRS = [\n",
    "    (\"investigational_products\", \"investigational_products_tt_drug_id\"),\n",
    "    (\"active_comparators\", \"active_comparators_tt_drug_id\"),\n",
    "    (\"standard_of_care\", \"standard_of_care_tt_drug_id\"),\n",
    "]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "\n",
    "    for base_col, tt_col in ROLE_PAIRS:\n",
    "        # aligned lists\n",
    "        names_list   = parse_listish(row.get(base_col, \"\"))\n",
    "        alts_list    = parse_listish(row.get(f\"{base_col}_alternative_names\", \"\"))\n",
    "        targets_list = parse_listish(row.get(f\"{base_col}_molecular_target\", \"\"))\n",
    "        mechs_list   = parse_listish(row.get(f\"{base_col}_mechanism\", \"\"))\n",
    "        tt_ids       = parse_listish(row.get(tt_col, \"\"))\n",
    "\n",
    "        # iterate by index over tt_ids (they define the products)\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "            if not tt_id:\n",
    "                continue\n",
    "\n",
    "            # init aggregate bucket if needed\n",
    "            if tt_id not in agg:\n",
    "                agg[tt_id] = {\n",
    "                    \"names\": set(),\n",
    "                    \"alt_names\": set(),\n",
    "                    \"targets\": set(),\n",
    "                    \"mechs\": set(),\n",
    "                    \"trials\": set(),\n",
    "                }\n",
    "\n",
    "            # record trial hash if available\n",
    "            if trial_hash:\n",
    "                agg[tt_id][\"trials\"].add(trial_hash)\n",
    "\n",
    "            # name\n",
    "            if i < len(names_list):\n",
    "                name = str(names_list[i]).strip()\n",
    "                if name:\n",
    "                    agg[tt_id][\"names\"].add(name)\n",
    "\n",
    "            # alternative names (may be nested lists)\n",
    "            if i < len(alts_list):\n",
    "                alt_entry = alts_list[i]\n",
    "                if isinstance(alt_entry, list):\n",
    "                    for a in alt_entry:\n",
    "                        a_str = str(a).strip()\n",
    "                        if a_str:\n",
    "                            agg[tt_id][\"alt_names\"].add(a_str)\n",
    "                else:\n",
    "                    a_str = str(alt_entry).strip()\n",
    "                    if a_str:\n",
    "                        agg[tt_id][\"alt_names\"].add(a_str)\n",
    "\n",
    "            # target\n",
    "            if i < len(targets_list):\n",
    "                tgt = str(targets_list[i]).strip()\n",
    "                if tgt:\n",
    "                    agg[tt_id][\"targets\"].add(tgt)\n",
    "\n",
    "            # mechanism\n",
    "            if i < len(mechs_list):\n",
    "                mech = str(mechs_list[i]).strip()\n",
    "                if mech:\n",
    "                    agg[tt_id][\"mechs\"].add(mech)\n",
    "\n",
    "# ---------------------------\n",
    "# Build aggregated DataFrame\n",
    "# ---------------------------\n",
    "rows_out = []\n",
    "for tt_id, payload in agg.items():\n",
    "    rows_out.append(\n",
    "        {\n",
    "            \"tt_drug_id\": tt_id,\n",
    "            \"drug_names\": sorted(payload[\"names\"]),\n",
    "            \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "            \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "            \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "            \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "grouped_df = pd.DataFrame(rows_out).sort_values(\"tt_drug_id\")\n",
    "\n",
    "print(\"Aggregated by tt_drug_id (first 10 rows):\")\n",
    "print(grouped_df.head(10).to_markdown(index=False))\n",
    "\n",
    "OUT_AGG = BASE_DIR / \"product_id_master_by_tt.csv\"\n",
    "grouped_df.to_csv(OUT_AGG, index=False)\n",
    "print(f\"Saved aggregated tt_drug_id table → {OUT_AGG}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Cell: Print all rows missing targets or mechanisms\n",
    "# ---------------------------------------------\n",
    "\n",
    "# A row is \"missing\" if either list is empty\n",
    "missing_mask = grouped_df[\"molecular_targets\"].apply(lambda x: len(x) == 0) & \\\n",
    "               grouped_df[\"product_mechanisms\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "missing_df = grouped_df[missing_mask].copy()\n",
    "\n",
    "print(\"Rows missing molecular_targets or product_mechanisms:\")\n",
    "if missing_df.empty:\n",
    "    print(\"✅ No missing values — every tt_drug_id has targets and mechanisms.\")\n",
    "else:\n",
    "    # Pretty print full table\n",
    "    print(missing_df.to_markdown(index=False))\n",
    "\n",
    "# Optionally save for debugging\n",
    "OUT_MISSING = BASE_DIR / \"product_id_missing_targets_or_mechs.csv\"\n",
    "missing_df.to_csv(OUT_MISSING, index=False)\n",
    "print(f\"Saved → {OUT_MISSING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d86691e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 products missing targets/mechanisms\n",
      "✅ Product mechanism inference complete. processed=2, skipped=0, llm_error=0, parse_error=0\n",
      "Per-product directory: cache/product_mechanism_inference\n",
      "Log directory:        cache/product_mechanism_inference_log\n",
      "Master file:          cache/product_mechanism_inference_master.json\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "RAW_TRIALS_CSV = BASE_DIR / \"raw_trials_with_hash.csv\"\n",
    "\n",
    "PRODUCT_MECH_DIR = BASE_DIR / \"product_mechanism_inference\"\n",
    "PRODUCT_MECH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRODUCT_MECH_LOG_DIR = BASE_DIR / \"product_mechanism_inference_log\"\n",
    "PRODUCT_MECH_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def safe_parse_listish(val):\n",
    "    \"\"\"\n",
    "    Parse list-like strings back into Python lists, if needed.\n",
    "    If already a list, return as-is.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if val is None:\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def build_product_prompt(row: dict, trial_context: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM (with web_search) to infer\n",
    "    molecular_target and mechanism for a product, based on:\n",
    "      - drug names / alternative names\n",
    "      - full metadata for associated trials\n",
    "    \"\"\"\n",
    "    drug_names = row.get(\"drug_names\", []) or []\n",
    "\n",
    "    # Ensure lists are JSON-serializable\n",
    "    try:\n",
    "        drug_names_json = json.dumps(drug_names, ensure_ascii=False)\n",
    "    except TypeError:\n",
    "        drug_names_json = json.dumps([str(x) for x in drug_names], ensure_ascii=False)\n",
    "\n",
    "    # Trials context JSON\n",
    "    trials_json = json.dumps(trial_context, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a pharmacology expert with access to web search.\n",
    "\n",
    "You are given:\n",
    "- A drug name.\n",
    "- Full metadata for one or more clinical trials in which this drug appears (JSON objects).\n",
    "\n",
    "Your goal:\n",
    "Using web search and your domain knowledge, determine:\n",
    "1. The primary molecular target(s) of the drug (e.g., EGFR, VEGFR2, TNF, CD20, JAK1/2).\n",
    "2. A concise, standard mechanism of action label (e.g., \"EGFR inhibitor\", \"Anti-PD-1 antibody\", \n",
    "   \"JAK inhibitor\", \"DNA-damaging cytotoxic\", etc.).\n",
    "\n",
    "Rules:\n",
    "- Try searching for the drug using all known names or aliases.\n",
    "- If **no molecular target or mechanism of action has been disclosed publicly**, then return **empty strings** for those fields.\n",
    "\n",
    "INPUT\n",
    "-----\n",
    "drug_name: {drug_names_json}\n",
    "trial_metadata:\n",
    "{trials_json}\n",
    "\n",
    "OUTPUT (JSON only)\n",
    "------------------\n",
    "{{\n",
    "  \"molecular_target\": \"\",\n",
    "  \"mechanism\": \"\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Shared counters & master mapping\n",
    "product_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "}\n",
    "product_counter_lock = threading.Lock()\n",
    "\n",
    "product_master: dict[str, dict] = {}\n",
    "product_master_lock = threading.Lock()\n",
    "\n",
    "# Load existing master if present\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    try:\n",
    "        product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        product_master = {}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LOAD TRIAL METADATA AND BUILD INDEX\n",
    "# -------------------------------------------------\n",
    "df_trials = pd.read_csv(RAW_TRIALS_CSV, dtype=str).fillna(\"\")\n",
    "trials_index: dict[str, dict] = {\n",
    "    str(row[\"trial_hash\"]).strip(): row.to_dict()\n",
    "    for _, row in df_trials.iterrows()\n",
    "}\n",
    "\n",
    "\n",
    "def process_product(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one tt_drug_id: prompt LLM+web_search with trial context, save output & log.\"\"\"\n",
    "    tt_drug_id = str(row.get(\"tt_drug_id\", \"\")).strip()\n",
    "    if not tt_drug_id:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing tt_drug_id, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = PRODUCT_MECH_DIR / f\"{tt_drug_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # Get associated trial hashes and build trial context list\n",
    "    trial_hashes_raw = row.get(\"trial_hashes\", [])\n",
    "    trial_hashes = safe_parse_listish(trial_hashes_raw)\n",
    "\n",
    "    trial_context = []\n",
    "    for th in trial_hashes:\n",
    "        th_key = str(th).strip()\n",
    "        if not th_key:\n",
    "            continue\n",
    "        trial_row = trials_index.get(th_key)\n",
    "        if trial_row:\n",
    "            trial_context.append(trial_row)\n",
    "\n",
    "    prompt = build_product_prompt(row, trial_context)\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM with web_search tool\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(\n",
    "            prompt=prompt,\n",
    "            model=MODEL,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "        )\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for tt_drug_id={tt_drug_id}: {e}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mech_obj = extract_json_object(text_response)\n",
    "\n",
    "    # Expect a dict with the two keys\n",
    "    if not isinstance(mech_obj, dict) or not mech_obj:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse/validity error tt_drug_id={tt_drug_id}, raw={text_response!r}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    molecular_target = str(mech_obj.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "    mechanism = str(mech_obj.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "    mapped = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"drug_names\": row.get(\"drug_names\", []),\n",
    "        \"alternative_names\": row.get(\"alternative_names\", []),\n",
    "        \"trial_hashes\": trial_hashes,\n",
    "        \"molecular_target\": molecular_target,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"source\": \"llm_web_search\",\n",
    "    }\n",
    "\n",
    "    # Save per-product JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (PRODUCT_MECH_LOG_DIR / f\"{tt_drug_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with product_master_lock:\n",
    "        product_master[tt_drug_id] = mapped\n",
    "        MASTER_PRODUCT_MECH_PATH.write_text(\n",
    "            json.dumps(product_master, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with product_counter_lock:\n",
    "        product_counter[\"processed\"] += 1\n",
    "        if product_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {product_counter['processed']} products...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY ON MISSING PRODUCTS\n",
    "# -------------------------------------------------\n",
    "# missing_df was defined in the previous cell and includes trial_hashes\n",
    "missing_rows = missing_df.to_dict(orient=\"records\")\n",
    "total_missing = len(missing_rows)\n",
    "print(f\"Loaded {total_missing} products missing targets/mechanisms\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_product, row, idx, total_missing): row.get(\"tt_drug_id\")\n",
    "        for idx, row in enumerate(missing_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        tid = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error tt_drug_id={tid}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"✅ Product mechanism inference complete. \"\n",
    "    f\"processed={product_counter['processed']}, \"\n",
    "    f\"skipped={product_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={product_counter['llm_error']}, \"\n",
    "    f\"parse_error={product_counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Per-product directory: {PRODUCT_MECH_DIR}\")\n",
    "print(f\"Log directory:        {PRODUCT_MECH_LOG_DIR}\")\n",
    "print(f\"Master file:          {MASTER_PRODUCT_MECH_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90e85a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trial breakdown: cache/trial_product_breakdown.csv, shape=(184, 23)\n",
      "Updated 1 trial rows with inferred mechanisms/targets.\n",
      "Filled targets: 1, filled mechanisms: 1\n",
      "✅ Saved filled trial breakdown to cache/trial_product_breakdown_filled.csv\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "IN_BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "OUT_FILLED_CSV = BASE_DIR / \"trial_product_breakdown_filled.csv\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def pad_to_length(lst, n):\n",
    "    \"\"\"Pad list with empty strings so len(lst) >= n.\"\"\"\n",
    "    lst = list(lst)\n",
    "    while len(lst) < n:\n",
    "        lst.append(\"\")\n",
    "    return lst\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load inputs\n",
    "# ----------------------------------------\n",
    "df = pd.read_csv(IN_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trial breakdown: {IN_BREAKDOWN_CSV}, shape={df.shape}\")\n",
    "\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    product_master = {}\n",
    "    print(f\"⚠️ No product master mech file found at {MASTER_PRODUCT_MECH_PATH}\")\n",
    "\n",
    "# role → tt_id / target / mech columns\n",
    "ROLE_SPECS = [\n",
    "    (\n",
    "        \"investigational_products\",\n",
    "        \"investigational_products_tt_drug_id\",\n",
    "        \"investigational_products_molecular_target\",\n",
    "        \"investigational_products_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"active_comparators\",\n",
    "        \"active_comparators_tt_drug_id\",\n",
    "        \"active_comparators_molecular_target\",\n",
    "        \"active_comparators_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"standard_of_care\",\n",
    "        \"standard_of_care_tt_drug_id\",\n",
    "        \"standard_of_care_molecular_target\",\n",
    "        \"standard_of_care_mechanism\",\n",
    "    ),\n",
    "    # Note: placebos have no tt_drug_id column, so we can't enrich them here.\n",
    "]\n",
    "\n",
    "# ----------------------------------------\n",
    "# Enrich targets/mechanisms from product_master\n",
    "# ----------------------------------------\n",
    "updated_rows = 0\n",
    "filled_targets = 0\n",
    "filled_mechs = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    row_changed = False\n",
    "\n",
    "    for role_name, tt_col, tgt_col, mech_col in ROLE_SPECS:\n",
    "        # If column missing (defensive), skip\n",
    "        if tt_col not in df.columns or tgt_col not in df.columns or mech_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        tt_ids = parse_listish(row.get(tt_col, \"\"))\n",
    "        if not tt_ids:\n",
    "            continue\n",
    "\n",
    "        targets = parse_listish(row.get(tgt_col, \"\"))\n",
    "        mechs = parse_listish(row.get(mech_col, \"\"))\n",
    "\n",
    "        # Ensure alignment\n",
    "        targets = pad_to_length(targets, len(tt_ids))\n",
    "        mechs = pad_to_length(mechs, len(tt_ids))\n",
    "\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "            if not tt_id:\n",
    "                continue\n",
    "\n",
    "            info = product_master.get(tt_id)\n",
    "            if not info:\n",
    "                continue\n",
    "\n",
    "            inferred_target = str(info.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "            inferred_mech = str(info.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "            # Only fill if currently empty and inference has a non-empty value\n",
    "            if (not targets[i].strip()) and inferred_target:\n",
    "                targets[i] = inferred_target\n",
    "                filled_targets += 1\n",
    "                row_changed = True\n",
    "\n",
    "            if (not mechs[i].strip()) and inferred_mech:\n",
    "                mechs[i] = inferred_mech\n",
    "                filled_mechs += 1\n",
    "                row_changed = True\n",
    "\n",
    "        # Write back updated lists (as Python-literal strings)\n",
    "        if row_changed:\n",
    "            df.at[idx, tgt_col] = repr(targets)\n",
    "            df.at[idx, mech_col] = repr(mechs)\n",
    "\n",
    "    if row_changed:\n",
    "        updated_rows += 1\n",
    "\n",
    "print(f\"Updated {updated_rows} trial rows with inferred mechanisms/targets.\")\n",
    "print(f\"Filled targets: {filled_targets}, filled mechanisms: {filled_mechs}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Save filled CSV\n",
    "# ----------------------------------------\n",
    "OUT_FILLED_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_FILLED_CSV, index=False)\n",
    "print(f\"✅ Saved filled trial breakdown to {OUT_FILLED_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709c055",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77190ce",
   "metadata": {},
   "source": [
    "Identify whether the drugs are innovative or/generic biosimilars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 180 trials with investigational products for innovation-status classification.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV    = BASE_DIR / \"raw_trials_with_hash.csv\"\n",
    "PRODUCT_BREAKDOWN_CSV   = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "INNOV_DIR = BASE_DIR / \"trial_investigational_drugs_classifications\"\n",
    "INNOV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INNOV_LOG_DIR = BASE_DIR / \"trial_investigational_drugs_classifications_log\"\n",
    "INNOV_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INNOV_PATH = BASE_DIR / \"trial_investigational_drugs_classifications_master.json\"\n",
    "\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "MAX_WORKERS_INNOV = 8\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def load_master_innov() -> dict:\n",
    "    if not MASTER_INNOV_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_INNOV_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "def parse_listish(s: str):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Common empty-list cases\n",
    "    if s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        # If it's something else, treat as a single non-empty token\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        # Fallback: treat non-empty string as a single element\n",
    "        return [s]\n",
    "\n",
    "master_innov = load_master_innov()\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "innov_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,\n",
    "}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def build_innovation_prompt(trial_payload: dict, investigational_products: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt to classify each investigational product as\n",
    "    Innovative / Generic / Biosimilar, with one-sentence explanation.\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "    drugs_json   = json.dumps(investigational_products, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and drug development expert.\n",
    "\n",
    "You are given:\n",
    "1) Structured information about a clinical trial (title, objective, results, etc.).\n",
    "2) A list of investigational products used in the trial.\n",
    "3) Structured fields describing how drugs are classified in the study\n",
    "   (investigational_products, active_comparators, placebos, standard_of_care, etc.).\n",
    "\n",
    "Your task: For EACH investigational product, classify whether it is:\n",
    "- \"Innovative\"\n",
    "- \"Generic\"\n",
    "- \"Biosimilar\"\n",
    "\n",
    "and provide a one-sentence concise explanation for your classification.\n",
    "\n",
    "DEFINITIONS / GUIDANCE\n",
    "----------------------\n",
    "\n",
    "Innovative:\n",
    "- A novel or proprietary drug.\n",
    "- New mechanism of action OR new biological entity OR clearly sponsor's lead product.\n",
    "- Often associated with superiority or efficacy language:\n",
    "  - \"evaluate efficacy\", \"vs placebo\", \"improve outcomes\", etc.\n",
    "- Not a copy of an already-approved product.\n",
    "\n",
    "Generic:\n",
    "- A small-molecule copy of an already-approved branded drug.\n",
    "- Same active ingredient, strength, dosage form, and route.\n",
    "- Often associated with:\n",
    "  - language like \"generic\", \"copy\", \"equivalent\",\n",
    "  - OR clear indication that the product is a non-branded version.\n",
    "\n",
    "Biosimilar:\n",
    "- A biologic product that is highly similar to an already-approved reference biologic.\n",
    "- Same target and mechanism as a branded biologic.\n",
    "- Strong clues:\n",
    "  - \"equivalence\", \"non-inferiority\", \"no clinically meaningful differences\",\n",
    "  - direct comparison to a specific branded reference biologic with the SAME active ingredient.\n",
    "\n",
    "Task 2 — Classify Innovation Status\n",
    "-----------------------------------\n",
    "\n",
    "Use clues within the text to determine whether each investigational drug is:\n",
    "- \"Innovative\"\n",
    "- \"Generic\"\n",
    "- \"Biosimilar\"\n",
    "\n",
    "Examples of helpful cues:\n",
    "- Innovative:\n",
    "  - Superiority/efficacy language (\"versus placebo\", \"evaluate efficacy\").\n",
    "  - Novel or advanced mechanism, new target, or first-in-class description.\n",
    "- Biosimilar:\n",
    "  - Equivalence or non-inferiority language.\n",
    "  - Direct comparison to a branded reference product with the same active ingredient.\n",
    "- Generic:\n",
    "  - Explicitly described as generic.\n",
    "  - Non-biologic small-molecule copy of an existing branded product.\n",
    "\n",
    "If the information is incomplete, choose the MOST LIKELY label based on the text and typical drug naming patterns.\n",
    "You MUST still choose ONE of the three labels (\"Innovative\", \"Generic\", \"Biosimilar\") for each drug.\n",
    "If you are uncertain, you may say so in the one-sentence explanation.\n",
    "\n",
    "OUTPUT FORMAT (IMPORTANT)\n",
    "-------------------------\n",
    "\n",
    "Return ONLY a valid JSON object, with:\n",
    "- KEYS   = exactly the investigational product names as provided in the list below\n",
    "- VALUES = an object with exactly two fields:\n",
    "    - \"classification\": one of \"Innovative\", \"Generic\", \"Biosimilar\"\n",
    "    - \"explanation\": a single, concise sentence explaining your reasoning\n",
    "\n",
    "You MUST provide a classification for EVERY investigational product name.\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "  \"DrugA\": {{\n",
    "    \"classification\": \"Innovative\",\n",
    "    \"explanation\": \"DrugA is a novel monoclonal antibody targeting a new receptor and is the sponsor's lead product.\"\n",
    "  }},\n",
    "  \"DrugB\": {{\n",
    "    \"classification\": \"Biosimilar\",\n",
    "    \"explanation\": \"DrugB is tested for non-inferiority compared to the branded biologic with the same target.\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "TRIAL PAYLOAD (includes trial text and all drug-role breakdown columns):\n",
    "{payload_json}\n",
    "\n",
    "INVESTIGATIONAL PRODUCTS (you MUST classify EACH of these):\n",
    "{drugs_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def process_innov_row(row: dict, idx: int, total: int, breakdown_cols: list[str]) -> None:\n",
    "    \"\"\"Process a single trial with investigational products.\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    investigational_products = row.get(\"investigational_products_parsed\") or []\n",
    "    investigational_products = [str(x).strip() for x in investigational_products if str(x).strip()]\n",
    "\n",
    "    if not investigational_products:\n",
    "        # Shouldn't happen due to filtering, but be safe\n",
    "        return\n",
    "\n",
    "    out_fp = INNOV_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            innov_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # Build payload from selected columns\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "\n",
    "    # 1) Trial-level textual fields from raw_trials_with_hash.csv\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    # 2) ALL columns from trial_product_breakdown.csv\n",
    "    for col in breakdown_cols:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_innovation_prompt(trial_payload, investigational_products)\n",
    "\n",
    "    token = trial_hash\n",
    "    hash_id = trial_hash\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for trial_hash={trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    classifications = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(classifications, dict) or not classifications:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse error trial_hash={trial_hash}, raw={text_response!r}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Check coverage: every investigational product must be present as a key\n",
    "    missing = [d for d in investigational_products if d not in classifications]\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"⚠️ [{idx}/{total}] Coverage error for trial_hash={trial_hash}: \"\n",
    "            f\"missing classifications for {missing}\"\n",
    "        )\n",
    "        with counter_lock:\n",
    "            innov_counter[\"coverage_error\"] += 1\n",
    "        # DO NOT save this trial so it can be re-run next time\n",
    "        return\n",
    "\n",
    "    # Optional: sanity check that each value has classification + explanation\n",
    "    for d in investigational_products:\n",
    "        meta = classifications.get(d, {})\n",
    "        if not isinstance(meta, dict):\n",
    "            print(f\"⚠️ [{idx}/{total}] Invalid meta for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "        if \"classification\" not in meta or \"explanation\" not in meta:\n",
    "            print(f\"⚠️ [{idx}/{total}] Missing fields for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"investigational_products\": investigational_products,\n",
    "        \"classifications\": classifications,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    # Save per-trial JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (INNOV_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_lock:\n",
    "        master_innov[trial_hash] = mapped\n",
    "        MASTER_INNOV_PATH.write_text(\n",
    "            json.dumps(master_innov, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        innov_counter[\"processed\"] += 1\n",
    "        if innov_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {innov_counter['processed']} trials for innovation status...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LOAD & MERGE DATA\n",
    "# -------------------------------------------------\n",
    "# Load breakdown (investigational products + all drug-role cols)\n",
    "df_breakdown = pd.read_csv(PRODUCT_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Reuse parse_listish from previous cell\n",
    "df_breakdown[\"investigational_products_parsed\"] = df_breakdown[\"investigational_products\"].apply(parse_listish)\n",
    "mask_has_inv = df_breakdown[\"investigational_products_parsed\"].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Restrict to rows with investigational products\n",
    "df_breakdown_sub = df_breakdown.loc[mask_has_inv].copy()\n",
    "\n",
    "# All columns from trial_product_breakdown.csv except trial_hash (which is already separate)\n",
    "BREAKDOWN_COLS = [c for c in df_breakdown_sub.columns if c != \"trial_hash\"]\n",
    "\n",
    "# Load raw trials (for RELEVANT_COLS)\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Merge on trial_hash; keep all breakdown columns + investigational_products_parsed + RELEVANT_COLS\n",
    "df_merged = df_breakdown_sub.merge(\n",
    "    df_trials[[\"trial_hash\"] + RELEVANT_COLS],\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "innov_rows = df_merged.to_dict(orient=\"records\")\n",
    "total_innov = len(innov_rows)\n",
    "print(f\"Loaded {total_innov} trials with investigational products for innovation-status classification.\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY\n",
    "# -------------------------------------------------\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_INNOV) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_innov_row, row, idx, total_innov, BREAKDOWN_COLS): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(innov_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        th = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error (innovation) trial_hash={th}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"✅ Trial investigational-drug innovation classification complete. \"\n",
    "    f\"processed={innov_counter['processed']}, \"\n",
    "    f\"skipped={innov_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={innov_counter['llm_error']}, \"\n",
    "    f\"parse_error={innov_counter['parse_error']}, \"\n",
    "    f\"coverage_error={innov_counter['coverage_error']}\"\n",
    ")\n",
    "print(f\"Classifications directory: {INNOV_DIR}\")\n",
    "print(f\"Log directory:             {INNOV_LOG_DIR}\")\n",
    "print(f\"Master classifications:    {MASTER_INNOV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/trial_investigational_drugs_classifications_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OUT_CSV = BASE_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fp in INNOV_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    inv_products_raw = obj.get(\"investigational_products\") or []\n",
    "    classifications_map = obj.get(\"classifications\") or {}\n",
    "\n",
    "    flat_products = []\n",
    "    flat_classifications = []\n",
    "\n",
    "    for drug_raw in inv_products_raw:\n",
    "        # drug_raw might be \"['inetetamab', 'toripalimab']\" or just \"SSGJ-707\"\n",
    "        if isinstance(drug_raw, str):\n",
    "            parsed_names = parse_listish(drug_raw)  # from earlier cell (uses ast.literal_eval)\n",
    "        else:\n",
    "            parsed_names = [drug_raw]\n",
    "\n",
    "        # Prefer classification using the exact key that was sent to the model\n",
    "        meta = classifications_map.get(drug_raw, {})\n",
    "        cls = meta.get(\"classification\", \"\")\n",
    "\n",
    "        # If not found, try each parsed name as a key\n",
    "        if not cls:\n",
    "            for name in parsed_names:\n",
    "                meta_n = classifications_map.get(name, {})\n",
    "                if \"classification\" in meta_n:\n",
    "                    cls = meta_n.get(\"classification\", \"\")\n",
    "                    break\n",
    "\n",
    "        if not cls:\n",
    "            print(\n",
    "                f\"⚠️ Missing classification for raw drug {drug_raw!r} in \"\n",
    "                f\"trial_hash={trial_hash}, file={fp.name}\"\n",
    "            )\n",
    "\n",
    "        # Add one entry per parsed name so both lists are flat and aligned\n",
    "        for name in parsed_names:\n",
    "            flat_products.append(name)\n",
    "            flat_classifications.append(cls)\n",
    "\n",
    "    # Sanity check: lengths must match\n",
    "    if len(flat_products) != len(flat_classifications):\n",
    "        print(\n",
    "            f\"⚠️ Length mismatch for trial_hash={trial_hash}: \"\n",
    "            f\"{len(flat_products)} products vs {len(flat_classifications)} classifications\"\n",
    "        )\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "            # store as JSON stringified flat lists\n",
    "            \"investigational_products\": json.dumps(flat_products, ensure_ascii=False),\n",
    "            \"investigational_products_classifications\": json.dumps(flat_classifications, ensure_ascii=False),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"✅ Saved investigational drug classifications to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96291d",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f9e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "MOA_MASTER_PATH = BASE_DIR / \"investigational_drug_moa_master.json\"\n",
    "\n",
    "# -----------------------------------\n",
    "# Helper: deterministic hash\n",
    "# -----------------------------------\n",
    "def make_hash_id(text: str) -> str:\n",
    "    \"\"\"Deterministic hash ID for a mechanism string.\"\"\"\n",
    "    normalized = text.strip().lower()\n",
    "    return \"mid_\" + hashlib.md5(normalized.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Ensure parse_listish exists\n",
    "try:\n",
    "    parse_listish\n",
    "except NameError:\n",
    "    import ast\n",
    "    def parse_listish(s: str):\n",
    "        if not isinstance(s, str):\n",
    "            return []\n",
    "        s = s.strip()\n",
    "        if not s or s in (\"[]\", \"[ ]\"):\n",
    "            return []\n",
    "        try:\n",
    "            val = ast.literal_eval(s)\n",
    "            return val if isinstance(val, list) else [val]\n",
    "        except Exception:\n",
    "            return [s]\n",
    "\n",
    "# -----------------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------------\n",
    "df = pd.read_csv(BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Columns\n",
    "INV_PRODUCTS_COL = \"investigational_products\"\n",
    "INV_MOA_COL      = \"investigational_products_mechanism\"\n",
    "\n",
    "MOA_COLS_EXTRA = [\n",
    "    \"active_comparators_mechanism\",\n",
    "    \"standard_of_care_mechanism\",\n",
    "]\n",
    "\n",
    "missing_cols = [c for c in [INV_PRODUCTS_COL, INV_MOA_COL] + MOA_COLS_EXTRA if c not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"⚠️ Missing expected columns: {missing_cols}\")\n",
    "\n",
    "# -----------------------------------\n",
    "# COLLECT UNIQUE MOAs\n",
    "# -----------------------------------\n",
    "unique_moas = set()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # --- 1) Investigational products: mechanism with fallback to product name ---\n",
    "    inv_products_raw = row.get(INV_PRODUCTS_COL, \"\")\n",
    "    inv_moa_raw      = row.get(INV_MOA_COL, \"\")\n",
    "\n",
    "    inv_products = parse_listish(inv_products_raw)\n",
    "    inv_moas     = parse_listish(inv_moa_raw)\n",
    "\n",
    "    # Make sure lists are aligned by index; fallback per drug\n",
    "    max_len = max(len(inv_products), len(inv_moas))\n",
    "    for i in range(max_len):\n",
    "        drug_name = str(inv_products[i]).strip() if i < len(inv_products) else \"\"\n",
    "        mech      = str(inv_moas[i]).strip() if i < len(inv_moas) else \"\"\n",
    "\n",
    "        if mech:\n",
    "            unique_moas.add(mech)\n",
    "        elif drug_name:\n",
    "            # Fallback: use investigational product name as the \"mechanism\" stand-in\n",
    "            unique_moas.add(drug_name)\n",
    "\n",
    "    # --- 2) Extra MOA columns (no fallback, same as before) ---\n",
    "    for col in MOA_COLS_EXTRA:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        raw = row.get(col, \"\")\n",
    "        parsed = parse_listish(raw)\n",
    "        for item in parsed:\n",
    "            item = str(item).strip()\n",
    "            if item:\n",
    "                unique_moas.add(item)\n",
    "\n",
    "print(f\"Found {len(unique_moas)} unique MOA strings (including fallbacks to product names).\")\n",
    "\n",
    "# -----------------------------------\n",
    "# BUILD MASTER DICT\n",
    "# -----------------------------------\n",
    "moa_master = {}\n",
    "\n",
    "for moa in sorted(unique_moas):\n",
    "    hash_id = make_hash_id(moa)\n",
    "    moa_master[hash_id] = {\n",
    "        \"moa_id\": hash_id,\n",
    "        \"mechanism\": moa,\n",
    "        \"source\": \"trial_product_breakdown_or_fallback\",\n",
    "    }\n",
    "\n",
    "# -----------------------------------\n",
    "# SAVE MASTER FILE\n",
    "# -----------------------------------\n",
    "MOA_MASTER_PATH.write_text(\n",
    "    json.dumps(moa_master, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Saved MOA master to {MOA_MASTER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed search for each MOA (HASH-BASED OUTPUT FILENAMES)\n",
    "\n",
    "import os, json, time, html, unicodedata\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / Config\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "MOA_MASTER_PATH   = BASE_DIR / \"investigational_drug_moa_master.json\"\n",
    "OUT_DIR           = BASE_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INDEX_PATH = BASE_DIR / \"investigational_drug_moa_pubmed_index.json\"\n",
    "\n",
    "EUTILS     = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "API_KEY    = os.getenv(\"NCBI_API_KEY\") or None\n",
    "EMAIL      = os.getenv(\"NCBI_EMAIL\") or None\n",
    "SLEEP      = 0.25\n",
    "RETRY_MAX  = 3\n",
    "RETRY_WAIT = 1.0\n",
    "\n",
    "# LLM config (for MOA refinement)\n",
    "MODEL  = \"gpt-5-mini\"   # adjust if needed\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "NAN_STRINGS = {\"nan\", \"none\", \"null\", \"\"}\n",
    "\n",
    "def _clean(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s_str = str(s).strip()\n",
    "    return \"\" if s_str.lower() in NAN_STRINGS else s_str\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = \" \".join(t.strip().lower().split())\n",
    "    return \"\" if t in NAN_STRINGS else t\n",
    "\n",
    "def _http_get_with_retry(url: str, params: dict, timeout: int) -> requests.Response:\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "def esearch_ids(term: str, n: int = 3) -> list[str]:\n",
    "    term = _clean(term)\n",
    "    if not term:\n",
    "        return []\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": n,\n",
    "        \"sort\": \"relevance\",\n",
    "    }\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/esearch.fcgi\", params=params, timeout=30)\n",
    "    return r.json().get(\"esearchresult\", {}).get(\"idlist\", []) or []\n",
    "\n",
    "def _parse_xml_with_retry(text: str) -> ET.Element:\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            return ET.fromstring(text)\n",
    "        except ET.ParseError as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "def efetch_details(pmids: list[str]) -> dict:\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    params = {\"db\": \"pubmed\", \"id\": \",\".join(pmids), \"retmode\": \"xml\"}\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/efetch.fcgi\", params=params, timeout=60)\n",
    "    root = _parse_xml_with_retry(r.text)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    def text_from_el(el):\n",
    "        return \"\".join(el.itertext()).strip() if el is not None else \"\"\n",
    "\n",
    "    def join_abstract(abs_parent):\n",
    "        parts = []\n",
    "        for t in abs_parent.findall(\"AbstractText\"):\n",
    "            label = t.attrib.get(\"Label\")\n",
    "            txt = text_from_el(t)\n",
    "            if txt:\n",
    "                parts.append(f\"{label}: {txt}\" if label else txt)\n",
    "        return \"\\n\".join(parts).strip()\n",
    "\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid_el = art.find(\".//MedlineCitation/PMID\")\n",
    "        if pmid_el is None or not (pmid_el.text or \"\").strip():\n",
    "            continue\n",
    "        pmid = pmid_el.text.strip()\n",
    "\n",
    "        title = text_from_el(art.find(\".//Article/ArticleTitle\"))\n",
    "        abs_parent = art.find(\".//Article/Abstract\")\n",
    "        abstract = join_abstract(abs_parent) if abs_parent is not None else \"\"\n",
    "\n",
    "        mesh_terms = []\n",
    "        for mh in art.findall(\".//MedlineCitation/MeshHeadingList/MeshHeading\"):\n",
    "            desc = mh.find(\"DescriptorName\")\n",
    "            if desc is None or not (desc.text or \"\").strip():\n",
    "                continue\n",
    "            d_text = desc.text.strip()\n",
    "            d_major = desc.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "            d_str = f\"{d_text}{'*' if d_major else ''}\"\n",
    "\n",
    "            quals = []\n",
    "            for q in mh.findall(\"QualifierName\"):\n",
    "                q_text = (q.text or \"\").strip()\n",
    "                if q_text:\n",
    "                    q_major = q.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "                    quals.append(f\"{q_text}{'*' if q_major else ''}\")\n",
    "\n",
    "            mesh_terms.append(d_str if not quals else d_str + \" / \" + \"; \".join(quals))\n",
    "\n",
    "        substances = []\n",
    "        for chem in art.findall(\".//Chemical\"):\n",
    "            nm_el = chem.find(\"NameOfSubstance\")\n",
    "            rn_el = chem.find(\"RegistryNumber\")\n",
    "            nm = nm_el.text.strip() if nm_el is not None else \"\"\n",
    "            rn = rn_el.text.strip() if rn_el is not None else \"\"\n",
    "            if nm and rn and rn != \"0\":\n",
    "                substances.append(f\"{nm} [RN:{rn}]\")\n",
    "            elif nm:\n",
    "                substances.append(nm)\n",
    "            elif rn and rn != \"0\":\n",
    "                substances.append(f\"[RN:{rn}]\")\n",
    "\n",
    "        def uniq(xs):\n",
    "            seen, out_local = set(), []\n",
    "            for x in xs:\n",
    "                if x and x not in seen:\n",
    "                    seen.add(x)\n",
    "                    out_local.append(x)\n",
    "            return out_local\n",
    "\n",
    "        out[pmid] = {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"mesh_terms\": uniq(mesh_terms),\n",
    "            \"substances\": uniq(substances),\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "def load_json_or_empty(path: Path) -> dict:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def split_terms(s: str):\n",
    "    \"\"\"\n",
    "    For MOA strings like:\n",
    "      'Thrombopoietin receptor agonist (recombinant growth factor); PEGylated recombinant human EPO'\n",
    "    we split on ';' and treat each piece as a candidate search term.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    raw = [t.strip() for t in str(s).split(\";\")]\n",
    "    return [t for t in raw if t and t.lower() not in NAN_STRINGS]\n",
    "\n",
    "# --------------- NEW: LLM refinement helpers ---------------\n",
    "\n",
    "def build_moa_refinement_prompt(mechanism: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the chatbot to turn a free-text MOA into a concise, canonical\n",
    "    mechanism-of-action phrase suitable for PubMed search.\n",
    "\n",
    "    Examples:\n",
    "      - \"Monoclonal antibody–IL-15 fusion (bifunctional immunocytokine) targeting B7-H3\"\n",
    "            -> \"Immunocytokines\"\n",
    "      - \"Non-absorbed phosphate-binding polymer (anion exchange resin)\"\n",
    "            -> \"Ion Exchange Resins\"\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert clinical pharmacologist and mechanisms-of-action classifier.\n",
    "\n",
    "Given the following mechanism-of-action (MOA) description from a drug development database:\n",
    "\n",
    "\\\"\\\"\\\"{mechanism}\\\"\\\"\\\"\n",
    "\n",
    "Rewrite or condense it into a SHORT, CANONICAL mechanism-of-action term that would work well as a PubMed search term.\n",
    "\n",
    "Rules:\n",
    "- Output a concise mechanism class or well-recognized pharmacologic concept, not a full sentence.\n",
    "- Prefer standard pharmacologic/mechanistic classes (e.g. \"Ion Exchange Resins\", \"Immunocytokines\",\n",
    "  \"Kinase Inhibitors\", \"Antibodies, Monoclonal\", \"Immune Checkpoint Inhibitors\").\n",
    "- Do NOT include long target listings or extra explanation.\n",
    "- If the original MOA is already an appropriate concise search term, you may return it unchanged.\n",
    "\n",
    "Return ONLY the refined mechanism phrase, with no additional explanation or formatting.\n",
    "\"\"\".strip()\n",
    "\n",
    "def refine_mechanism_with_llm(mechanism: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use the OpenAIWrapper .query() interface to get a refined mechanism phrase.\n",
    "    Returns the refined phrase or None on failure.\n",
    "    \"\"\"\n",
    "    mech_clean = _clean(mechanism)\n",
    "    if not mech_clean:\n",
    "        return None\n",
    "\n",
    "    prompt = build_moa_refinement_prompt(mech_clean)\n",
    "\n",
    "    try:\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        text = (res.get(\"text_response\") or \"\").strip()\n",
    "        # Strip surrounding quotes if the model adds them\n",
    "        text = text.strip().strip('\"').strip(\"'\")\n",
    "        refined = _clean(text)\n",
    "        return refined or None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LLM refinement failed for mechanism='{mech_clean[:80]}': {e}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Load MOA master\n",
    "# -----------------------------\n",
    "moa_master = load_json_or_empty(MOA_MASTER_PATH)\n",
    "if not moa_master:\n",
    "    raise RuntimeError(f\"No MOA entries found in {MOA_MASTER_PATH}\")\n",
    "\n",
    "master_index = load_json_or_empty(MASTER_INDEX_PATH) or {}\n",
    "\n",
    "total = len(moa_master)\n",
    "print(f\"{total} MOA entries to process\")\n",
    "processed = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: one PubMed search per MOA\n",
    "# -----------------------------\n",
    "for moa_id, rec in moa_master.items():\n",
    "    # HASHES ARE PRESERVED — we use moa_id directly as filename and key\n",
    "    if moa_id in master_index:\n",
    "        mech = rec.get(\"mechanism\", \"\")\n",
    "        print(f\"{mech[:60]} || already processed\")\n",
    "        processed += 1\n",
    "        continue\n",
    "\n",
    "    mechanism = _clean(rec.get(\"mechanism\", \"\"))\n",
    "    if not mechanism:\n",
    "        print(f\"⚠️ Empty mechanism for moa_id={moa_id}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Normalized key (for index / dedup purposes, if needed later)\n",
    "    mech_key = norm_text(mechanism)\n",
    "\n",
    "    # Build candidate search terms from mechanism string\n",
    "    terms = split_terms(mechanism)\n",
    "    if not terms:\n",
    "        terms = [mechanism]\n",
    "\n",
    "    tried_terms: list[str] = []\n",
    "    matched_term: str | None = None\n",
    "    pmids: list[str] = []\n",
    "    records: dict = {}\n",
    "    llm_refined: str | None = None\n",
    "\n",
    "    # 1) First-pass: direct PubMed search using raw/split terms\n",
    "    for t in terms:\n",
    "        t_clean = _clean(t)\n",
    "        if not t_clean:\n",
    "            continue\n",
    "        tried_terms.append(t_clean)\n",
    "\n",
    "        query = f\"\\\"{t_clean}\\\"\"\n",
    "        try:\n",
    "            pmids = esearch_ids(query, n=5)\n",
    "        except Exception:\n",
    "            pmids = []\n",
    "\n",
    "        if pmids:\n",
    "            matched_term = t_clean\n",
    "            try:\n",
    "                records = efetch_details(pmids)\n",
    "            except Exception as e:\n",
    "                records = {\"_error\": str(e)}\n",
    "            break\n",
    "\n",
    "    # 2) If no hits, ask LLM to refine the mechanism and try again\n",
    "    if not pmids:\n",
    "        llm_refined = refine_mechanism_with_llm(mechanism)\n",
    "        if llm_refined:\n",
    "            tried_terms.append(llm_refined + \" [LLM]\")\n",
    "            query = f\"\\\"{llm_refined}\\\"\"\n",
    "            try:\n",
    "                pmids = esearch_ids(query, n=5)\n",
    "            except Exception:\n",
    "                pmids = []\n",
    "\n",
    "            if pmids:\n",
    "                matched_term = llm_refined\n",
    "                try:\n",
    "                    records = efetch_details(pmids)\n",
    "                except Exception as e:\n",
    "                    records = {\"_error\": str(e)}\n",
    "\n",
    "    # 3) If still no PMIDs, skip saving (so you can rerun later)\n",
    "    if not pmids:\n",
    "        print(f\"⚠️ No PubMed hits for moa_id={moa_id} after raw + LLM refinement, skipping\")\n",
    "        # Do NOT write any JSON or master entry\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # HASH-BASED OUTPUT\n",
    "    # -----------------------------\n",
    "    fname = f\"{moa_id}.json\"\n",
    "    out_path = OUT_DIR / fname\n",
    "\n",
    "    payload = {\n",
    "        \"type\": \"moa_pubmed_search\",\n",
    "        \"moa_id\": moa_id,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"tried_terms\": tried_terms,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "        \"match\": {\n",
    "            \"term\": matched_term,\n",
    "            \"pmids\": pmids,\n",
    "            \"records\": records,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    save_json(out_path, payload)\n",
    "\n",
    "    master_index[moa_id] = {\n",
    "        \"mechanism\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"json_path\": f\"{OUT_DIR.name}/{fname}\",\n",
    "        \"pmids\": pmids,\n",
    "        \"matched_term\": matched_term,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "    }\n",
    "    save_json(MASTER_INDEX_PATH, master_index)\n",
    "\n",
    "    processed += 1\n",
    "    if processed % 50 == 0:\n",
    "        print(f\"Processed {processed}/{total}…\")\n",
    "\n",
    "    time.sleep(SLEEP)\n",
    "\n",
    "save_json(MASTER_INDEX_PATH, master_index)\n",
    "print(f\"✅ Completed {processed} MOA entries with at least one PubMed hit. Files written to {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper  # your wrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "MOA_PUBMED_DIR        = BASE_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "MOA_CHOICE_DIR        = BASE_DIR / \"investigational_drug_moa_chosen\"\n",
    "MOA_CHOICE_LOG_DIR    = BASE_DIR / \"investigational_drug_moa_chosen_log\"\n",
    "MASTER_MOA_CHOICES_PATH = BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "\n",
    "MOA_CHOICE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MOA_CHOICE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS_MOA = 8\n",
    "MODEL = \"gpt-5\"   # adjust if needed\n",
    "\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "def load_master_moa_choices() -> dict:\n",
    "    if not MASTER_MOA_CHOICES_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_MOA_CHOICES_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def build_moa_mesh_prompt(moa_payload: dict, candidate_mesh_terms: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the LLM to choose the best MeSH term that represents the mechanism of action.\n",
    "    If no suitable MeSH term exists, the model MUST return \"[none]\".\n",
    "    \"\"\"\n",
    "\n",
    "    payload_json = json.dumps(moa_payload, ensure_ascii=False, indent=2)\n",
    "    mesh_json    = json.dumps(candidate_mesh_terms, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert pharmacologist and MeSH annotation specialist.\n",
    "\n",
    "You are given:\n",
    "1) A mechanism-of-action (MOA) text string describing how a drug works.\n",
    "2) A set of PubMed-derived MeSH terms (candidate list).\n",
    "3) Condensed PubMed records used for MOA search.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 1 — Select the Best MeSH Term\n",
    "------------------------------------------------------------\n",
    "Choose EXACTLY ONE MeSH term that best represents the mechanism of action.\n",
    "\n",
    "Rules:\n",
    "- You MUST select a term *only* from the candidate list.\n",
    "- Choose the most mechanistic/specific pharmacologic concept available\n",
    "  (e.g., \"Receptor Antagonists\", \"Antibodies, Monoclonal\", \"Kinase Inhibitors\").\n",
    "- Avoid generic terms (\"Humans\", \"Adult\", \"Neoplasms\") unless absolutely no mechanistic term exists.\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 2 — Handle Cases with No Good Mechanistic Term\n",
    "------------------------------------------------------------\n",
    "If NONE of the candidate MeSH terms meaningfully represent the MOA:\n",
    "\n",
    "You MUST output:\n",
    "\n",
    "  \"chosen_mesh_term\": \"[none]\",\n",
    "  \"source_pmid\": null,\n",
    "  \"rationale\": \"Explain why no term fits.\"\n",
    "\n",
    "This is a VALID and EXPECTED outcome.\n",
    "\n",
    "------------------------------------------------------------\n",
    "OUTPUT FORMAT  (STRICT)\n",
    "------------------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object with EXACTLY these fields:\n",
    "\n",
    "{{\n",
    "  \"chosen_mesh_term\": \"<one exact candidate term OR '[none]'>\",\n",
    "  \"source_pmid\": \"<PMID you relied on OR null>\",\n",
    "  \"rationale\": \"One concise sentence explaining your decision.\"\n",
    "}}\n",
    "\n",
    "Constraints:\n",
    "- If you choose a MeSH term, it MUST MATCH EXACTLY one item from the candidate list.\n",
    "- If no suitable term exists, return \"[none]\".\n",
    "- JSON must be valid and parseable.\n",
    "\n",
    "------------------------------------------------------------\n",
    "MOA Payload (input data)\n",
    "------------------------------------------------------------\n",
    "{payload_json}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Candidate MeSH Terms\n",
    "------------------------------------------------------------\n",
    "{mesh_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "master_moa_choices = load_master_moa_choices()\n",
    "master_moa_lock = threading.Lock()\n",
    "\n",
    "moa_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,   # includes \"chosen term not in JSON-derived list\"\n",
    "    \"no_candidates\": 0,\n",
    "}\n",
    "moa_counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_moa_file(fp: Path, idx: int, total: int) -> None:\n",
    "    \"\"\"Process a single MOA PubMed-search JSON file.\"\"\"\n",
    "    try:\n",
    "        payload = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] Error reading {fp.name}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    moa_id = payload.get(\"moa_id\") or fp.stem\n",
    "    mechanism = payload.get(\"mechanism\", \"\") or \"\"\n",
    "\n",
    "    if not moa_id:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing moa_id in {fp.name}, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = MOA_CHOICE_DIR / f\"{moa_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    match = payload.get(\"match\") or {}\n",
    "    records = match.get(\"records\") or {}\n",
    "    pmids = match.get(\"pmids\") or []\n",
    "\n",
    "    # Collect candidate MeSH terms (unique, in stable order) FROM THE JSON ONLY\n",
    "    candidate_terms = []\n",
    "    seen_terms = set()\n",
    "    for pmid, rec in records.items():\n",
    "        mesh_terms = rec.get(\"mesh_terms\") or []\n",
    "        for term in mesh_terms:\n",
    "            if term and term not in seen_terms:\n",
    "                seen_terms.add(term)\n",
    "                candidate_terms.append(term)\n",
    "\n",
    "    if not candidate_terms:\n",
    "        print(f\"⚠️ [{idx}/{total}] No candidate MeSH terms for moa_id={moa_id}, skipping\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"no_candidates\"] += 1\n",
    "        return\n",
    "\n",
    "    # Condensed payload for the model (avoid full abstracts to save tokens)\n",
    "    condensed_records = {\n",
    "        pmid: {\n",
    "            \"title\": (rec.get(\"title\") or \"\"),\n",
    "            \"mesh_terms\": (rec.get(\"mesh_terms\") or []),\n",
    "        }\n",
    "        for pmid, rec in records.items()\n",
    "    }\n",
    "\n",
    "    moa_payload = {\n",
    "        \"moa_id\": moa_id,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"tried_terms\": payload.get(\"tried_terms\") or [],\n",
    "        \"pmids\": pmids,\n",
    "        \"records\": condensed_records,\n",
    "    }\n",
    "\n",
    "    prompt = build_moa_mesh_prompt(moa_payload, candidate_terms)\n",
    "\n",
    "    token = moa_id\n",
    "    hash_id = moa_id\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for moa_id={moa_id}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    obj = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(obj, dict) or not obj:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse error moa_id={moa_id}, raw={text_response!r}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    chosen_term = obj.get(\"chosen_mesh_term\")\n",
    "    source_pmid = obj.get(\"source_pmid\")\n",
    "    rationale = obj.get(\"rationale\")\n",
    "\n",
    "    # HARD CHECK: chosen term\n",
    "    if not chosen_term or not isinstance(chosen_term, str):\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing or invalid chosen_mesh_term for moa_id={moa_id}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"coverage_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Special allowed sentinel for \"no good term\"\n",
    "    if chosen_term == \"[none]\":\n",
    "        # Accept even though it's not in candidate_terms\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "    else:\n",
    "        # For any real term, it MUST come from the JSON-derived candidate list\n",
    "        if chosen_term not in candidate_terms:\n",
    "            # DNE in JSON (hallucinated or modified term) → reject, do NOT save\n",
    "            print(\n",
    "                f\"⚠️ [{idx}/{total}] chosen_mesh_term not in JSON-derived candidate list \"\n",
    "                f\"for moa_id={moa_id}: {chosen_term!r}\"\n",
    "            )\n",
    "            with moa_counter_lock:\n",
    "                moa_counter[\"coverage_error\"] += 1\n",
    "            return\n",
    "\n",
    "        # Optional: source_pmid sanity check (must be one of pmids or None)\n",
    "        if source_pmid is not None and source_pmid not in pmids:\n",
    "            print(\n",
    "                f\"⚠️ [{idx}/{total}] source_pmid {source_pmid!r} not in pmids for moa_id={moa_id}; \"\n",
    "                f\"still accepting chosen_mesh_term\"\n",
    "            )\n",
    "\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "\n",
    "    # Save per-MOA JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (MOA_CHOICE_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_moa_lock:\n",
    "        master_moa_choices[moa_id] = mapped\n",
    "        MASTER_MOA_CHOICES_PATH.write_text(\n",
    "            json.dumps(master_moa_choices, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with moa_counter_lock:\n",
    "        moa_counter[\"processed\"] += 1\n",
    "        if moa_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {moa_counter['processed']} MOA entries...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY OVER MOA PUBMED SEARCH FILES\n",
    "# -------------------------------------------------\n",
    "moa_files = sorted(MOA_PUBMED_DIR.glob(\"*.json\"))\n",
    "total_moa = len(moa_files)\n",
    "print(f\"Loaded {total_moa} MOA PubMed-search files for MeSH-term selection.\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_MOA) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_moa_file, fp, idx, total_moa): fp.name\n",
    "        for idx, fp in enumerate(moa_files, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        name = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error (MOA MeSH selection) file={name}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"✅ MOA MeSH-term selection complete. \"\n",
    "    f\"processed={moa_counter['processed']}, \"\n",
    "    f\"skipped={moa_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={moa_counter['llm_error']}, \"\n",
    "    f\"parse_error={moa_counter['parse_error']}, \"\n",
    "    f\"coverage_error={moa_counter['coverage_error']}, \"\n",
    "    f\"no_candidates={moa_counter['no_candidates']}\"\n",
    ")\n",
    "print(f\"Chosen MOA directory: {MOA_CHOICE_DIR}\")\n",
    "print(f\"Log directory:        {MOA_CHOICE_LOG_DIR}\")\n",
    "print(f\"Master choices:       {MASTER_MOA_CHOICES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh\"\n",
    "OUT_DIR = Path(\"cache\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FILES = [\"desc2025.xml\", \"supp2025.xml\"]\n",
    "\n",
    "for fname in FILES:\n",
    "    url = f\"{BASE_URL}/{fname}\"\n",
    "    out_path = OUT_DIR / fname\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        print(f\"Skipping {fname}, already exists.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"⬇Downloading {url} -> {out_path}\")\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"✅ Downloaded {fname}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xml.etree.ElementTree as ET\n",
    "import html, unicodedata\n",
    "\n",
    "DESC_XML  = \"cache/desc2025.xml\"\n",
    "SUPP_XML  = \"cache/supp2025.xml\"\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\").replace(\"\\u2032\", \"'\").replace(\"\\u2033\", '\"')\n",
    "    t = t.replace(\"\\u201C\", '\"').replace(\"\\u201D\", '\"')\n",
    "    t = t.replace(\"\\u2010\", \"-\").replace(\"\\u2011\", \"-\").replace(\"\\u2012\", \"-\").replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    return \" \".join(t.strip().lower().split())\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Unicode-clean + collapse whitespace (preserve case).\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\").replace(\"\\u2032\", \"'\").replace(\"\\u2033\", '\"')\n",
    "    t = t.replace(\"\\u201C\", '\"').replace(\"\\u201D\", '\"')\n",
    "    t = t.replace(\"\\u2010\", \"-\").replace(\"\\u2011\", \"-\").replace(\"\\u2012\", \"-\").replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    return \" \".join(t.strip().split())\n",
    "\n",
    "def _dedup(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "def _extract_scope_note_from_descriptor(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Prefer the ScopeNote of the PreferredConcept (PreferredConceptYN='Y'),\n",
    "    else fall back to the first ScopeNote present under any Concept.\n",
    "    \"\"\"\n",
    "    # Preferred concept first\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    # Any concept scope note as fallback\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def _extract_scope_note_from_supp(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    For SCRs, ScopeNote can also live under Concept.\n",
    "    Prefer the PreferredConcept (if flagged), else the first available.\n",
    "    \"\"\"\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def load_mesh_tree_and_id(desc_xml_fp: str, supp_xml_fp: str) -> dict[str, dict[str, list[str] | str]]:\n",
    "    # term_map[normalized_term] = {\"mesh_id\": <UI>, \"tree_numbers\": [..], \"scope_note\": <str>}\n",
    "    term_map: dict[str, dict[str, list[str] | str]] = {}\n",
    "\n",
    "    # Helper maps for fallbacks/joins\n",
    "    heading_to_tree: dict[str, list[str]] = {}\n",
    "    ui_to_tree: dict[str, list[str]] = {}\n",
    "    heading_to_scope: dict[str, str] = {}\n",
    "    ui_to_scope: dict[str, str] = {}\n",
    "\n",
    "    # --- Descriptors ---\n",
    "    if os.path.exists(desc_xml_fp):\n",
    "        root = ET.parse(desc_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//DescriptorRecord\"):\n",
    "            desc_ui = (rec.findtext(\"DescriptorUI\") or \"\").strip()\n",
    "            tree_numbers = _dedup([tn.text.strip() for tn in rec.findall(\".//TreeNumberList/TreeNumber\") if tn.text])\n",
    "\n",
    "            heading_raw = rec.findtext(\"DescriptorName/String\")\n",
    "            heading_norm = norm(heading_raw) if heading_raw else \"\"\n",
    "            scope_note = _extract_scope_note_from_descriptor(rec)\n",
    "\n",
    "            if heading_norm:\n",
    "                heading_to_tree[heading_norm] = tree_numbers\n",
    "                heading_to_scope[heading_norm] = scope_note\n",
    "            if desc_ui:\n",
    "                ui_to_tree[desc_ui] = tree_numbers\n",
    "                ui_to_scope[desc_ui] = scope_note\n",
    "\n",
    "            # Collect all terms mapped to this descriptor\n",
    "            terms = set()\n",
    "            if heading_raw:\n",
    "                terms.add(heading_norm)\n",
    "            for concept in rec.findall(\".//Concept\"):\n",
    "                for term in concept.findall(\".//Term\"):\n",
    "                    s = term.findtext(\"String\")\n",
    "                    if s:\n",
    "                        terms.add(norm(s))\n",
    "\n",
    "            for term in terms:\n",
    "                term_map[term] = {\n",
    "                    \"mesh_id\": desc_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": scope_note\n",
    "                }\n",
    "\n",
    "    # --- Supplementary (SCRs) ---\n",
    "    if os.path.exists(supp_xml_fp):\n",
    "        root = ET.parse(supp_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//SupplementalRecord\"):\n",
    "            supp_ui = (rec.findtext(\"SupplementalRecordUI\") or \"\").strip()\n",
    "\n",
    "            # Collect ALL names for this SCR\n",
    "            names = set()\n",
    "\n",
    "            for s in rec.findall(\".//SupplementalRecordName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/ConceptName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/TermList/Term/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            # Direct trees (often none for SCRs)\n",
    "            tree_numbers = [tn.text.strip() for tn in rec.findall(\".//TreeNumberList/TreeNumber\") if tn.text]\n",
    "\n",
    "            # SCR scope note (preferred concept first)\n",
    "            scr_scope_note = _extract_scope_note_from_supp(rec)\n",
    "\n",
    "            # Fallback via HeadingMappedTo (names and UIs)\n",
    "            mapped_scope_note = \"\"\n",
    "            if not tree_numbers:\n",
    "                # Try mapped names\n",
    "                mapped_names = [n.text.strip() for n in rec.findall(\".//HeadingMappedTo/DescriptorReferredTo/DescriptorName/String\") if n is not None and n.text]\n",
    "                for m in mapped_names:\n",
    "                    m_norm = norm(m)\n",
    "                    tns = heading_to_tree.get(m_norm)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if not mapped_scope_note and m_norm in heading_to_scope and heading_to_scope[m_norm]:\n",
    "                        mapped_scope_note = heading_to_scope[m_norm]\n",
    "\n",
    "                # Try mapped UIs\n",
    "                mapped_uis = [u.text.strip().lstrip(\"*\") for u in rec.findall(\".//HeadingMappedTo/DescriptorReferredTo/DescriptorUI\") if u is not None and u.text]\n",
    "                for mui in mapped_uis:\n",
    "                    tns = ui_to_tree.get(mui)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if not mapped_scope_note and mui in ui_to_scope and ui_to_scope[mui]:\n",
    "                        mapped_scope_note = ui_to_scope[mui]\n",
    "\n",
    "                tree_numbers = _dedup(tree_numbers)\n",
    "\n",
    "            final_scope_note = scr_scope_note or mapped_scope_note or \"\"\n",
    "\n",
    "            for name in names:\n",
    "                # Keep Descriptor mapping if already present for same term\n",
    "                if name in term_map and str(term_map[name].get(\"mesh_id\", \"\")).startswith(\"D\"):\n",
    "                    continue\n",
    "                term_map[name] = {\n",
    "                    \"mesh_id\": supp_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": final_scope_note\n",
    "                }\n",
    "\n",
    "    return term_map\n",
    "\n",
    "# -------- Run --------\n",
    "TREE_INDEX = load_mesh_tree_and_id(DESC_XML, SUPP_XML)\n",
    "print(f\"✅ Loaded MeSH index terms: {len(TREE_INDEX):,} unique normalized terms\")\n",
    "\n",
    "# Quick checks\n",
    "for q in [\"sotatercept\", \"ACE-011\", \"winrevair\"]:\n",
    "    k = norm(q)\n",
    "    print(q, \"→\", TREE_INDEX.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "TRIALS_IN_PATH  = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "TRIALS_OUT_PATH = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "\n",
    "MASTER_MOA_CHOICES_PATH = BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# Sanity: TREE_INDEX and norm must already be loaded\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    TREE_INDEX\n",
    "except NameError:\n",
    "    raise RuntimeError(\"TREE_INDEX is not defined — run the MeSH loader cell first.\")\n",
    "\n",
    "try:\n",
    "    norm\n",
    "except NameError:\n",
    "    raise RuntimeError(\"norm() is not defined — ensure it is defined in the MeSH loader cell.\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Load MOA → MeSH choices\n",
    "# -----------------------------------------\n",
    "def load_master_moa_choices() -> dict:\n",
    "    if not MASTER_MOA_CHOICES_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_MOA_CHOICES_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "master_moa_choices = load_master_moa_choices()\n",
    "\n",
    "# Map from mechanism string → choice record\n",
    "# (mechanism strings may be either true MOAs or drug names like \"narfurine hydrochloride\")\n",
    "mechanism_to_choice: dict[str, dict] = {}\n",
    "for moa_id, rec in master_moa_choices.items():\n",
    "    mech = rec.get(\"mechanism\")\n",
    "    if mech:\n",
    "        # first occurrence wins, keep deterministic mapping\n",
    "        mechanism_to_choice.setdefault(mech, rec)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------\n",
    "def parse_listish(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def mesh_heading_to_tree_numbers(chosen_mesh_term: str):\n",
    "    \"\"\"\n",
    "    Return *all* tree numbers for the chosen MeSH heading.\n",
    "    \"\"\"\n",
    "    if not chosen_mesh_term or chosen_mesh_term == \"[none]\":\n",
    "        return []\n",
    "    base = chosen_mesh_term.split(\" / \")[0].replace(\"*\", \"\").strip()\n",
    "    key = norm(base)\n",
    "    info = TREE_INDEX.get(key)\n",
    "    if not info:\n",
    "        return []\n",
    "    return info.get(\"tree_numbers\", []) or []\n",
    "\n",
    "\n",
    "# Clinical-pharmacology-ish heuristic for ONE primary tree number\n",
    "PRIORITY_PREFIXES = [\n",
    "    \"D12.\",  # Proteins: receptors, enzymes, cytokines, antibodies (biologics / targets)\n",
    "    \"D27.\",  # Chemical Actions and Uses: classic pharmacologic classes\n",
    "    \"D02.\",  # Organic Chemicals: small-molecule drugs\n",
    "    \"D09.\",  # Carbohydrates\n",
    "    \"D23.\",  # Immunologic Factors\n",
    "    \"D26.\",  # Biological Factors\n",
    "]\n",
    "\n",
    "def _depth(tn: str) -> int:\n",
    "    # fewer segments = higher-level class\n",
    "    return len(tn.split(\".\"))\n",
    "\n",
    "def choose_primary_tree(tree_numbers):\n",
    "    \"\"\"\n",
    "    Given a list of tree numbers for ONE MeSH term,\n",
    "    choose a single 'primary' tree that best reflects the\n",
    "    pharmacologic / target-level concept.\n",
    "    \"\"\"\n",
    "    if not tree_numbers:\n",
    "        return \"\"\n",
    "\n",
    "    tns = [t.strip() for t in tree_numbers if isinstance(t, str) and t.strip()]\n",
    "    if not tns:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Prefer specific high-value branches (by prefix)\n",
    "    for prefix in PRIORITY_PREFIXES:\n",
    "        candidates = [t for t in tns if t.startswith(prefix)]\n",
    "        if candidates:\n",
    "            # choose the highest-level (shortest depth) node in that branch\n",
    "            return max(candidates, key=_depth)\n",
    "\n",
    "    # 2) Else prefer any Chemicals & Drugs branch (D*)\n",
    "    d_candidates = [t for t in tns if t.startswith(\"D\")]\n",
    "    if d_candidates:\n",
    "        return max(d_candidates, key=_depth)\n",
    "\n",
    "    # 3) Fallback: shortest overall\n",
    "    return max(tns, key=_depth)\n",
    "\n",
    "\n",
    "def map_mechanism_list(mech_list, fallback_terms=None):\n",
    "    \"\"\"\n",
    "    For a list of MOA strings (mech_list):\n",
    "\n",
    "    - If mech_list[i] is non-empty and not \"[none]\", map using that mechanism string.\n",
    "    - If mech_list[i] is empty / \"[none]\" and fallback_terms is provided, then:\n",
    "        * use fallback_terms[i] (e.g., the investigational product name)\n",
    "          as the key into mechanism_to_choice, if present.\n",
    "\n",
    "    Returns:\n",
    "      mapped_terms   : list of MeSH headings (one per mechanism, or \"\")\n",
    "      all_tree_lists : list of [list-of-tree-numbers] per mechanism\n",
    "      primary_trees  : list of ONE chosen tree number per mechanism (\"\" if none)\n",
    "    \"\"\"\n",
    "    mapped_terms   = []\n",
    "    all_tree_lists = []\n",
    "    primary_trees  = []\n",
    "\n",
    "    fallback_terms = fallback_terms or []\n",
    "\n",
    "    for idx, mech in enumerate(mech_list):\n",
    "        mech_str = (mech or \"\").strip()\n",
    "\n",
    "        # If mechanism is missing / \"[none]\", try fallback = investigational product name\n",
    "        if (not mech_str or mech_str == \"[none]\") and fallback_terms:\n",
    "            fb = (fallback_terms[idx] if idx < len(fallback_terms) else \"\") or \"\"\n",
    "            fb = fb.strip()\n",
    "            if fb:\n",
    "                mech_str = fb\n",
    "\n",
    "        # If still nothing, bail on this slot\n",
    "        if not mech_str:\n",
    "            mapped_terms.append(\"\")\n",
    "            all_tree_lists.append([])\n",
    "            primary_trees.append(\"\")\n",
    "            continue\n",
    "\n",
    "        choice = mechanism_to_choice.get(mech_str)\n",
    "        if not choice:\n",
    "            # No mapping found in master MOA choices\n",
    "            mapped_terms.append(\"\")\n",
    "            all_tree_lists.append([])\n",
    "            primary_trees.append(\"\")\n",
    "            continue\n",
    "\n",
    "        chosen = choice.get(\"chosen_mesh_term\") or \"\"\n",
    "        mapped_terms.append(chosen)\n",
    "\n",
    "        all_trees = mesh_heading_to_tree_numbers(chosen)\n",
    "        all_tree_lists.append(all_trees)\n",
    "\n",
    "        primary = choose_primary_tree(all_trees)\n",
    "        primary_trees.append(primary)\n",
    "\n",
    "    return mapped_terms, all_tree_lists, primary_trees\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Load trial dataset\n",
    "# -----------------------------------------\n",
    "df = pd.read_csv(TRIALS_IN_PATH)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Compute mapped columns row-wise\n",
    "# -----------------------------------------\n",
    "inv_mapped = []\n",
    "inv_trees_all  = []\n",
    "inv_trees_primary = []\n",
    "\n",
    "ac_mapped  = []\n",
    "ac_trees_all   = []\n",
    "ac_trees_primary = []\n",
    "\n",
    "soc_mapped = []\n",
    "soc_trees_all  = []\n",
    "soc_trees_primary = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Parse mechanisms\n",
    "    inv_mechs = parse_listish(row.get(\"investigational_products_mechanism\"))\n",
    "    ac_mechs  = parse_listish(row.get(\"active_comparators_mechanism\"))\n",
    "    soc_mechs = parse_listish(row.get(\"standard_of_care_mechanism\"))\n",
    "\n",
    "    # Parse investigational product names for fallback\n",
    "    inv_products = parse_listish(row.get(\"investigational_products\"))\n",
    "\n",
    "    # Investigational products: use mechanism, but if empty, fall back to product name\n",
    "    inv_m, inv_all_t, inv_primary_t = map_mechanism_list(inv_mechs, fallback_terms=inv_products)\n",
    "    inv_mapped.append(inv_m)\n",
    "    inv_trees_all.append(inv_all_t)\n",
    "    inv_trees_primary.append(inv_primary_t)\n",
    "\n",
    "    # Active comparators: no fallback requested → use mechanisms directly\n",
    "    ac_m,  ac_all_t,  ac_primary_t  = map_mechanism_list(ac_mechs, fallback_terms=None)\n",
    "    ac_mapped.append(ac_m)\n",
    "    ac_trees_all.append(ac_all_t)\n",
    "    ac_trees_primary.append(ac_primary_t)\n",
    "\n",
    "    # Standard of care: no fallback requested → use mechanisms directly\n",
    "    soc_m, soc_all_t, soc_primary_t = map_mechanism_list(soc_mechs, fallback_terms=None)\n",
    "    soc_mapped.append(soc_m)\n",
    "    soc_trees_all.append(soc_all_t)\n",
    "    soc_trees_primary.append(soc_primary_t)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Insert columns next to original mechanism columns (logical order)\n",
    "# -----------------------------------------\n",
    "def insert_after(df, col, newcol, values):\n",
    "    cols = list(df.columns)\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "# Investigational products\n",
    "insert_after(df,\n",
    "             \"investigational_products_mechanism\",\n",
    "             \"investigational_products_mechanism_mapped\",\n",
    "             inv_mapped)\n",
    "\n",
    "insert_after(df,\n",
    "             \"investigational_products_mechanism_mapped\",\n",
    "             \"investigational_products_mechanism_tree_numbers\",\n",
    "             inv_trees_all)\n",
    "\n",
    "insert_after(df,\n",
    "             \"investigational_products_mechanism_tree_numbers\",\n",
    "             \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "             inv_trees_primary)\n",
    "\n",
    "# Active comparators\n",
    "insert_after(df,\n",
    "             \"active_comparators_mechanism\",\n",
    "             \"active_comparators_mechanism_mapped\",\n",
    "             ac_mapped)\n",
    "\n",
    "insert_after(df,\n",
    "             \"active_comparators_mechanism_mapped\",\n",
    "             \"active_comparators_mechanism_tree_numbers\",\n",
    "             ac_trees_all)\n",
    "\n",
    "insert_after(df,\n",
    "             \"active_comparators_mechanism_tree_numbers\",\n",
    "             \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "             ac_trees_primary)\n",
    "\n",
    "# Standard of care\n",
    "insert_after(df,\n",
    "             \"standard_of_care_mechanism\",\n",
    "             \"standard_of_care_mechanism_mapped\",\n",
    "             soc_mapped)\n",
    "\n",
    "insert_after(df,\n",
    "             \"standard_of_care_mechanism_mapped\",\n",
    "             \"standard_of_care_mechanism_tree_numbers\",\n",
    "             soc_trees_all)\n",
    "\n",
    "insert_after(df,\n",
    "             \"standard_of_care_mechanism_tree_numbers\",\n",
    "             \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "             soc_trees_primary)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Save output\n",
    "# -----------------------------------------\n",
    "df.to_csv(TRIALS_OUT_PATH, index=False)\n",
    "print(f\"✅ Wrote: {TRIALS_OUT_PATH}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e09af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Columns with mapped MeSH terms\n",
    "MESH_TERM_COLS = [\n",
    "    \"investigational_products_mechanism_mapped\",\n",
    "    \"active_comparators_mechanism_mapped\",\n",
    "    \"standard_of_care_mechanism_mapped\",\n",
    "]\n",
    "\n",
    "# Columns with *primary* tree numbers (one tree per mechanism)\n",
    "TREE_COLS = [\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "]\n",
    "\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        return val if isinstance(val, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "tree_to_mesh_terms = defaultdict(list)\n",
    "pair_counter = Counter()   # (mesh_term, primary_tree_number) → count\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Parse lists for each mechanism category\n",
    "    mesh_term_lists = [parse_list(row[c]) for c in MESH_TERM_COLS]\n",
    "    tree_number_lists = [parse_list(row[c]) for c in TREE_COLS]\n",
    "\n",
    "    # Iterate over the three mechanism categories in parallel\n",
    "    for mesh_terms, tree_nums in zip(mesh_term_lists, tree_number_lists):\n",
    "        # mesh_terms: e.g. [\"Antibodies, Bispecific* / pharmacology; immunology\", \"[none]\", ...]\n",
    "        # tree_nums:  e.g. [\"D12.776.124.486.485.114.125\", \"\", ...]\n",
    "        for mesh_term, primary_tn in zip(mesh_terms, tree_nums):\n",
    "            # Skip unusable entries\n",
    "            if not mesh_term or mesh_term == \"[none]\":\n",
    "                continue\n",
    "            if not isinstance(primary_tn, str) or not primary_tn.strip():\n",
    "                continue\n",
    "\n",
    "            tn = primary_tn.strip()\n",
    "            pair_counter[(mesh_term, tn)] += 1\n",
    "            tree_to_mesh_terms[tn].append(mesh_term)\n",
    "\n",
    "# Convert to DataFrame\n",
    "out_rows = [\n",
    "    {\n",
    "        \"mesh_term\": mesh_term,\n",
    "        \"tree_number\": tree_num,\n",
    "        \"count\": count,\n",
    "    }\n",
    "    for (mesh_term, tree_num), count in pair_counter.items()\n",
    "]\n",
    "\n",
    "out_df = pd.DataFrame(out_rows).sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Saved tree number + term breakdown → {OUTPUT_PATH}\")\n",
    "print(\"Top 20 combinations:\")\n",
    "print(out_df.head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Define super-group labels (5 buckets)\n",
    "# -------------------------------------------------------------------\n",
    "G1 = \"hematopoietic_growth_factor_esa\"\n",
    "G2 = \"antibody_or_bispecific_therapy\"\n",
    "G3 = \"classic_cytotoxic_or_antimetabolite\"\n",
    "G4 = \"small_molecule_immunomodulator\"\n",
    "G5 = \"metabolic_enzyme_endocrine_other\"\n",
    "\n",
    "\n",
    "def classify_super_group(mesh_term: str, tree_number: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic mapping of mesh_term + tree_number to one of 5 MOA super-groups.\n",
    "    Think like a clinical pharmacologist, but keep it deterministic and simple.\n",
    "    \"\"\"\n",
    "    t = (mesh_term or \"\").lower()\n",
    "    tn = (tree_number or \"\").strip()\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 1: Hematopoietic growth factors / ESAs\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"erythropoietin\", \"thrombopoietin\", \"epo\", \"tpo\", \"hematopoietic\"\n",
    "    ]):\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.150\") or tn.startswith(\"D12.776.543.750.705.852.610\"):\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.644.276.374.410.240.150\") or tn.startswith(\"D12.644.276.374.410.240.750\"):\n",
    "        return G1\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 2: Antibodies & bispecifics (checkpoint, cytokine, receptor targeting)\n",
    "    # -----------------------\n",
    "    # Obvious antibody keywords\n",
    "    if \"antibod\" in t or \"immunoconjugate\" in t or \"chimeric antigen\" in t or \"cancer vaccines\" in t:\n",
    "        return G2\n",
    "    # MeSH branches that are essentially antibody land or fusion proteins\n",
    "    if tn.startswith(\"D12.776.124.486.485.114\") or tn.startswith(\"D12.776.124.790.651.114\"):\n",
    "        return G2\n",
    "    # Checkpoint / cytokine biology where in practice these are nearly always mAbs / fusion biologics\n",
    "    if any(kw in t for kw in [\n",
    "        \"programmed cell death 1 receptor\", \"pd-1\", \"pd1\",\n",
    "        \"pd-l1\", \"pdl1\",\n",
    "        \"erbB-2\", \"erbb2\",\n",
    "        \"tumor necrosis factor\", \"tnf\",\n",
    "        \"interleukin-4 receptor\", \"interleukin-5\", \"interleukin-17\",\n",
    "        \"cd47 antigen\", \"lectins, c-type\"\n",
    "    ]):\n",
    "        return G2\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 3: Classic cytotoxic / antimetabolite oncology drugs\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"antimetabolites, antineoplastic\",\n",
    "        \"antineoplastic agents, alkylating\",\n",
    "        \"topoisomerase i inhibitors\",\n",
    "        \"topoisomerase ii inhibitors\",\n",
    "        \"vinca alkaloids\",\n",
    "        \"vinblastine\",\n",
    "        \"taxoids\",\n",
    "        \"paclitaxel\",\n",
    "        \"fluorouracil\",\n",
    "        \"phosphoramide mustards\",\n",
    "        \"tubulin modulators\"\n",
    "    ]):\n",
    "        return G3\n",
    "    # If the tree number clearly sits in classic chemo small-molecule branches\n",
    "    if tn.startswith(\"D27.505.519.186\") or tn.startswith(\"D27.505.519.124\"):\n",
    "        return G3\n",
    "    if tn.startswith(\"D02.455.526.728\") or tn.startswith(\"D03.633.100.496.500.500.681.827\"):\n",
    "        return G3\n",
    "    if tn.startswith(\"D01.710\"):  # platinum compounds\n",
    "        return G3\n",
    "    if tn.startswith(\"D27.505.519.593.249.500\"):  # tubulin modulators\n",
    "        return G3\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 4: Small-molecule immunomodulators / signaling modifiers\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"calcineurin inhibitors\",\n",
    "        \"janus kinase inhibitors\",\n",
    "        \"immunosuppressive agents\",\n",
    "        \"glucocorticoids\",\n",
    "        \"adrenal cortex hormones\",\n",
    "        \"tor serine-threonine kinases\",\n",
    "        \"hypoxia-inducible factor 1\",\n",
    "        \"hypoxia-inducible factor-proline dioxygenases\",\n",
    "        \"interleukin-2\",\n",
    "    ]):\n",
    "        return G4\n",
    "    # HIF pathway signaling\n",
    "    if tn.startswith(\"D12.776.260.103.625\") or tn.startswith(\"D08.811.682.690.416.617.500\"):\n",
    "        return G4\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 5: Metabolic / enzyme / endocrine / other\n",
    "    # -----------------------\n",
    "    # Metabolic / enzyme / endocrine-type keywords\n",
    "    if any(kw in t for kw in [\n",
    "        \"biguanides\",\n",
    "        \"urate oxidase\",\n",
    "        \"phosphodiesterase 4 inhibitors\",\n",
    "        \"histamine h1 antagonists\",\n",
    "        \"che lating agents\",\n",
    "        \"chelating agents\",\n",
    "        \"heparin\",\n",
    "        \"antimalarials\",\n",
    "        \"imp dehydrogenase\",\n",
    "        \"thymidine phosphorylase\",\n",
    "        \"thymidylate synthase\",\n",
    "        \"leucovorin\",\n",
    "        \"cross-linking reagents\",\n",
    "        \"membrane glycoproteins\"\n",
    "    ]):\n",
    "        return G5\n",
    "    # Heuristic: if it's in D27.505.* but we didn't classify as chemo or immunomod,\n",
    "    # it's likely a metabolic / enzyme / other chemical agent\n",
    "    if tn.startswith(\"D27.505.\"):\n",
    "        return G5\n",
    "\n",
    "    # Default fallback\n",
    "    return G5\n",
    "\n",
    "\n",
    "# Apply classifier\n",
    "out_df = df[[\"mesh_term\", \"tree_number\"]].copy()\n",
    "out_df[\"mechanism_super_group\"] = [\n",
    "    classify_super_group(m, tn) for m, tn in zip(out_df[\"mesh_term\"], out_df[\"tree_number\"])\n",
    "]\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"✅ Saved mechanism super-group mapping → {OUTPUT_PATH}\")\n",
    "print(out_df.head(20).to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "MAP_PATH      = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "TRIALS_IN     = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "TRIALS_OUT    = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load mapping: (mesh_term, tree_number) → super_group\n",
    "# ---------------------------------------------------\n",
    "map_df = pd.read_csv(MAP_PATH)\n",
    "\n",
    "# Normalize tree_number a bit (strip whitespace)\n",
    "map_df[\"tree_number\"] = map_df[\"tree_number\"].astype(str).str.strip()\n",
    "\n",
    "mapping = {\n",
    "    (str(row[\"mesh_term\"]), str(row[\"tree_number\"])): row[\"mechanism_super_group\"]\n",
    "    for _, row in map_df.iterrows()\n",
    "}\n",
    "\n",
    "print(f\"Loaded {len(mapping):,} (mesh_term, tree_number) → super_group mappings\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings (e.g. \"['a','b']\") into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        return []\n",
    "    # If it's a single scalar, wrap in list\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def build_super_group_list(mech_terms, mech_trees):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      mech_terms : list of MeSH headings (strings)\n",
    "      mech_trees : list of single tree numbers (strings)\n",
    "    Return:\n",
    "      list of mechanism_super_group strings (same length).\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for term, tn in zip(mech_terms, mech_trees):\n",
    "        term = (term or \"\").strip()\n",
    "        tn   = (tn or \"\").strip()\n",
    "        if not term or term == \"[none]\" or not tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        key = (term, tn)\n",
    "        out.append(mapping.get(key, \"\"))  # \"\" if not found\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_super_group_list_with_fallback(\n",
    "    mech_terms,\n",
    "    mech_trees,\n",
    "    fallback_terms,\n",
    "    fallback_trees,\n",
    "):\n",
    "    \"\"\"\n",
    "    For investigational products:\n",
    "    - First try mechanism-based MeSH mapping (mech_terms/mech_trees).\n",
    "    - If the mechanism term is missing / '[none]' / empty, fall back to\n",
    "      the investigational product MeSH mapping (fallback_terms/fallback_trees).\n",
    "    \"\"\"\n",
    "    # Ensure all are lists\n",
    "    mech_terms   = mech_terms or []\n",
    "    mech_trees   = mech_trees or []\n",
    "    fallback_terms = fallback_terms or []\n",
    "    fallback_trees = fallback_trees or []\n",
    "\n",
    "    n = max(len(mech_terms), len(mech_trees), len(fallback_terms), len(fallback_trees))\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Primary (mechanism-based) term/tn\n",
    "        term_mech = (mech_terms[i] if i < len(mech_terms) else \"\") or \"\"\n",
    "        tn_mech   = (mech_trees[i] if i < len(mech_trees) else \"\") or \"\"\n",
    "\n",
    "        term_mech = term_mech.strip()\n",
    "        tn_mech   = tn_mech.strip()\n",
    "\n",
    "        # Fallback (drug-based) term/tn\n",
    "        term_fb = (fallback_terms[i] if i < len(fallback_terms) else \"\") or \"\"\n",
    "        tn_fb   = (fallback_trees[i] if i < len(fallback_trees) else \"\") or \"\"\n",
    "\n",
    "        term_fb = term_fb.strip()\n",
    "        tn_fb   = tn_fb.strip()\n",
    "\n",
    "        # Decide which to use\n",
    "        chosen_term = None\n",
    "        chosen_tn   = None\n",
    "\n",
    "        # 1) Use mechanism term if present and not [none]\n",
    "        if term_mech and term_mech != \"[none]\" and tn_mech:\n",
    "            chosen_term = term_mech\n",
    "            chosen_tn   = tn_mech\n",
    "        # 2) Else fall back to investigational product MeSH term\n",
    "        elif term_fb and term_fb != \"[none]\" and tn_fb:\n",
    "            chosen_term = term_fb\n",
    "            chosen_tn   = tn_fb\n",
    "\n",
    "        if not chosen_term or not chosen_tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "\n",
    "        key = (chosen_term, chosen_tn)\n",
    "        out.append(mapping.get(key, \"\"))  # \"\" if no mapping\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load trials and build super-group columns\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_csv(TRIALS_IN)\n",
    "\n",
    "inv_sg_list = []\n",
    "ac_sg_list  = []\n",
    "soc_sg_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # -----------------------------\n",
    "    # INVESTIGATIONAL PRODUCTS\n",
    "    # -----------------------------\n",
    "    # Mechanism-based mapping\n",
    "    inv_mech_terms = parse_list(row.get(\"investigational_products_mechanism_mapped\"))\n",
    "    inv_mech_tn    = parse_list(row.get(\"investigational_products_mechanism_primary_tree_numbers\"))\n",
    "\n",
    "    # Fallback: investigational product MeSH mapping\n",
    "    inv_prod_terms = parse_list(row.get(\"investigational_products_mapped\"))\n",
    "    inv_prod_tn    = parse_list(row.get(\"investigational_products_primary_tree_numbers\"))\n",
    "\n",
    "    inv_sg_list.append(\n",
    "        build_super_group_list_with_fallback(\n",
    "            inv_mech_terms,\n",
    "            inv_mech_tn,\n",
    "            inv_prod_terms,\n",
    "            inv_prod_tn,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # ACTIVE COMPARATORS\n",
    "    # (no fallback requested, so keep as-is)\n",
    "    # -----------------------------\n",
    "    ac_terms = parse_list(row.get(\"active_comparators_mechanism_mapped\"))\n",
    "    ac_tn    = parse_list(row.get(\"active_comparators_mechanism_primary_tree_numbers\"))\n",
    "    ac_sg_list.append(build_super_group_list(ac_terms, ac_tn))\n",
    "\n",
    "    # -----------------------------\n",
    "    # STANDARD OF CARE\n",
    "    # (no fallback requested, so keep as-is)\n",
    "    # -----------------------------\n",
    "    soc_terms = parse_list(row.get(\"standard_of_care_mechanism_mapped\"))\n",
    "    soc_tn    = parse_list(row.get(\"standard_of_care_mechanism_primary_tree_numbers\"))\n",
    "    soc_sg_list.append(build_super_group_list(soc_terms, soc_tn))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Insert new columns next to the primary_tree_numbers\n",
    "# ---------------------------------------------------\n",
    "def insert_after(df, col, newcol, values):\n",
    "    cols = list(df.columns)\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"investigational_products_mechanism_super_group\",\n",
    "    inv_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_super_group\",\n",
    "    ac_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_super_group\",\n",
    "    soc_sg_list,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Save final CSV\n",
    "# ---------------------------------------------------\n",
    "df.to_csv(TRIALS_OUT, index=False)\n",
    "print(f\"✅ Wrote final trial-level file with super-groups → {TRIALS_OUT}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_super_group_distribution.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(x)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "super_group_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inv_sg_list = parse_list(row.get(\"investigational_products_mechanism_super_group\"))\n",
    "    soc_sg_list = parse_list(row.get(\"standard_of_care_mechanism_super_group\"))\n",
    "\n",
    "    chosen = \"\"\n",
    "\n",
    "    # Priority 1 — investigational product supergroup\n",
    "    if inv_sg_list and isinstance(inv_sg_list, list) and inv_sg_list[0].strip():\n",
    "        chosen = inv_sg_list[0].strip()\n",
    "    # Priority 2 — fallback to SOC\n",
    "    elif soc_sg_list and isinstance(soc_sg_list, list) and soc_sg_list[0].strip():\n",
    "        chosen = soc_sg_list[0].strip()\n",
    "    else:\n",
    "        chosen = \"\"  # nothing found\n",
    "\n",
    "    super_group_list.append(chosen)\n",
    "\n",
    "# Count distribution\n",
    "dist = Counter(super_group_list)\n",
    "\n",
    "# Convert to DataFrame\n",
    "dist_df = pd.DataFrame(\n",
    "    [{\"mechanism_super_group\": sg, \"count\": count}\n",
    "     for sg, count in dist.items() if sg]  # exclude empty\n",
    ").sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Save\n",
    "dist_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"✅ Saved distribution → {OUTPUT_PATH}\")\n",
    "print(\"Top categories:\\n\")\n",
    "print(dist_df.head(20).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deae989",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e05ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "CLASS_PATH   = BASE_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "MECH_PATH    = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "OUTPUT_PATH  = BASE_DIR / \"trial_results_table.csv\"\n",
    "\n",
    "# ---------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def get_first_or_blank(x):\n",
    "    lst = parse_listish(x)\n",
    "    return lst[0] if lst else \"\"\n",
    "\n",
    "# ---------------------------------\n",
    "# Load inputs\n",
    "# ---------------------------------\n",
    "df_class = pd.read_csv(CLASS_PATH)\n",
    "df_mech  = pd.read_csv(MECH_PATH)\n",
    "\n",
    "# ---------------------------------\n",
    "# Prepare classification info\n",
    "# ---------------------------------\n",
    "df_class[\"drug_name\"]   = df_class[\"investigational_products\"].apply(get_first_or_blank)\n",
    "df_class[\"innovation\"]  = df_class[\"investigational_products_classifications\"].apply(get_first_or_blank)\n",
    "\n",
    "# If you want to strictly limit to \"Innovative\" / \"Biosimilar\" and blank out others:\n",
    "def normalize_innovation(val: str) -> str:\n",
    "    v = (val or \"\").strip()\n",
    "    if v in {\"Innovative\", \"Biosimilar\"}:\n",
    "        return v\n",
    "    return v  # or return \"\" if you want to hide \"Generic\"\n",
    "\n",
    "df_class[\"innovation\"] = df_class[\"innovation\"].apply(normalize_innovation)\n",
    "\n",
    "# ---------------------------------\n",
    "# Prepare mechanism / category info\n",
    "# ---------------------------------\n",
    "# MOA: first investigational mechanism text\n",
    "df_mech[\"moa\"] = df_mech[\"investigational_products_mechanism\"].apply(get_first_or_blank)\n",
    "\n",
    "# Category: first non-empty investigational super-group;\n",
    "# if empty, fall back to first non-empty SOC super-group\n",
    "def pick_category(row):\n",
    "    inv_list = parse_listish(row.get(\"investigational_products_mechanism_super_group\"))\n",
    "    soc_list = parse_listish(row.get(\"standard_of_care_mechanism_super_group\"))\n",
    "    for val in inv_list:\n",
    "        if val:\n",
    "            return val\n",
    "    for val in soc_list:\n",
    "        if val:\n",
    "            return val\n",
    "    return \"\"\n",
    "\n",
    "df_mech[\"category\"] = df_mech.apply(pick_category, axis=1)\n",
    "\n",
    "# For convenience, also extract first drug name from the mech file (to sanity-check alignment)\n",
    "df_mech[\"drug_name_mech\"] = df_mech[\"investigational_products\"].apply(get_first_or_blank)\n",
    "\n",
    "# ---------------------------------\n",
    "# Merge on trial_hash\n",
    "# ---------------------------------\n",
    "merged = pd.merge(\n",
    "    df_mech[[\"trial_hash\", \"moa\", \"category\"]],\n",
    "    df_class[[\"trial_hash\", \"drug_name\", \"innovation\"]],\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# ---------------------------------\n",
    "# Build final table\n",
    "# ---------------------------------\n",
    "# NOTE: we don't have a title column here, so we use trial_hash as a proxy.\n",
    "# If you have a separate trials file with titles, you can join it here and replace trial_hash.\n",
    "merged[\"trial_title\"] = merged[\"trial_hash\"]\n",
    "\n",
    "results = merged[[\"trial_title\", \"drug_name\", \"moa\", \"innovation\", \"category\"]].copy()\n",
    "\n",
    "# Save\n",
    "results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"✅ Saved results table → {OUTPUT_PATH}\")\n",
    "print(results.head(20).to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Find rows with missing values\n",
    "# ---------------------------------\n",
    "\n",
    "# Treat \"\" as missing for easier filtering\n",
    "cols_to_check = [\"trial_title\", \"drug_name\", \"moa\", \"innovation\", \"category\"]\n",
    "\n",
    "def is_missing(x):\n",
    "    return (pd.isna(x)) or (str(x).strip() == \"\")\n",
    "\n",
    "mask_missing = results[cols_to_check].applymap(is_missing).any(axis=1)\n",
    "\n",
    "missing_rows = results[mask_missing].copy()\n",
    "\n",
    "print(f\"Found {len(missing_rows)} rows with at least one missing value.\")\n",
    "\n",
    "# Save for debugging\n",
    "MISSING_OUTPUT_PATH = BASE_DIR / \"trial_results_table_missing_rows.csv\"\n",
    "missing_rows.to_csv(MISSING_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"❗ Missing rows saved to → {MISSING_OUTPUT_PATH}\")\n",
    "print(missing_rows.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a338a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
