{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff4ae34",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97784b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load raw data from provided CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"data/raw_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e73a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69802af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                 |   0 |\n",
      "|:--------------------------------|----:|\n",
      "| title                           |   0 |\n",
      "| objective                       |   3 |\n",
      "| outcome_details                 | 146 |\n",
      "| phase                           |   0 |\n",
      "| primary_completion_date         |  61 |\n",
      "| primary_endpoints_reported_date | 161 |\n",
      "| prior_concurrent_therapy        | 184 |\n",
      "| start_date                      |  45 |\n",
      "| study_design                    |  16 |\n",
      "| treatment_plan                  |   1 |\n",
      "| record_type                     |   0 |\n",
      "| patients_per_site_per_month     | 119 |\n",
      "| primary_endpoint_json           |   0 |\n",
      "| other_endpoint_json             |   0 |\n",
      "| associated_cro_json             |   0 |\n",
      "| notes_json                      |   0 |\n",
      "| outcomes_json                   |   0 |\n",
      "| patient_dispositions_json       |   0 |\n",
      "| results_json                    |   0 |\n",
      "| study_keywords_json             |   0 |\n",
      "| tags_json                       |   0 |\n",
      "| primary_drugs_tested_json       |   0 |\n",
      "| other_drugs_tested_json         |   0 |\n",
      "| therapeutic_areas_json          |   0 |\n",
      "| bmt_other_drugs_tested_json     |   0 |\n",
      "| bmt_primary_drugs_tested_json   |   0 |\n",
      "| ct_gov_listed_locations_json    |   0 |\n",
      "| ct_gov_mesh_terms_json          |   0 |\n",
      "Shape: (184, 28)\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum().to_markdown())\n",
    "print(\"Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6897de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw trials from: data/raw_trials.csv\n",
      "Generating trial_hash values ...\n",
      "Data columns: Index(['trial_hash', 'title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n",
      "Data shape: (184, 29)\n",
      "Saved to cache/data_preprocess/raw_trials_with_hash.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate a unique, deterministic trial hash for each clinical trial and save an\n",
    "augmented CSV.\n",
    "\n",
    "Inputs:\n",
    "- CSV file: data/raw_trials.csv\n",
    "    Must contain at least:\n",
    "    • \"title\"\n",
    "    • \"start_date\"\n",
    "    • \"phase\"\n",
    "\n",
    "Process:\n",
    "- Load the raw trials into a DataFrame.\n",
    "- For each row, build a small JSON payload from (title, start_date, phase).\n",
    "- Compute an MD5 hash of the payload and prefix with \"tid_\" to form\n",
    "  a deterministic trial identifier.\n",
    "- Insert \"trial_hash\" as the first column.\n",
    "\n",
    "Outputs:\n",
    "- CSV written to:\n",
    "      cache/data_preprocess/raw_trials_with_hash.csv\n",
    "  containing all original columns plus the leading \"trial_hash\" column.\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "INPUT_PATH = Path(\"data/raw_trials.csv\")\n",
    "OUTPUT_PATH = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "print(f\"Loading raw trials from: {INPUT_PATH}\")\n",
    "data = pd.read_csv(INPUT_PATH, dtype=str).fillna(\"\")\n",
    "\n",
    "print(\"Generating trial_hash values ...\")\n",
    "\n",
    "def make_trial_hash(row):\n",
    "    \"\"\"Deterministic hash for a trial based on stable fields.\"\"\"\n",
    "    payload = {\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"start_date\": row.get(\"start_date\", \"\"),\n",
    "        \"phase\": row.get(\"phase\", \"\"),\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False)\n",
    "    return \"tid_\" + hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Create trial_hash column\n",
    "data[\"trial_hash\"] = data.apply(make_trial_hash, axis=1)\n",
    "\n",
    "# Move trial_hash to first column\n",
    "cols = [\"trial_hash\"] + [c for c in data.columns if c != \"trial_hash\"]\n",
    "data = data[cols]\n",
    "\n",
    "print(\"Data columns:\", data.columns)\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Export\n",
    "data.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14351eb6",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 184 trials from cache/data_preprocess/raw_trials_with_hash.csv\n",
      "Processed 50 trials...\n",
      "Processed 100 trials...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use a chatbot to extract structured drug-role metadata for each clinical trial.\n",
    "\n",
    "Inputs:\n",
    "- `cache/data_preprocess/raw_trials_with_hash.csv`\n",
    "    One row per trial, including:\n",
    "    • trial_hash (unique ID)\n",
    "    • title, objective, treatment_plan\n",
    "    • *_drugs_tested_json fields\n",
    "    • other structured or semi-structured metadata used to identify interventions.\n",
    "\n",
    "Process:\n",
    "- For each trial, build an LLM prompt using selected columns.\n",
    "- Ask the model to identify all distinct interventions and classify them.\n",
    "- For each drug:\n",
    "    • Assign role (Investigational Product, Active Comparator, Placebo, SOC)\n",
    "    • List alternative names / synonyms\n",
    "    • Identify molecular target and mechanism (if known)\n",
    "    • Assign tt_drug_id and bmt_drug_id only when matchable with high confidence.\n",
    "- Runs in parallel using ThreadPoolExecutor.\n",
    "- Skips writing output for trials that already have saved results.\n",
    "- Tracks processed, skipped, LLM errors, and JSON-parse errors.\n",
    "\n",
    "Outputs:\n",
    "- Per-trial mapped interventions:\n",
    "      `cache/task_1/trial_drug_roles/{trial_hash}.json`\n",
    "- Per-trial log files (prompt + raw response + cost):\n",
    "      `cache/task_1/trial_drug_roles_log/{trial_hash}.json`\n",
    "- Aggregated master index of all mappings:\n",
    "      `cache/task_1/trial_drug_roles_master.json`\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"task_1\" / \"trial_drug_roles\"\n",
    "DRUG_ROLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DRUG_ROLE_LOG_DIR = BASE_DIR / \"task_1\" / \"trial_drug_roles_log\"\n",
    "DRUG_ROLE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_ROLES_PATH = BASE_DIR / \"task_1\" / \"trial_drug_roles_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def build_prompt(trial_payload: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM to:\n",
    "    - Extract drug names\n",
    "    - Canonicalize names by removing company/manufacturer/location qualifiers\n",
    "    - Deduplicate synonymous names\n",
    "    - For each canonical drug, return a dict with:\n",
    "        * role (Investigational Product / Placebo / Active Comparator / Standard of Care)\n",
    "        * alternative_names (list)\n",
    "        * molecular_target\n",
    "        * mechanism\n",
    "        * tt_drug_id (TrialTrove/PharmaProjects drugId as string)\n",
    "        * bmt_drug_id (BioMedTracker bmtDrugId as string)\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and interpretation expert.\n",
    "\n",
    "You are given structured information about a clinical trial, including:\n",
    "- Title and objective\n",
    "- Study design and treatment plan\n",
    "- JSON fields listing drugs tested in the study:\n",
    "  - primary_drugs_tested_json\n",
    "  - other_drugs_tested_json\n",
    "  - bmt_other_drugs_tested_json\n",
    "  - bmt_primary_drugs_tested_json\n",
    "- These JSON fields may also contain metadata such as\n",
    "  drugApprovalStatus (Approved / Unapproved), mechanisms, synonyms, etc.\n",
    "- In the TrialTrove/PharmaProjects JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"drugId\".\n",
    "- In the BioMedTracker JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"bmtDrugId\".\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Identify all DISTINCT physical drug entities explicitly used in the study.\n",
    "   - Strings in the *_drugs_tested_json fields are drug-name candidates.\n",
    "   - If these fields contain structured JSON, infer names from keys such as\n",
    "     \"name\", \"drug_name\", \"drugName\", \"drugPrimaryName\", \"preferred_name\",\n",
    "     \"label\", etc.\n",
    "\n",
    "2. Canonicalize each drug name:\n",
    "   Remove company names, manufacturer qualifiers, geographic qualifiers,\n",
    "   dosage-form qualifiers, or parenthetical descriptors that do NOT change\n",
    "   the name of the underlying drug.\n",
    "\n",
    "   Examples:\n",
    "   - \"AlphaBlocker (CompanyX)\" → \"AlphaBlocker\"\n",
    "   - \"Recombinant Growth Factor (rgf)\" → \"Recombinant Growth Factor\"\n",
    "   - \"DrugX citrate (RegionY)\" → \"DrugX citrate\"\n",
    "   - \"BrandName (compound-42, MakerCorp)\" → \"BrandName\"\n",
    "\n",
    "3. Deduplicate synonymous names referring to the SAME drug.\n",
    "   - Prefer the simplest, most standard canonical name.\n",
    "   - Collect all other variations under alternative_names.\n",
    "\n",
    "4. For EACH distinct drug, build an object with SIX fields:\n",
    "\n",
    "   • \"role\": one of:\n",
    "       - \"Investigational Product\"\n",
    "       - \"Placebo\"\n",
    "       - \"Active Comparator\"\n",
    "       - \"Standard of Care\"\n",
    "\n",
    "     ROLE ASSIGNMENT RULES (SUMMARY):\n",
    "     - \"Investigational Product\": sponsor's novel/proprietary product.\n",
    "     - \"Standard of Care\": background therapy routinely used in practice.\n",
    "     - \"Active Comparator\": non-placebo comparator drug.\n",
    "     - \"Placebo\": inert control.\n",
    "\n",
    "   • \"alternative_names\": list of synonymous variants.\n",
    "   • \"molecular_target\": e.g., \"CD20\", \"VEGF-A\". If unknown, \"\".\n",
    "   • \"mechanism\": e.g., \"monoclonal antibody\", \"kinase inhibitor\". If unknown, \"\".\n",
    "   • \"tt_drug_id\": STRING. If not confidently matchable, \"\".\n",
    "   • \"bmt_drug_id\": STRING. If not confidently matchable, \"\".\n",
    "\n",
    "5. ID MISMATCH SAFETY RULE:\n",
    "   - Do NOT assign tt_drug_id or bmt_drug_id if they clearly belong to a different drug\n",
    "     (different target, mechanism, indication, modality, or obviously mismatched name).\n",
    "   - If there is ANY doubt about the correctness of an ID:\n",
    "       → Set BOTH \"tt_drug_id\" and \"bmt_drug_id\" to \"\".\n",
    "\n",
    "Output format (IMPORTANT):\n",
    "Return ONLY a valid JSON object with:\n",
    "  - keys   = canonical drug names\n",
    "  - values = objects with EXACTLY:\n",
    "        * \"role\"\n",
    "        * \"alternative_names\"\n",
    "        * \"molecular_target\"\n",
    "        * \"mechanism\"\n",
    "        * \"tt_drug_id\"\n",
    "        * \"bmt_drug_id\"\n",
    "\n",
    "Example:\n",
    "{{\n",
    "  \"ABC-123\": {{\n",
    "    \"role\": \"Investigational Product\",\n",
    "    \"alternative_names\": [\"ABC123\", \"Compound-ABC\"],\n",
    "    \"molecular_target\": \"Receptor-Z\",\n",
    "    \"mechanism\": \"Bispecific antibody\",\n",
    "    \"tt_drug_id\": \"123456\",\n",
    "    \"bmt_drug_id\": \"78901\"\n",
    "  }},\n",
    "  \"DrugX\": {{\n",
    "    \"role\": \"Standard of Care\",\n",
    "    \"alternative_names\": [\"GenericX\", \"ChemX\"],\n",
    "    \"molecular_target\": \"Enzyme-A\",\n",
    "    \"mechanism\": \"Antimetabolite\",\n",
    "    \"tt_drug_id\": \"\",\n",
    "    \"bmt_drug_id\": \"\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Input JSON:\n",
    "{payload_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "counter = {\"processed\": 0, \"skipped_existing\": 0, \"llm_error\": 0, \"parse_error\": 0}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "master_roles: dict[str, dict] = {}\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_trial(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one trial: call LLM, validate output, save role JSON and log.\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = DRUG_ROLE_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_prompt(trial_payload)\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LLM error for {trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    drug_roles = extract_json_object(text_response)\n",
    "    if not isinstance(drug_roles, dict) or not drug_roles:\n",
    "        print(f\"⚠️ JSON parse error trial_hash={trial_hash}\")\n",
    "        with counter_lock:\n",
    "            counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"title\": row.get(\"title\"),\n",
    "        \"drug_roles\": drug_roles,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    log_payload = {\n",
    "        \"token\": trial_hash,\n",
    "        \"hash_id\": trial_hash,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (DRUG_ROLE_LOG_DIR / f\"{trial_hash}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    with master_lock:\n",
    "        master_roles[trial_hash] = mapped\n",
    "        MASTER_ROLES_PATH.write_text(\n",
    "            json.dumps(master_roles, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        counter[\"processed\"] += 1\n",
    "        if counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Processed {counter['processed']} trials...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "rows = df_trials.to_dict(orient=\"records\")\n",
    "total_trials = len(rows)\n",
    "print(f\"Loaded {total_trials} trials from {TRIALS_WITH_HASH_CSV}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_trial, row, idx, total_trials): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Complete. processed={counter['processed']}, \"\n",
    "    f\"skipped={counter['skipped_existing']}, \"\n",
    "    f\"llm_error={counter['llm_error']}, \"\n",
    "    f\"parse_error={counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Roles directory: {DRUG_ROLE_DIR}\")\n",
    "print(f\"Log directory:   {DRUG_ROLE_LOG_DIR}\")\n",
    "print(f\"Master roles:    {MASTER_ROLES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce191b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summarize total LLM usage cost for previous cell by reading all per-trial log files.\n",
    "\n",
    "Inputs:\n",
    "- Directory: cache/task_1/trial_drug_roles_log/\n",
    "    Each log JSON contains:\n",
    "        • total_cost (float)\n",
    "        • other metadata (prompt, raw response, timing, etc.)\n",
    "\n",
    "Process:\n",
    "- Load each log file and extract its total_cost value.\n",
    "- Aggregate total cost, count entries, and compute average cost per trial.\n",
    "- Sort trials by cost to identify the most expensive prompts.\n",
    "\n",
    "Outputs:\n",
    "- Console summary including:\n",
    "    • Total cost\n",
    "    • Number of logged trials\n",
    "    • Average cost per trial\n",
    "    • Top 10 highest-cost trials (filename + cost)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/task_1/trial_drug_roles_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e313110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate per-trial drug-role JSONs into a wide trial-level product breakdown CSV.\n",
    "\n",
    "Inputs:\n",
    "- Directory: cache/task_1/trial_drug_roles/\n",
    "    Each file: {trial_hash}.json with structure:\n",
    "        {\n",
    "          \"trial_hash\": \"<tid_...>\",\n",
    "          \"title\": \"...\",\n",
    "          \"drug_roles\": {\n",
    "            \"<drug_name>\": {\n",
    "              \"role\": \"Investigational Product\" | \"Active Comparator\" | \"Placebo\" | \"Standard of Care\",\n",
    "              \"alternative_names\": [...],\n",
    "              \"molecular_target\": \"...\",\n",
    "              \"mechanism\": \"...\",\n",
    "              \"tt_drug_id\": \"...\",\n",
    "              \"bmt_drug_id\": \"...\"\n",
    "            },\n",
    "            ...\n",
    "          }\n",
    "        }\n",
    "\n",
    "Process:\n",
    "- Iterate over all JSONs in cache/task_1/trial_drug_roles/.\n",
    "- For each trial:\n",
    "    • Partition drugs into four buckets: investigational, active comparator, placebo, standard of care.\n",
    "    • Collect, per role:\n",
    "        - canonical names\n",
    "        - alternative_names (as list-of-lists)\n",
    "        - molecular_target\n",
    "        - mechanism\n",
    "        - tt_drug_id / bmt_drug_id where applicable.\n",
    "- Build one row per trial with list-valued columns for each role.\n",
    "\n",
    "Outputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "    One row per trial, columns:\n",
    "        trial_hash\n",
    "        investigational_products, investigational_products_alternative_names, ...\n",
    "        active_comparators, ...\n",
    "        placebos, ...\n",
    "        standard_of_care, ...\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Base directory for task_1 cache + input/output\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "\n",
    "# Directory that contains per-trial drug-role JSONs\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"trial_drug_roles\"\n",
    "\n",
    "# Output CSV path\n",
    "OUT_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fp in DRUG_ROLE_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    drug_roles = obj.get(\"drug_roles\") or {}\n",
    "    if not isinstance(drug_roles, dict):\n",
    "        print(f\"⚠️ drug_roles not dict in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Containers\n",
    "    inv_names = []\n",
    "    inv_alt_names = []          # list of lists\n",
    "    inv_targets = []\n",
    "    inv_mechanisms = []\n",
    "    inv_tt_ids = []\n",
    "    inv_bmt_ids = []\n",
    "\n",
    "    ac_names = []\n",
    "    ac_alt_names = []           # list of lists\n",
    "    ac_targets = []\n",
    "    ac_mechanisms = []\n",
    "    ac_tt_ids = []\n",
    "    ac_bmt_ids = []\n",
    "\n",
    "    plc_names = []\n",
    "    plc_alt_names = []          # list of lists\n",
    "    plc_targets = []\n",
    "    plc_mechanisms = []\n",
    "\n",
    "    soc_names = []\n",
    "    soc_alt_names = []          # list of lists\n",
    "    soc_targets = []\n",
    "    soc_mechanisms = []\n",
    "    soc_tt_ids = []\n",
    "    soc_bmt_ids = []\n",
    "\n",
    "    for drug_name, meta in drug_roles.items():\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "\n",
    "        role = (meta.get(\"role\") or \"\").strip()\n",
    "        role_norm = role.lower()\n",
    "\n",
    "        alt_names = meta.get(\"alternative_names\") or []\n",
    "        if not isinstance(alt_names, list):\n",
    "            alt_names = [str(alt_names)]\n",
    "\n",
    "        molecular_target = meta.get(\"molecular_target\") or \"\"\n",
    "        mechanism = meta.get(\"mechanism\") or \"\"\n",
    "\n",
    "        # IDs are always stored as strings in the LLM output, but be defensive\n",
    "        tt_id = str(meta.get(\"tt_drug_id\") or \"\")\n",
    "        bmt_id = str(meta.get(\"bmt_drug_id\") or \"\")\n",
    "\n",
    "        if role_norm == \"investigational product\":\n",
    "            inv_names.append(drug_name)\n",
    "            inv_alt_names.append(alt_names)\n",
    "            inv_targets.append(molecular_target)\n",
    "            inv_mechanisms.append(mechanism)\n",
    "            inv_tt_ids.append(tt_id)\n",
    "            inv_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"active comparator\":\n",
    "            ac_names.append(drug_name)\n",
    "            ac_alt_names.append(alt_names)\n",
    "            ac_targets.append(molecular_target)\n",
    "            ac_mechanisms.append(mechanism)\n",
    "            ac_tt_ids.append(tt_id)\n",
    "            ac_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"placebo\":\n",
    "            plc_names.append(drug_name)\n",
    "            plc_alt_names.append(alt_names)\n",
    "            plc_targets.append(molecular_target)\n",
    "            plc_mechanisms.append(mechanism)\n",
    "\n",
    "        elif role_norm == \"standard of care\":\n",
    "            soc_names.append(drug_name)\n",
    "            soc_alt_names.append(alt_names)\n",
    "            soc_targets.append(molecular_target)\n",
    "            soc_mechanisms.append(mechanism)\n",
    "            soc_tt_ids.append(tt_id)\n",
    "            soc_bmt_ids.append(bmt_id)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "\n",
    "            \"investigational_products\": inv_names,\n",
    "            \"investigational_products_alternative_names\": inv_alt_names,\n",
    "            \"investigational_products_molecular_target\": inv_targets,\n",
    "            \"investigational_products_mechanism\": inv_mechanisms,\n",
    "            \"investigational_products_tt_drug_id\": inv_tt_ids,\n",
    "            \"investigational_products_bmt_drug_id\": inv_bmt_ids,\n",
    "\n",
    "            \"active_comparators\": ac_names,\n",
    "            \"active_comparators_alternative_names\": ac_alt_names,\n",
    "            \"active_comparators_molecular_target\": ac_targets,\n",
    "            \"active_comparators_mechanism\": ac_mechanisms,\n",
    "            \"active_comparators_tt_drug_id\": ac_tt_ids,\n",
    "            \"active_comparators_bmt_drug_id\": ac_bmt_ids,\n",
    "\n",
    "            \"placebos\": plc_names,\n",
    "            \"placebos_alternative_names\": plc_alt_names,\n",
    "            \"placebos_molecular_target\": plc_targets,\n",
    "            \"placebos_mechanism\": plc_mechanisms,\n",
    "\n",
    "            \"standard_of_care\": soc_names,\n",
    "            \"standard_of_care_alternative_names\": soc_alt_names,\n",
    "            \"standard_of_care_molecular_target\": soc_targets,\n",
    "            \"standard_of_care_mechanism\": soc_mechanisms,\n",
    "            \"standard_of_care_tt_drug_id\": soc_tt_ids,\n",
    "            \"standard_of_care_bmt_drug_id\": soc_bmt_ids,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved trial product breakdown to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Identify trials with no investigational products.\n",
    "\n",
    "Purpose:\n",
    "- Load the trial_product_breakdown.csv file.\n",
    "- Parse the stringified list column \"investigational_products\" into real Python lists.\n",
    "- Flag all trials where the parsed list is empty (i.e., no investigational product identified).\n",
    "- Print summary statistics and display rows missing investigational products.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "\n",
    "Outputs:\n",
    "- Console summary of how many trials lack investigational products.\n",
    "- Markdown preview of example rows with missing investigational products.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "def parse_listish(s: str):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "# Parse the investigational_products column into real lists\n",
    "df[\"investigational_products_parsed\"] = df[\"investigational_products\"].apply(parse_listish)\n",
    "\n",
    "# Flag rows with no investigational products\n",
    "no_inv_mask = df[\"investigational_products_parsed\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "num_no_inv = int(no_inv_mask.sum())\n",
    "total = len(df)\n",
    "\n",
    "print(f\"Rows with NO investigational products: {num_no_inv} / {total}\")\n",
    "\n",
    "print(\n",
    "    df.loc[no_inv_mask, [\"trial_hash\", \"investigational_products\"]]\n",
    "      .head(20)\n",
    "      .to_markdown(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ab97f",
   "metadata": {},
   "source": [
    "Manual checks \n",
    "- tid_4c45730f6411aa1e5a38bb1223d66988\n",
    "    - This trial is combining three standard-of-care agents into a regimen “DCF”\n",
    "- tid_67de51bf9728e056a6fb42c76e4b0212\n",
    "    - Even though they administer Yisaipu in a structured way, it is an approved drug and not being tested for regulatory approval.\n",
    "- tid_8cab7b7177fcb0d10255bced8b0633ee\n",
    "    - The trial is studying treatment strategies, regimens, algorithms, imaging-guided regimen selection, or dosing, using only approved standard therapies.\n",
    "- tid_bb1e0571142dde8a49976632c349593c\n",
    "    - The trial's focus is on optimizing regimen selection (e.g., TIPy or TCbIPy) via imaging, rather than testing a new drug entity.\n",
    "\n",
    "these are all confirmed generics biosimilars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate trial-level product data by TrialTrove tt_drug_id and identify products\n",
    "missing both molecular targets and mechanisms.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "    Contains per-trial lists of products and their tt_drug_id, targets, and mechanisms.\n",
    "\n",
    "Process:\n",
    "- Parse list-like columns from strings into Python lists.\n",
    "- Aggregate across all trials, keyed by tt_drug_id, collecting:\n",
    "    • drug_names\n",
    "    • alternative_names\n",
    "    • molecular_targets\n",
    "    • product_mechanisms\n",
    "    • trial_hashes where each product appears.\n",
    "- Build a product-level master table (one row per tt_drug_id).\n",
    "- Identify tt_drug_id entries that are missing BOTH molecular_targets\n",
    "  and product_mechanisms.\n",
    "\n",
    "Outputs:\n",
    "- Product master table:\n",
    "      cache/task_1/product_id_master_by_tt.csv\n",
    "- Table of products missing both targets and mechanisms:\n",
    "      cache/task_1/product_id_missing_targets_or_mechs.csv\n",
    "- Console preview of the first 10 aggregated rows and all missing rows.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "OUT_AGG = BASE_DIR / \"product_id_master_by_tt.csv\"\n",
    "OUT_MISSING = BASE_DIR / \"product_id_missing_targets_or_mechs.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Load trial-level breakdown\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Aggregate everything keyed by tt_drug_id\n",
    "agg = {}  # tt_id -> {\"names\": set(), \"alt_names\": set(), \"targets\": set(), \"mechs\": set(), \"trials\": set()}\n",
    "\n",
    "ROLE_PAIRS = [\n",
    "    (\"investigational_products\", \"investigational_products_tt_drug_id\"),\n",
    "    (\"active_comparators\", \"active_comparators_tt_drug_id\"),\n",
    "    (\"standard_of_care\", \"standard_of_care_tt_drug_id\"),\n",
    "]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "\n",
    "    for base_col, tt_col in ROLE_PAIRS:\n",
    "        # aligned lists\n",
    "        names_list   = parse_listish(row.get(base_col, \"\"))\n",
    "        alts_list    = parse_listish(row.get(f\"{base_col}_alternative_names\", \"\"))\n",
    "        targets_list = parse_listish(row.get(f\"{base_col}_molecular_target\", \"\"))\n",
    "        mechs_list   = parse_listish(row.get(f\"{base_col}_mechanism\", \"\"))\n",
    "        tt_ids       = parse_listish(row.get(tt_col, \"\"))\n",
    "\n",
    "        # iterate by index over tt_ids (they define the products)\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "            if not tt_id:\n",
    "                continue\n",
    "\n",
    "            # init aggregate bucket if needed\n",
    "            if tt_id not in agg:\n",
    "                agg[tt_id] = {\n",
    "                    \"names\": set(),\n",
    "                    \"alt_names\": set(),\n",
    "                    \"targets\": set(),\n",
    "                    \"mechs\": set(),\n",
    "                    \"trials\": set(),\n",
    "                }\n",
    "\n",
    "            # record trial hash if available\n",
    "            if trial_hash:\n",
    "                agg[tt_id][\"trials\"].add(trial_hash)\n",
    "\n",
    "            # name\n",
    "            if i < len(names_list):\n",
    "                name = str(names_list[i]).strip()\n",
    "                if name:\n",
    "                    agg[tt_id][\"names\"].add(name)\n",
    "\n",
    "            # alternative names (may be nested lists)\n",
    "            if i < len(alts_list):\n",
    "                alt_entry = alts_list[i]\n",
    "                if isinstance(alt_entry, list):\n",
    "                    for a in alt_entry:\n",
    "                        a_str = str(a).strip()\n",
    "                        if a_str:\n",
    "                            agg[tt_id][\"alt_names\"].add(a_str)\n",
    "                else:\n",
    "                    a_str = str(alt_entry).strip()\n",
    "                    if a_str:\n",
    "                        agg[tt_id][\"alt_names\"].add(a_str)\n",
    "\n",
    "            # target\n",
    "            if i < len(targets_list):\n",
    "                tgt = str(targets_list[i]).strip()\n",
    "                if tgt:\n",
    "                    agg[tt_id][\"targets\"].add(tgt)\n",
    "\n",
    "            # mechanism\n",
    "            if i < len(mechs_list):\n",
    "                mech = str(mechs_list[i]).strip()\n",
    "                if mech:\n",
    "                    agg[tt_id][\"mechs\"].add(mech)\n",
    "\n",
    "# Build aggregated DataFrame\n",
    "rows_out = []\n",
    "for tt_id, payload in agg.items():\n",
    "    rows_out.append(\n",
    "        {\n",
    "            \"tt_drug_id\": tt_id,\n",
    "            \"drug_names\": sorted(payload[\"names\"]),\n",
    "            \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "            \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "            \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "            \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "grouped_df = pd.DataFrame(rows_out).sort_values(\"tt_drug_id\")\n",
    "\n",
    "print(\"Aggregated by tt_drug_id (first 10 rows):\")\n",
    "print(grouped_df.head(10).to_markdown(index=False))\n",
    "\n",
    "grouped_df.to_csv(OUT_AGG, index=False)\n",
    "print(f\"Saved aggregated tt_drug_id table → {OUT_AGG}\")\n",
    "\n",
    "# Identify rows missing both targets AND mechanisms\n",
    "missing_mask = grouped_df[\"molecular_targets\"].apply(lambda x: len(x) == 0) & \\\n",
    "               grouped_df[\"product_mechanisms\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "missing_df = grouped_df[missing_mask].copy()\n",
    "\n",
    "print(\"Rows missing molecular_targets AND product_mechanisms:\")\n",
    "if missing_df.empty:\n",
    "    print(\"No missing values — every tt_drug_id has at least one target or mechanism.\")\n",
    "else:\n",
    "    print(missing_df.to_markdown(index=False))\n",
    "\n",
    "missing_df.to_csv(OUT_MISSING, index=False)\n",
    "print(f\"Saved → {OUT_MISSING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86691e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Infer molecular targets and mechanisms of action for products missing both fields,\n",
    "using LLM + web search on trial context.\n",
    "\n",
    "Inputs:\n",
    "- Product-level CSV of products missing targets/mechanisms:\n",
    "      cache/task_1/product_id_missing_targets_or_mechs.csv\n",
    "    (one row per tt_drug_id; columns include tt_drug_id, drug_names,\n",
    "     alternative_names, trial_hashes, etc.)\n",
    "- Trial metadata CSV:\n",
    "      cache/data_preprocess/raw_trials_with_hash.csv\n",
    "    (one row per trial; must include trial_hash and core text/JSON fields).\n",
    "\n",
    "Process:\n",
    "- Load the \"missing products\" table from disk.\n",
    "- Build an index of trial_hash → full trial metadata.\n",
    "- For each tt_drug_id in the missing-products table:\n",
    "    • Parse trial_hashes and gather associated trials.\n",
    "    • Build a prompt with all known drug names and trial context.\n",
    "    • Call the LLM with web_search tools to infer:\n",
    "        - molecular_target\n",
    "        - mechanism\n",
    "      If the target/mechanism is not publicly disclosed, both fields should be \"\".\n",
    "- Save a per-product JSON file and a structured log.\n",
    "- Maintain a rolling master JSON of all inferred product mechanisms.\n",
    "\n",
    "Outputs:\n",
    "- Per-product mechanism JSON:\n",
    "      cache/task_1/product_mechanism_inference/{tt_drug_id}.json\n",
    "- Per-product log JSON:\n",
    "      cache/task_1/product_mechanism_inference_log/{tt_drug_id}.json\n",
    "- Master mapping:\n",
    "      cache/task_1/product_mechanism_inference_master.json\n",
    "- Console summary of processed / skipped / error counts.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# Base dir for this task's outputs\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "\n",
    "# Trial metadata (shared across tasks)\n",
    "RAW_TRIALS_CSV = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "\n",
    "# Missing-products table (from previous aggregation cell)\n",
    "MISSING_PRODUCTS_CSV = BASE_DIR / \"product_id_missing_targets_or_mechs.csv\"\n",
    "\n",
    "# Output dirs/files for product mechanism inference\n",
    "PRODUCT_MECH_DIR = BASE_DIR / \"product_mechanism_inference\"\n",
    "PRODUCT_MECH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRODUCT_MECH_LOG_DIR = BASE_DIR / \"product_mechanism_inference_log\"\n",
    "PRODUCT_MECH_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def safe_parse_listish(val):\n",
    "    \"\"\"\n",
    "    Parse list-like strings back into Python lists, if needed.\n",
    "    If already a list, return as-is.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if val is None:\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def build_product_prompt(row: dict, trial_context: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM (with web_search) to infer\n",
    "    molecular_target and mechanism for a product, based on:\n",
    "      - drug names / alternative names\n",
    "      - full metadata for associated trials\n",
    "    \"\"\"\n",
    "    drug_names = safe_parse_listish(row.get(\"drug_names\", [])) or []\n",
    "\n",
    "    # Ensure lists are JSON-serializable\n",
    "    try:\n",
    "        drug_names_json = json.dumps(drug_names, ensure_ascii=False)\n",
    "    except TypeError:\n",
    "        drug_names_json = json.dumps([str(x) for x in drug_names], ensure_ascii=False)\n",
    "\n",
    "    # Trials context JSON\n",
    "    trials_json = json.dumps(trial_context, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a pharmacology expert with access to web search.\n",
    "\n",
    "You are given:\n",
    "- A drug name (and aliases).\n",
    "- Full metadata for one or more clinical trials in which this drug appears (JSON objects).\n",
    "\n",
    "Your goal:\n",
    "Using web search and your domain knowledge, determine:\n",
    "1. The primary molecular target(s) of the drug (e.g., EGFR, VEGFR2, TNF, CD20, JAK1/2).\n",
    "2. A concise, standard mechanism of action label (e.g., \"EGFR inhibitor\", \"Anti-PD-1 antibody\", \n",
    "   \"JAK inhibitor\", \"DNA-damaging cytotoxic\", etc.).\n",
    "\n",
    "Rules:\n",
    "- Try searching for the drug using all known names or aliases.\n",
    "- If no molecular target or mechanism of action has been publicly disclosed,\n",
    "  then return empty strings for BOTH fields.\n",
    "\n",
    "INPUT\n",
    "-----\n",
    "drug_name: {drug_names_json}\n",
    "trial_metadata:\n",
    "{trials_json}\n",
    "\n",
    "OUTPUT (JSON only)\n",
    "------------------\n",
    "{{\n",
    "  \"molecular_target\": \"\",\n",
    "  \"mechanism\": \"\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Shared counters & master mapping\n",
    "product_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "}\n",
    "product_counter_lock = threading.Lock()\n",
    "\n",
    "product_master: dict[str, dict] = {}\n",
    "product_master_lock = threading.Lock()\n",
    "\n",
    "# Load existing master if present\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    try:\n",
    "        product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        product_master = {}\n",
    "\n",
    "# Load trial metadata and build index\n",
    "df_trials = pd.read_csv(RAW_TRIALS_CSV, dtype=str).fillna(\"\")\n",
    "trials_index: dict[str, dict] = {\n",
    "    str(row[\"trial_hash\"]).strip(): row.to_dict()\n",
    "    for _, row in df_trials.iterrows()\n",
    "}\n",
    "\n",
    "\n",
    "def process_product(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one tt_drug_id: call LLM+web_search with trial context, save output & log.\"\"\"\n",
    "    tt_drug_id = str(row.get(\"tt_drug_id\", \"\")).strip()\n",
    "    if not tt_drug_id:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing tt_drug_id, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = PRODUCT_MECH_DIR / f\"{tt_drug_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # Get associated trial hashes and build trial context list\n",
    "    trial_hashes_raw = row.get(\"trial_hashes\", [])\n",
    "    trial_hashes = safe_parse_listish(trial_hashes_raw)\n",
    "\n",
    "    trial_context = []\n",
    "    for th in trial_hashes:\n",
    "        th_key = str(th).strip()\n",
    "        if not th_key:\n",
    "            continue\n",
    "        trial_row = trials_index.get(th_key)\n",
    "        if trial_row:\n",
    "            trial_context.append(trial_row)\n",
    "\n",
    "    prompt = build_product_prompt(row, trial_context)\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM with web_search tool\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(\n",
    "            prompt=prompt,\n",
    "            model=MODEL,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "        )\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for tt_drug_id={tt_drug_id}: {e}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mech_obj = extract_json_object(text_response)\n",
    "\n",
    "    # Expect a dict with the two keys\n",
    "    if not isinstance(mech_obj, dict) or not mech_obj:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse/validity error tt_drug_id={tt_drug_id}, raw={text_response!r}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    molecular_target = str(mech_obj.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "    mechanism = str(mech_obj.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "    mapped = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"drug_names\": safe_parse_listish(row.get(\"drug_names\", [])),\n",
    "        \"alternative_names\": safe_parse_listish(row.get(\"alternative_names\", [])),\n",
    "        \"trial_hashes\": trial_hashes,\n",
    "        \"molecular_target\": molecular_target,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"source\": \"llm_web_search\",\n",
    "    }\n",
    "\n",
    "    # Save per-product JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (PRODUCT_MECH_LOG_DIR / f\"{tt_drug_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with product_master_lock:\n",
    "        product_master[tt_drug_id] = mapped\n",
    "        MASTER_PRODUCT_MECH_PATH.write_text(\n",
    "            json.dumps(product_master, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with product_counter_lock:\n",
    "        product_counter[\"processed\"] += 1\n",
    "        if product_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {product_counter['processed']} products...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "missing_df = pd.read_csv(MISSING_PRODUCTS_CSV, dtype=str).fillna(\"\")\n",
    "missing_rows = missing_df.to_dict(orient=\"records\")\n",
    "total_missing = len(missing_rows)\n",
    "print(f\"Loaded {total_missing} products missing targets/mechanisms from {MISSING_PRODUCTS_CSV}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_product, row, idx, total_missing): row.get(\"tt_drug_id\")\n",
    "        for idx, row in enumerate(missing_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        tid = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error tt_drug_id={tid}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Product mechanism inference complete. \"\n",
    "    f\"processed={product_counter['processed']}, \"\n",
    "    f\"skipped={product_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={product_counter['llm_error']}, \"\n",
    "    f\"parse_error={product_counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Per-product directory: {PRODUCT_MECH_DIR}\")\n",
    "print(f\"Log directory:        {PRODUCT_MECH_LOG_DIR}\")\n",
    "print(f\"Master file:          {MASTER_PRODUCT_MECH_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e85a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a did-keyed master product dictionary by merging trial-level product info\n",
    "with (optional) LLM-inferred mechanisms.\n",
    "\n",
    "Inputs:\n",
    "- Trial product breakdown CSV:\n",
    "      cache/task_1/trial_product_breakdown.csv\n",
    "  Contains, per trial_hash:\n",
    "      • investigational_products / active_comparators / standard_of_care\n",
    "      • *_alternative_names\n",
    "      • *_molecular_target\n",
    "      • *_mechanism\n",
    "      • *_tt_drug_id\n",
    "- Optional LLM mechanism master:\n",
    "      cache/task_1/product_mechanism_inference_master.json\n",
    "  Maps tt_drug_id → inferred molecular_target and mechanism.\n",
    "\n",
    "Process:\n",
    "- For each role (investigational_products, active_comparators, standard_of_care):\n",
    "    • Parse list-like columns into Python lists.\n",
    "    • Aggregate by tt_drug_id:\n",
    "        - collect all names, alt_names, targets, mechanisms, trial_hashes.\n",
    "        - fill missing targets/mechanisms from product_mechanism_inference_master.json\n",
    "    • For entries without tt_drug_id but with target/mechanism:\n",
    "        - create synthetic product entries keyed by (role, name, target, mechanism).\n",
    "- Generate a stable deterministic \"did_*\" ID:\n",
    "    • For known tt_drug_id: hash of tt_drug_id.\n",
    "    • For unknown products: hash of composite key (role + data).\n",
    "\n",
    "Outputs:\n",
    "- did-keyed JSON:\n",
    "      cache/task_1/product_id_master_by_did.json\n",
    "  Structure:\n",
    "      {\n",
    "        \"did_<hash>\": {\n",
    "          \"did\": \"did_<hash>\",\n",
    "          \"tt_drug_id\": \"<tt_id or ''>\",\n",
    "          \"drug_names\": [...],\n",
    "          \"alternative_names\": [...],\n",
    "          \"molecular_targets\": [...],\n",
    "          \"product_mechanisms\": [...],\n",
    "          \"trial_hashes\": [...]\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "import ast\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "OUT_JSON = BASE_DIR / \"product_id_master_by_did.json\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def pad_to_length(lst, n):\n",
    "    \"\"\"Pad list with empty strings so len(lst) >= n.\"\"\"\n",
    "    lst = list(lst)\n",
    "    while len(lst) < n:\n",
    "        lst.append(\"\")\n",
    "    return lst\n",
    "\n",
    "\n",
    "def make_did_from_tt(tt_drug_id: str) -> str:\n",
    "    \"\"\"Deterministic drug hash ID based on tt_drug_id (for known IDs).\"\"\"\n",
    "    h = hashlib.md5(tt_drug_id.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"did_{h}\"\n",
    "\n",
    "\n",
    "def make_did_for_unknown(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic drug hash ID for products without tt_drug_id.\n",
    "    Key can be any composite string (e.g., role + name + target + mechanism).\n",
    "    \"\"\"\n",
    "    h = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"did_{h}\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------\n",
    "# Load inputs\n",
    "df = pd.read_csv(IN_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trial breakdown: {IN_BREAKDOWN_CSV}, shape={df.shape}\")\n",
    "\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    product_master = {}\n",
    "    print(f\"⚠️ No product master mech file found at {MASTER_PRODUCT_MECH_PATH}\")\n",
    "\n",
    "# role → (base_name_col, tt_id_col, target_col, mech_col)\n",
    "ROLE_SPECS = [\n",
    "    (\n",
    "        \"investigational_products\",\n",
    "        \"investigational_products_tt_drug_id\",\n",
    "        \"investigational_products_molecular_target\",\n",
    "        \"investigational_products_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"active_comparators\",\n",
    "        \"active_comparators_tt_drug_id\",\n",
    "        \"active_comparators_molecular_target\",\n",
    "        \"active_comparators_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"standard_of_care\",\n",
    "        \"standard_of_care_tt_drug_id\",\n",
    "        \"standard_of_care_molecular_target\",\n",
    "        \"standard_of_care_mechanism\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Aggregate per tt_drug_id and per \"unknown but has mech/target\"\n",
    "agg_tt = {}       # tt_id -> {...}\n",
    "agg_unknown = {}  # composite_key -> {...}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "\n",
    "    for base_col, tt_col, tgt_col, mech_col in ROLE_SPECS:\n",
    "        # Skip if any required column is missing\n",
    "        if tt_col not in df.columns or tgt_col not in df.columns or mech_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Base name + alt-name columns\n",
    "        names_list = parse_listish(row.get(base_col, \"\"))\n",
    "        alt_list   = parse_listish(row.get(f\"{base_col}_alternative_names\", \"\"))\n",
    "\n",
    "        tt_ids   = parse_listish(row.get(tt_col, \"\"))\n",
    "        targets  = parse_listish(row.get(tgt_col, \"\"))\n",
    "        mechs    = parse_listish(row.get(mech_col, \"\"))\n",
    "\n",
    "        # Align target/mech lists to tt_ids length\n",
    "        targets = pad_to_length(targets, len(tt_ids))\n",
    "        mechs   = pad_to_length(mechs, len(tt_ids))\n",
    "\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "\n",
    "            # Name (by position if available)\n",
    "            name = \"\"\n",
    "            if i < len(names_list):\n",
    "                name = str(names_list[i]).strip()\n",
    "\n",
    "            # Alternative names (can be list-of-lists or flat)\n",
    "            alt_names_for_this = []\n",
    "            if i < len(alt_list):\n",
    "                alt_entry = alt_list[i]\n",
    "                if isinstance(alt_entry, list):\n",
    "                    for a in alt_entry:\n",
    "                        a_str = str(a).strip()\n",
    "                        if a_str:\n",
    "                            alt_names_for_this.append(a_str)\n",
    "                else:\n",
    "                    a_str = str(alt_entry).strip()\n",
    "                    if a_str:\n",
    "                        alt_names_for_this.append(a_str)\n",
    "\n",
    "            # Existing target/mechanism from CSV\n",
    "            csv_target = str(targets[i]).strip()\n",
    "            csv_mech   = str(mechs[i]).strip()\n",
    "\n",
    "            # Case 1: Have a tt_drug_id → normal aggregation\n",
    "            if tt_id:\n",
    "                if tt_id not in agg_tt:\n",
    "                    agg_tt[tt_id] = {\n",
    "                        \"names\": set(),\n",
    "                        \"alt_names\": set(),\n",
    "                        \"targets\": set(),\n",
    "                        \"mechs\": set(),\n",
    "                        \"trials\": set(),\n",
    "                    }\n",
    "\n",
    "                # Record trial hash\n",
    "                if trial_hash:\n",
    "                    agg_tt[tt_id][\"trials\"].add(trial_hash)\n",
    "\n",
    "                # Names\n",
    "                if name:\n",
    "                    agg_tt[tt_id][\"names\"].add(name)\n",
    "\n",
    "                for a_str in alt_names_for_this:\n",
    "                    agg_tt[tt_id][\"alt_names\"].add(a_str)\n",
    "\n",
    "                # LLM-inferred target/mechanism (if available)\n",
    "                info = product_master.get(tt_id) or {}\n",
    "                inferred_target = str(info.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "                inferred_mech   = str(info.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "                # Final chosen values for this (trial, index, tt_id)\n",
    "                final_target = csv_target or inferred_target\n",
    "                final_mech   = csv_mech   or inferred_mech\n",
    "\n",
    "                if final_target:\n",
    "                    agg_tt[tt_id][\"targets\"].add(final_target)\n",
    "                if final_mech:\n",
    "                    agg_tt[tt_id][\"mechs\"].add(final_mech)\n",
    "\n",
    "            # Case 2: NO tt_drug_id, but we have target or mechanism\n",
    "            # → create a synthetic product entry with empty tt_drug_id\n",
    "            else:\n",
    "                # If we have no name and no mechanistic info, skip\n",
    "                if not (name or csv_target or csv_mech):\n",
    "                    continue\n",
    "\n",
    "                # Only create unknown entry if there is mechanistic info\n",
    "                if not (csv_target or csv_mech):\n",
    "                    continue\n",
    "\n",
    "                # Build a composite key to deduplicate unknown products\n",
    "                composite_key = f\"{base_col}||{name}||{csv_target}||{csv_mech}\"\n",
    "\n",
    "                if composite_key not in agg_unknown:\n",
    "                    agg_unknown[composite_key] = {\n",
    "                        \"names\": set(),\n",
    "                        \"alt_names\": set(),\n",
    "                        \"targets\": set(),\n",
    "                        \"mechs\": set(),\n",
    "                        \"trials\": set(),\n",
    "                    }\n",
    "\n",
    "                if trial_hash:\n",
    "                    agg_unknown[composite_key][\"trials\"].add(trial_hash)\n",
    "\n",
    "                if name:\n",
    "                    agg_unknown[composite_key][\"names\"].add(name)\n",
    "\n",
    "                for a_str in alt_names_for_this:\n",
    "                    agg_unknown[composite_key][\"alt_names\"].add(a_str)\n",
    "\n",
    "                if csv_target:\n",
    "                    agg_unknown[composite_key][\"targets\"].add(csv_target)\n",
    "                if csv_mech:\n",
    "                    agg_unknown[composite_key][\"mechs\"].add(csv_mech)\n",
    "\n",
    "print(f\"Aggregated {len(agg_tt)} distinct tt_drug_id entries.\")\n",
    "print(f\"Aggregated {len(agg_unknown)} products without tt_drug_id but with target/mechanism.\")\n",
    "\n",
    "# Build did-keyed JSON structure\n",
    "drug_master_by_did = {}\n",
    "\n",
    "# 1) Entries with real tt_drug_id\n",
    "for tt_id, payload in agg_tt.items():\n",
    "    did = make_did_from_tt(tt_id)\n",
    "    drug_master_by_did[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": tt_id,\n",
    "        \"drug_names\": sorted(payload[\"names\"]),\n",
    "        \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "        \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "        \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "        \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "    }\n",
    "\n",
    "# 2) Entries without tt_drug_id (tt_drug_id = \"\")\n",
    "for composite_key, payload in agg_unknown.items():\n",
    "    did = make_did_for_unknown(composite_key)\n",
    "    drug_master_by_did[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": \"\",  # explicitly empty as requested\n",
    "        \"drug_names\": sorted(payload[\"names\"]),\n",
    "        \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "        \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "        \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "        \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "    }\n",
    "\n",
    "# Save JSON\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(\n",
    "    json.dumps(drug_master_by_did, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(f\"Saved did-keyed drug master JSON → {OUT_JSON}\")\n",
    "print(f\"Total drugs: {len(drug_master_by_did)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709c055",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77190ce",
   "metadata": {},
   "source": [
    "Identify whether the drugs are innovative or/generic biosimilars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV    = BASE_DIR / \"raw_trials_with_hash.csv\"\n",
    "PRODUCT_BREAKDOWN_CSV   = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "PRODUCT_BY_DID_JSON     = BASE_DIR / \"product_id_master_by_did.json\"\n",
    "\n",
    "INNOV_DIR = BASE_DIR / \"trial_investigational_drugs_classifications\"\n",
    "INNOV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INNOV_LOG_DIR = BASE_DIR / \"trial_investigational_drugs_classifications_log\"\n",
    "INNOV_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INNOV_PATH = BASE_DIR / \"trial_investigational_drugs_classifications_master.json\"\n",
    "\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "MAX_WORKERS_INNOV = 8\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def load_master_innov() -> dict:\n",
    "    if not MASTER_INNOV_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_INNOV_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "def parse_listish(s):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Common empty-list cases\n",
    "    if s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        # If it's something else, treat as a single non-empty token\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        # Fallback: treat non-empty string as a single element\n",
    "        return [s]\n",
    "\n",
    "def pad_to_length(lst, n):\n",
    "    \"\"\"Pad list with empty strings so len(lst) >= n.\"\"\"\n",
    "    lst = list(lst)\n",
    "    while len(lst) < n:\n",
    "        lst.append(\"\")\n",
    "    return lst\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Load per-drug metadata (product_id_master_by_did.json)\n",
    "# -------------------------------------------------\n",
    "if PRODUCT_BY_DID_JSON.exists():\n",
    "    product_by_did = json.loads(PRODUCT_BY_DID_JSON.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    product_by_did = {}\n",
    "    print(f\"⚠️ No drug master JSON found at {PRODUCT_BY_DID_JSON}\")\n",
    "\n",
    "# Build tt_drug_id → metadata mapping for quick lookup\n",
    "tt_to_drug_meta: dict[str, dict] = {}\n",
    "for did, rec in product_by_did.items():\n",
    "    tt = str(rec.get(\"tt_drug_id\", \"\")).strip()\n",
    "    if tt and tt not in tt_to_drug_meta:\n",
    "        tt_to_drug_meta[tt] = rec\n",
    "\n",
    "\n",
    "def build_innovation_prompt(trial_payload: dict, drug_contexts: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt to classify each investigational product as\n",
    "    Innovative / Generic / Biosimilar, with one-sentence explanation,\n",
    "    using extra context about each drug (names, alt names, targets, mechanisms).\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "    drugs_json   = json.dumps(drug_contexts, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and drug development expert.\n",
    "\n",
    "You are given:\n",
    "1) Structured information about a clinical trial (title, objective, results, etc.).\n",
    "2) A list of investigational products used in the trial, with extra metadata for each.\n",
    "   Each investigational drug entry has:\n",
    "   - \"name\": the canonical investigational product name in this trial\n",
    "   - \"tt_drug_id\": the TrialTrove/PharmaProjects drugId as a string (if known)\n",
    "   - \"drug_names\": list of names for this drug\n",
    "   - \"alternative_names\": list of alternative or synonym names\n",
    "   - \"molecular_targets\": list of known molecular targets\n",
    "   - \"product_mechanisms\": list of known mechanisms of action\n",
    "\n",
    "Your task: For EACH investigational product (by its \"name\"), classify whether it is:\n",
    "- \"Innovative\"\n",
    "- \"Generic\"\n",
    "- \"Biosimilar\"\n",
    "\n",
    "and provide:\n",
    "- a one-sentence concise explanation for your classification\n",
    "- the tt_drug_id (string; use \"\" if unknown).\n",
    "\n",
    "DEFINITIONS / GUIDANCE\n",
    "----------------------\n",
    "\n",
    "Innovative:\n",
    "- A novel or proprietary drug (new or sponsor-specific product).\n",
    "- New mechanism of action OR new molecular entity OR clearly the sponsor's lead product.\n",
    "- Often associated with efficacy or superiority language:\n",
    "  - \"evaluate efficacy\", \"vs placebo\", \"improve outcomes\", etc.\n",
    "- Not a copy of an already-approved product.\n",
    "\n",
    "Generic:\n",
    "- A small-molecule copy of an already-approved branded drug.\n",
    "- Same active ingredient, strength, dosage form, and route.\n",
    "- Often explicitly described as generic or equivalent.\n",
    "\n",
    "Biosimilar:\n",
    "- A biologic product that is highly similar to an already-approved reference biologic.\n",
    "- Same target and mechanism as a branded biologic.\n",
    "- Strong clues:\n",
    "  - \"equivalence\", \"non-inferiority\", \"no clinically meaningful differences\",\n",
    "  - direct comparison to a specific branded reference biologic.\n",
    "\n",
    "You MUST choose ONE of the three labels (\"Innovative\", \"Generic\", \"Biosimilar\") for each drug.\n",
    "If you are uncertain, you may say so in the one-sentence explanation, but still pick a label.\n",
    "\n",
    "Use all available information:\n",
    "- Trial text and design\n",
    "- Investigational vs comparator roles\n",
    "- Known targets/mechanisms from the drug metadata.\n",
    "\n",
    "OUTPUT FORMAT (IMPORTANT)\n",
    "-------------------------\n",
    "\n",
    "Return ONLY a valid JSON object, with:\n",
    "- KEYS   = exactly the investigational product \"name\" values given below\n",
    "- VALUES = an object with exactly three fields:\n",
    "    - \"classification\": one of \"Innovative\", \"Generic\", \"Biosimilar\"\n",
    "    - \"explanation\": a single, concise sentence explaining your reasoning\n",
    "    - \"tt_drug_id\": the string tt_drug_id for this drug (\"\" if unknown)\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "  \"DrugA\": {{\n",
    "    \"classification\": \"Innovative\",\n",
    "    \"explanation\": \"DrugA is a novel monoclonal antibody targeting a new receptor and is the sponsor's lead product.\",\n",
    "    \"tt_drug_id\": \"123456\"\n",
    "  }},\n",
    "  \"DrugB\": {{\n",
    "    \"classification\": \"Biosimilar\",\n",
    "    \"explanation\": \"DrugB is tested for equivalence compared to the branded biologic with the same active ingredient.\",\n",
    "    \"tt_drug_id\": \"789012\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "TRIAL PAYLOAD (includes trial text and all drug-role breakdown columns):\n",
    "{payload_json}\n",
    "\n",
    "INVESTIGATIONAL DRUG CONTEXTS (you MUST classify EACH by its 'name' key):\n",
    "{drugs_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "master_innov = load_master_innov()\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "innov_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,\n",
    "}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_innov_row(row: dict, idx: int, total: int, breakdown_cols: list[str]) -> None:\n",
    "    \"\"\"Process a single trial with investigational products.\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    # Names as used in the trial\n",
    "    investigational_products = row.get(\"investigational_products_parsed\") or []\n",
    "    investigational_products = [str(x).strip() for x in investigational_products if str(x).strip()]\n",
    "\n",
    "    if not investigational_products:\n",
    "        # Shouldn't happen due to filtering, but be safe\n",
    "        return\n",
    "\n",
    "    out_fp = INNOV_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            innov_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Build per-drug context from did JSON\n",
    "    # ------------------------------------\n",
    "    # Aligned TrialTrove IDs for investigational products\n",
    "    inv_tt_raw = row.get(\"investigational_products_tt_drug_id\", \"\")\n",
    "    inv_tt_ids = parse_listish(inv_tt_raw)\n",
    "    inv_tt_ids = pad_to_length(inv_tt_ids, len(investigational_products))\n",
    "\n",
    "    drug_contexts = []\n",
    "    for i, name in enumerate(investigational_products):\n",
    "        tt_id = str(inv_tt_ids[i]).strip() if i < len(inv_tt_ids) else \"\"\n",
    "        meta = tt_to_drug_meta.get(tt_id, {}) if tt_id else {}\n",
    "\n",
    "        drug_contexts.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"tt_drug_id\": tt_id,\n",
    "                \"drug_names\": meta.get(\"drug_names\", []),\n",
    "                \"alternative_names\": meta.get(\"alternative_names\", []),\n",
    "                \"molecular_targets\": meta.get(\"molecular_targets\", []),\n",
    "                \"product_mechanisms\": meta.get(\"product_mechanisms\", []),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Build payload from selected columns\n",
    "    # ------------------------------------\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "\n",
    "    # 1) Trial-level textual fields from raw_trials_with_hash.csv\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    # 2) ALL columns from trial_product_breakdown.csv\n",
    "    for col in breakdown_cols:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_innovation_prompt(trial_payload, drug_contexts)\n",
    "\n",
    "    token = trial_hash\n",
    "    hash_id = trial_hash\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for trial_hash={trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    classifications = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(classifications, dict) or not classifications:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse error trial_hash={trial_hash}, raw={text_response!r}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Check coverage: every investigational product must be present as a key\n",
    "    missing = [d for d in investigational_products if d not in classifications]\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"⚠️ [{idx}/{total}] Coverage error for trial_hash={trial_hash}: \"\n",
    "            f\"missing classifications for {missing}\"\n",
    "        )\n",
    "        with counter_lock:\n",
    "            innov_counter[\"coverage_error\"] += 1\n",
    "        # DO NOT save this trial so it can be re-run next time\n",
    "        return\n",
    "\n",
    "    # Sanity check: each value has classification, explanation, tt_drug_id\n",
    "    for d in investigational_products:\n",
    "        meta = classifications.get(d, {})\n",
    "        if not isinstance(meta, dict):\n",
    "            print(f\"⚠️ [{idx}/{total}] Invalid meta for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "        if (\"classification\" not in meta) or (\"explanation\" not in meta) or (\"tt_drug_id\" not in meta):\n",
    "            print(f\"⚠️ [{idx}/{total}] Missing fields for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"investigational_products\": investigational_products,\n",
    "        \"classifications\": classifications,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    # Save per-trial JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (INNOV_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_lock:\n",
    "        master_innov[trial_hash] = mapped\n",
    "        MASTER_INNOV_PATH.write_text(\n",
    "            json.dumps(master_innov, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        innov_counter[\"processed\"] += 1\n",
    "        if innov_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {innov_counter['processed']} trials for innovation status...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LOAD & MERGE DATA\n",
    "# -------------------------------------------------\n",
    "# Load breakdown (investigational products + all drug-role cols)\n",
    "df_breakdown = pd.read_csv(PRODUCT_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "df_breakdown[\"investigational_products_parsed\"] = df_breakdown[\"investigational_products\"].apply(parse_listish)\n",
    "mask_has_inv = df_breakdown[\"investigational_products_parsed\"].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Restrict to rows with investigational products\n",
    "df_breakdown_sub = df_breakdown.loc[mask_has_inv].copy()\n",
    "\n",
    "# All columns from trial_product_breakdown.csv except trial_hash (which is already separate)\n",
    "BREAKDOWN_COLS = [c for c in df_breakdown_sub.columns if c != \"trial_hash\"]\n",
    "\n",
    "# Load raw trials (for RELEVANT_COLS)\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Merge on trial_hash; keep all breakdown columns + investigational_products_parsed + RELEVANT_COLS\n",
    "df_merged = df_breakdown_sub.merge(\n",
    "    df_trials[[\"trial_hash\"] + RELEVANT_COLS],\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "innov_rows = df_merged.to_dict(orient=\"records\")\n",
    "total_innov = len(innov_rows)\n",
    "print(f\"Loaded {total_innov} trials with investigational products for innovation-status classification.\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY\n",
    "# -------------------------------------------------\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_INNOV) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_innov_row, row, idx, total_innov, BREAKDOWN_COLS): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(innov_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        th = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error (innovation) trial_hash={th}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Trial investigational-drug innovation classification complete. \"\n",
    "    f\"processed={innov_counter['processed']}, \"\n",
    "    f\"skipped={innov_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={innov_counter['llm_error']}, \"\n",
    "    f\"parse_error={innov_counter['parse_error']}, \"\n",
    "    f\"coverage_error={innov_counter['coverage_error']}\"\n",
    ")\n",
    "print(f\"Classifications directory: {INNOV_DIR}\")\n",
    "print(f\"Log directory:             {INNOV_LOG_DIR}\")\n",
    "print(f\"Master classifications:    {MASTER_INNOV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/trial_investigational_drugs_classifications_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OUT_CSV = BASE_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fp in INNOV_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    inv_products_raw = obj.get(\"investigational_products\") or []\n",
    "    classifications_map = obj.get(\"classifications\") or {}\n",
    "\n",
    "    flat_products = []\n",
    "    flat_classifications = []\n",
    "\n",
    "    for drug_raw in inv_products_raw:\n",
    "        # drug_raw might be \"['inetetamab', 'toripalimab']\" or just \"SSGJ-707\"\n",
    "        if isinstance(drug_raw, str):\n",
    "            parsed_names = parse_listish(drug_raw)  # from earlier cell (uses ast.literal_eval)\n",
    "        else:\n",
    "            parsed_names = [drug_raw]\n",
    "\n",
    "        # Prefer classification using the exact key that was sent to the model\n",
    "        meta = classifications_map.get(drug_raw, {})\n",
    "        cls = meta.get(\"classification\", \"\")\n",
    "\n",
    "        # If not found, try each parsed name as a key\n",
    "        if not cls:\n",
    "            for name in parsed_names:\n",
    "                meta_n = classifications_map.get(name, {})\n",
    "                if \"classification\" in meta_n:\n",
    "                    cls = meta_n.get(\"classification\", \"\")\n",
    "                    break\n",
    "\n",
    "        if not cls:\n",
    "            print(\n",
    "                f\"⚠️ Missing classification for raw drug {drug_raw!r} in \"\n",
    "                f\"trial_hash={trial_hash}, file={fp.name}\"\n",
    "            )\n",
    "\n",
    "        # Add one entry per parsed name so both lists are flat and aligned\n",
    "        for name in parsed_names:\n",
    "            flat_products.append(name)\n",
    "            flat_classifications.append(cls)\n",
    "\n",
    "    # Sanity check: lengths must match\n",
    "    if len(flat_products) != len(flat_classifications):\n",
    "        print(\n",
    "            f\"⚠️ Length mismatch for trial_hash={trial_hash}: \"\n",
    "            f\"{len(flat_products)} products vs {len(flat_classifications)} classifications\"\n",
    "        )\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "            # store as JSON stringified flat lists\n",
    "            \"investigational_products\": json.dumps(flat_products, ensure_ascii=False),\n",
    "            \"investigational_products_classifications\": json.dumps(flat_classifications, ensure_ascii=False),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved investigational drug classifications to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96291d",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce1036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubMed search for each drug's MOA (HASH-BASED OUTPUT FILENAMES)\n",
    "\n",
    "import os, json, time, html, unicodedata\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -----------------------------\n",
    "# Paths / Config\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "PRODUCT_MASTER_PATH = BASE_DIR / \"product_id_master_by_did.json\"\n",
    "OUT_DIR             = BASE_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INDEX_PATH   = BASE_DIR / \"investigational_drug_moa_pubmed_index.json\"\n",
    "\n",
    "EUTILS     = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "API_KEY    = os.getenv(\"NCBI_API_KEY\") or None\n",
    "EMAIL      = os.getenv(\"NCBI_EMAIL\") or None\n",
    "SLEEP      = 0.25\n",
    "RETRY_MAX  = 3\n",
    "RETRY_WAIT = 1.0\n",
    "\n",
    "# LLM config (for MOA refinement)\n",
    "MODEL  = \"gpt-5-mini\"   # adjust if needed\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "NAN_STRINGS = {\"nan\", \"none\", \"null\", \"\"}\n",
    "\n",
    "def _clean(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s_str = str(s).strip()\n",
    "    return \"\" if s_str.lower() in NAN_STRINGS else s_str\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = \" \".join(t.strip().lower().split())\n",
    "    return \"\" if t in NAN_STRINGS else t\n",
    "\n",
    "def _http_get_with_retry(url: str, params: dict, timeout: int) -> requests.Response:\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "def esearch_ids(term: str, n: int = 3) -> list[str]:\n",
    "    term = _clean(term)\n",
    "    if not term:\n",
    "        return []\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": n,\n",
    "        \"sort\": \"relevance\",\n",
    "    }\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/esearch.fcgi\", params=params, timeout=30)\n",
    "    return r.json().get(\"esearchresult\", {}).get(\"idlist\", []) or []\n",
    "\n",
    "def _parse_xml_with_retry(text: str) -> ET.Element:\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            return ET.fromstring(text)\n",
    "        except ET.ParseError as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "def efetch_details(pmids: list[str]) -> dict:\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    params = {\"db\": \"pubmed\", \"id\": \",\".join(pmids), \"retmode\": \"xml\"}\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/efetch.fcgi\", params=params, timeout=60)\n",
    "    root = _parse_xml_with_retry(r.text)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    def text_from_el(el):\n",
    "        return \"\".join(el.itertext()).strip() if el is not None else \"\"\n",
    "\n",
    "    def join_abstract(abs_parent):\n",
    "        parts = []\n",
    "        for t in abs_parent.findall(\"AbstractText\"):\n",
    "            label = t.attrib.get(\"Label\")\n",
    "            txt = text_from_el(t)\n",
    "            if txt:\n",
    "                parts.append(f\"{label}: {txt}\" if label else txt)\n",
    "        return \"\\n\".join(parts).strip()\n",
    "\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid_el = art.find(\".//MedlineCitation/PMID\")\n",
    "        if pmid_el is None or not (pmid_el.text or \"\").strip():\n",
    "            continue\n",
    "        pmid = pmid_el.text.strip()\n",
    "\n",
    "        title = text_from_el(art.find(\".//Article/ArticleTitle\"))\n",
    "        abs_parent = art.find(\".//Article/Abstract\")\n",
    "        abstract = join_abstract(abs_parent) if abs_parent is not None else \"\"\n",
    "\n",
    "        mesh_terms = []\n",
    "        for mh in art.findall(\".//MedlineCitation/MeshHeadingList/MeshHeading\"):\n",
    "            desc = mh.find(\"DescriptorName\")\n",
    "            if desc is None or not (desc.text or \"\").strip():\n",
    "                continue\n",
    "            d_text = desc.text.strip()\n",
    "            d_major = desc.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "            d_str = f\"{d_text}{'*' if d_major else ''}\"\n",
    "\n",
    "            quals = []\n",
    "            for q in mh.findall(\"QualifierName\"):\n",
    "                q_text = (q.text or \"\").strip()\n",
    "                if q_text:\n",
    "                    q_major = q.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "                    quals.append(f\"{q_text}{'*' if q_major else ''}\")\n",
    "\n",
    "            mesh_terms.append(d_str if not quals else d_str + \" / \" + \"; \".join(quals))\n",
    "\n",
    "        substances = []\n",
    "        for chem in art.findall(\".//Chemical\"):\n",
    "            nm_el = chem.find(\"NameOfSubstance\")\n",
    "            rn_el = chem.find(\"RegistryNumber\")\n",
    "            nm = nm_el.text.strip() if nm_el is not None else \"\"\n",
    "            rn = rn_el.text.strip() if rn_el is not None else \"\"\n",
    "            if nm and rn and rn != \"0\":\n",
    "                substances.append(f\"{nm} [RN:{rn}]\")\n",
    "            elif nm:\n",
    "                substances.append(nm)\n",
    "            elif rn and rn != \"0\":\n",
    "                substances.append(f\"[RN:{rn}]\")\n",
    "\n",
    "        def uniq(xs):\n",
    "            seen, out_local = set(), []\n",
    "            for x in xs:\n",
    "                if x and x not in seen:\n",
    "                    seen.add(x)\n",
    "                    out_local.append(x)\n",
    "            return out_local\n",
    "\n",
    "        out[pmid] = {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"mesh_terms\": uniq(mesh_terms),\n",
    "            \"substances\": uniq(substances),\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "def load_json_or_empty(path: Path) -> dict:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def split_terms(s: str):\n",
    "    \"\"\"\n",
    "    For MOA strings like:\n",
    "      'Thrombopoietin receptor agonist (recombinant growth factor); PEGylated recombinant human EPO'\n",
    "    we split on ';' and treat each piece as a candidate search term.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    raw = [t.strip() for t in str(s).split(\";\")]\n",
    "    return [t for t in raw if t and t.lower() not in NAN_STRINGS]\n",
    "\n",
    "# --------------- LLM refinement helpers ---------------\n",
    "\n",
    "def build_moa_refinement_prompt(mechanism: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the chatbot to turn a free-text MOA into a concise, canonical\n",
    "    mechanism-of-action phrase suitable for PubMed search.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert clinical pharmacologist and mechanisms-of-action classifier.\n",
    "\n",
    "Given the following mechanism-of-action (MOA) description from a drug development database:\n",
    "\n",
    "\\\"\\\"\\\"{mechanism}\\\"\\\"\\\"\n",
    "\n",
    "Rewrite or condense it into a SHORT, CANONICAL mechanism-of-action term that would work well as a PubMed search term.\n",
    "\n",
    "Rules:\n",
    "- Output a concise mechanism class or well-recognized pharmacologic concept, not a full sentence.\n",
    "- Prefer standard pharmacologic/mechanistic classes (e.g. \"Ion Exchange Resins\", \"Immunocytokines\",\n",
    "  \"Kinase Inhibitors\", \"Antibodies, Monoclonal\", \"Immune Checkpoint Inhibitors\").\n",
    "- Do NOT include long target listings or extra explanation.\n",
    "- If the original MOA is already an appropriate concise search term, you may return it unchanged.\n",
    "\n",
    "Return ONLY the refined mechanism phrase, with no additional explanation or formatting.\n",
    "\"\"\".strip()\n",
    "\n",
    "def refine_mechanism_with_llm(mechanism: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use the OpenAIWrapper .query() interface to get a refined mechanism phrase.\n",
    "    Returns the refined phrase or None on failure.\n",
    "    \"\"\"\n",
    "    mech_clean = _clean(mechanism)\n",
    "    if not mech_clean:\n",
    "        return None\n",
    "\n",
    "    prompt = build_moa_refinement_prompt(mech_clean)\n",
    "\n",
    "    try:\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        text = (res.get(\"text_response\") or \"\").strip()\n",
    "        # Strip surrounding quotes if the model adds them\n",
    "        text = text.strip().strip('\"').strip(\"'\")\n",
    "        refined = _clean(text)\n",
    "        return refined or None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LLM refinement failed for mechanism='{mech_clean[:80]}': {e}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Load product master (by did)\n",
    "# -----------------------------\n",
    "product_master_by_did = load_json_or_empty(PRODUCT_MASTER_PATH)\n",
    "if not product_master_by_did:\n",
    "    raise RuntimeError(f\"No product entries found in {PRODUCT_MASTER_PATH}\")\n",
    "\n",
    "master_index = load_json_or_empty(MASTER_INDEX_PATH) or {}\n",
    "\n",
    "total = len(product_master_by_did)\n",
    "print(f\"{total} drug entries (did_*) to process\")\n",
    "processed = 0\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: one PubMed search per DRUG (by did)\n",
    "# -----------------------------\n",
    "for did, rec in product_master_by_did.items():\n",
    "    # Skip if already indexed\n",
    "    if did in master_index:\n",
    "        mech_list = rec.get(\"product_mechanisms\", []) or []\n",
    "        mech_preview = \"; \".join(mech_list)[:60]\n",
    "        print(f\"{mech_preview} || already processed for {did}\")\n",
    "        processed += 1\n",
    "        continue\n",
    "\n",
    "    # product_mechanisms is a list; join into a single string for a \"combo\" key,\n",
    "    # but we'll search EACH mechanism (and its ';'-split pieces) separately.\n",
    "    mech_list = rec.get(\"product_mechanisms\", []) or []\n",
    "    mechanism = _clean(\"; \".join(mech_list))\n",
    "    if not mechanism:\n",
    "        print(f\"⚠️ Empty mechanism list for did={did}, skipping\")\n",
    "        continue\n",
    "\n",
    "    mech_key = norm_text(mechanism)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build candidate search terms\n",
    "    # -----------------------------\n",
    "    # For MOAs: search EACH mechanism string (and each ';'-split subterm).\n",
    "    mechanism_terms: list[str] = []\n",
    "    for mech in mech_list:\n",
    "        mech = _clean(mech)\n",
    "        if not mech:\n",
    "            continue\n",
    "        subterms = split_terms(mech)\n",
    "        if not subterms:\n",
    "            subterms = [mech]\n",
    "        for t in subterms:\n",
    "            t_clean = _clean(t)\n",
    "            if t_clean and t_clean not in mechanism_terms:\n",
    "                mechanism_terms.append(t_clean)\n",
    "\n",
    "    # For molecular targets: direct terms\n",
    "    target_terms: list[str] = []\n",
    "    for tgt in rec.get(\"molecular_targets\", []) or []:\n",
    "        t_clean = _clean(tgt)\n",
    "        if t_clean and t_clean not in target_terms:\n",
    "            target_terms.append(t_clean)\n",
    "\n",
    "    if not mechanism_terms and not target_terms:\n",
    "        print(f\"⚠️ No usable mechanism or target terms for did={did}, skipping\")\n",
    "        continue\n",
    "\n",
    "    tried_terms: list[str] = []\n",
    "    first_hit_term: str | None = None\n",
    "    llm_refined: str | None = None\n",
    "\n",
    "    # Detailed per-term results\n",
    "    mechanism_search: dict[str, dict] = {}\n",
    "    target_search: dict[str, dict] = {}\n",
    "\n",
    "    # Aggregate across all searches for summary\n",
    "    all_pmids: set[str] = set()\n",
    "    all_records: dict[str, dict] = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Mechanism term searches\n",
    "    # -----------------------------\n",
    "    for term in mechanism_terms:\n",
    "        tried_terms.append(term)\n",
    "        query = f\"\\\"{term}\\\"\"\n",
    "        try:\n",
    "            pmids = esearch_ids(query, n=5)\n",
    "        except Exception:\n",
    "            pmids = []\n",
    "\n",
    "        records = {}\n",
    "        if pmids:\n",
    "            try:\n",
    "                records = efetch_details(pmids)\n",
    "            except Exception as e:\n",
    "                records = {\"_error\": str(e)}\n",
    "\n",
    "            # Track first term that hits\n",
    "            if first_hit_term is None and pmids:\n",
    "                first_hit_term = term\n",
    "\n",
    "            for p in pmids:\n",
    "                all_pmids.add(p)\n",
    "                if p not in all_records and p in records:\n",
    "                    all_records[p] = records[p]\n",
    "\n",
    "        mechanism_search[term] = {\n",
    "            \"pmids\": pmids,\n",
    "            \"records\": records,\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) LLM refinement if NO mechanism hits\n",
    "    # -----------------------------\n",
    "    if not all_pmids:\n",
    "        llm_refined = refine_mechanism_with_llm(mechanism)\n",
    "        if llm_refined:\n",
    "            llm_term_key = llm_refined  # store as-is\n",
    "            tried_terms.append(llm_refined + \" [LLM]\")\n",
    "            query = f\"\\\"{llm_refined}\\\"\"\n",
    "            try:\n",
    "                pmids = esearch_ids(query, n=5)\n",
    "            except Exception:\n",
    "                pmids = []\n",
    "\n",
    "            records = {}\n",
    "            if pmids:\n",
    "                try:\n",
    "                    records = efetch_details(pmids)\n",
    "                except Exception as e:\n",
    "                    records = {\"_error\": str(e)}\n",
    "\n",
    "                if first_hit_term is None and pmids:\n",
    "                    first_hit_term = llm_refined\n",
    "\n",
    "                for p in pmids:\n",
    "                    all_pmids.add(p)\n",
    "                    if p not in all_records and p in records:\n",
    "                        all_records[p] = records[p]\n",
    "\n",
    "            mechanism_search[llm_term_key] = {\n",
    "                \"pmids\": pmids,\n",
    "                \"records\": records,\n",
    "                \"llm_refined\": True,\n",
    "            }\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Molecular target term searches (10 PMIDs each)\n",
    "    # -----------------------------\n",
    "    for term in target_terms:\n",
    "        tried_terms.append(term)\n",
    "        query = f\"\\\"{term}\\\"\"\n",
    "        try:\n",
    "            pmids = esearch_ids(query, n=10)  # ← 10 studies per target term\n",
    "        except Exception:\n",
    "            pmids = []\n",
    "\n",
    "        records = {}\n",
    "        if pmids:\n",
    "            try:\n",
    "                records = efetch_details(pmids)\n",
    "            except Exception as e:\n",
    "                records = {\"_error\": str(e)}\n",
    "\n",
    "            if first_hit_term is None and pmids:\n",
    "                first_hit_term = term\n",
    "\n",
    "            for p in pmids:\n",
    "                all_pmids.add(p)\n",
    "                if p not in all_records and p in records:\n",
    "                    all_records[p] = records[p]\n",
    "\n",
    "        target_search[term] = {\n",
    "            \"pmids\": pmids,\n",
    "            \"records\": records,\n",
    "        }\n",
    "\n",
    "    # 4) If STILL no PMIDs at all, skip saving (so you can rerun later)\n",
    "    if not all_pmids:\n",
    "        print(f\"⚠️ No PubMed hits for did={did} after mechanisms + targets + LLM, skipping\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # HASH-BASED OUTPUT (BY did)\n",
    "    # -----------------------------\n",
    "    fname    = f\"{did}.json\"\n",
    "    out_path = OUT_DIR / fname\n",
    "\n",
    "    payload = {\n",
    "        \"type\": \"drug_moa_pubmed_search\",\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": rec.get(\"tt_drug_id\"),\n",
    "        \"drug_names\": rec.get(\"drug_names\", []),\n",
    "        \"alternative_names\": rec.get(\"alternative_names\", []),\n",
    "        \"molecular_targets\": rec.get(\"molecular_targets\", []),\n",
    "        \"product_mechanisms\": mech_list,\n",
    "        \"mechanism_combined\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"tried_terms\": tried_terms,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "        # New detailed breakdowns:\n",
    "        \"mechanism_search\": mechanism_search,\n",
    "        \"target_search\": target_search,\n",
    "        # Backward-compatible summary:\n",
    "        \"match\": {\n",
    "            \"term\": first_hit_term,\n",
    "            \"pmids\": sorted(all_pmids),\n",
    "            \"records\": all_records,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    save_json(out_path, payload)\n",
    "\n",
    "    # Index entry keyed by did (summary only, as before)\n",
    "    master_index[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": rec.get(\"tt_drug_id\"),\n",
    "        \"drug_names\": rec.get(\"drug_names\", []),\n",
    "        \"product_mechanisms\": mech_list,\n",
    "        \"mechanism_combined\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"json_path\": f\"{OUT_DIR.name}/{fname}\",\n",
    "        \"pmids\": sorted(all_pmids),\n",
    "        \"matched_term\": first_hit_term,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "    }\n",
    "    save_json(MASTER_INDEX_PATH, master_index)\n",
    "\n",
    "    processed += 1\n",
    "    if processed % 50 == 0:\n",
    "        print(f\"Processed {processed}/{total}…\")\n",
    "\n",
    "    time.sleep(SLEEP)\n",
    "\n",
    "save_json(MASTER_INDEX_PATH, master_index)\n",
    "print(f\"Completed {processed} drug entries with at least one PubMed hit. Files written to {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fdab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper  # your wrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "MOA_PUBMED_DIR        = BASE_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "MOA_CHOICE_DIR        = BASE_DIR / \"investigational_drug_moa_chosen\"\n",
    "MOA_CHOICE_LOG_DIR    = BASE_DIR / \"investigational_drug_moa_chosen_log\"\n",
    "MASTER_MOA_CHOICES_PATH = BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "\n",
    "MOA_CHOICE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MOA_CHOICE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS_MOA = 8\n",
    "MODEL = \"gpt-5\"\n",
    "\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "def load_master_moa_choices() -> dict:\n",
    "    if not MASTER_MOA_CHOICES_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_MOA_CHOICES_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def build_moa_mesh_prompt(moa_payload: dict, candidate_mesh_terms: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the LLM to choose the best MeSH term that represents the mechanism of action.\n",
    "    If no suitable MeSH term exists, the model MUST return \"[none]\".\n",
    "    \"\"\"\n",
    "\n",
    "    payload_json = json.dumps(moa_payload, ensure_ascii=False, indent=2)\n",
    "    mesh_json    = json.dumps(candidate_mesh_terms, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert pharmacologist and MeSH annotation specialist.\n",
    "\n",
    "You are given:\n",
    "1) A mechanism-of-action (MOA) text string describing how a drug works.\n",
    "2) A set of PubMed-derived MeSH terms (candidate list).\n",
    "3) Condensed PubMed records used for MOA search.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 1 — Select the Best MeSH Term\n",
    "------------------------------------------------------------\n",
    "Choose EXACTLY ONE MeSH term that best represents the mechanism of action.\n",
    "\n",
    "Rules:\n",
    "- You MUST select a term *only* from the candidate list.\n",
    "- Choose the most mechanistic/specific pharmacologic concept available\n",
    "  (e.g., \"Receptor Antagonists\", \"Antibodies, Monoclonal\", \"Kinase Inhibitors\").\n",
    "- Avoid generic terms (\"Humans\", \"Adult\", \"Neoplasms\") unless absolutely no mechanistic term exists.\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 2 — Handle Cases with No Good Mechanistic Term\n",
    "------------------------------------------------------------\n",
    "If NONE of the candidate MeSH terms meaningfully represent the MOA:\n",
    "\n",
    "You MUST output:\n",
    "\n",
    "  \"chosen_mesh_term\": \"[none]\",\n",
    "  \"source_pmid\": null,\n",
    "  \"rationale\": \"Explain why no term fits.\"\n",
    "\n",
    "This is a VALID and EXPECTED outcome.\n",
    "\n",
    "------------------------------------------------------------\n",
    "OUTPUT FORMAT  (STRICT)\n",
    "------------------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object with EXACTLY these fields:\n",
    "\n",
    "{{\n",
    "  \"chosen_mesh_term\": \"<one exact candidate term OR '[none]'>\",\n",
    "  \"source_pmid\": \"<PMID you relied on OR null>\",\n",
    "  \"rationale\": \"One concise sentence explaining your decision.\"\n",
    "}}\n",
    "\n",
    "Constraints:\n",
    "- If you choose a MeSH term, it MUST MATCH EXACTLY one item from the candidate list.\n",
    "- If no suitable term exists, return \"[none]\".\n",
    "- JSON must be valid and parseable.\n",
    "\n",
    "------------------------------------------------------------\n",
    "MOA Payload (input data)\n",
    "------------------------------------------------------------\n",
    "{payload_json}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Candidate MeSH Terms\n",
    "------------------------------------------------------------\n",
    "{mesh_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "master_moa_choices = load_master_moa_choices()\n",
    "master_moa_lock = threading.Lock()\n",
    "\n",
    "moa_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,   # includes \"chosen term not in JSON-derived list\"\n",
    "    \"no_candidates\": 0,\n",
    "}\n",
    "moa_counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_moa_file(fp: Path, idx: int, total: int) -> None:\n",
    "    \"\"\"Process a single MOA PubMed-search JSON file.\"\"\"\n",
    "    try:\n",
    "        payload = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] Error reading {fp.name}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    moa_id = (\n",
    "        payload.get(\"moa_id\")\n",
    "        or payload.get(\"did\")   \n",
    "        or fp.stem\n",
    "    )\n",
    "\n",
    "    mechanism = (\n",
    "        payload.get(\"mechanism\")        \n",
    "        or payload.get(\"mechanism_combined\")\n",
    "        or \"; \".join(payload.get(\"product_mechanisms\", []) or [])\n",
    "        or \"\"\n",
    "    )\n",
    "\n",
    "    if not moa_id:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing moa_id in {fp.name}, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = MOA_CHOICE_DIR / f\"{moa_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    match = payload.get(\"match\") or {}\n",
    "    records = match.get(\"records\") or {}\n",
    "    pmids = match.get(\"pmids\") or []\n",
    "\n",
    "    # Collect candidate MeSH terms (unique, in stable order) FROM THE JSON ONLY\n",
    "    candidate_terms = []\n",
    "    seen_terms = set()\n",
    "    for pmid, rec in records.items():\n",
    "        mesh_terms = rec.get(\"mesh_terms\") or []\n",
    "        for term in mesh_terms:\n",
    "            if term and term not in seen_terms:\n",
    "                seen_terms.add(term)\n",
    "                candidate_terms.append(term)\n",
    "\n",
    "    if not candidate_terms:\n",
    "        print(f\"⚠️ [{idx}/{total}] No candidate MeSH terms for moa_id={moa_id}, skipping\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"no_candidates\"] += 1\n",
    "        return\n",
    "\n",
    "    # Condensed payload for the model (avoid full abstracts to save tokens)\n",
    "    condensed_records = {\n",
    "        pmid: {\n",
    "            \"title\": (rec.get(\"title\") or \"\"),\n",
    "            \"mesh_terms\": (rec.get(\"mesh_terms\") or []),\n",
    "        }\n",
    "        for pmid, rec in records.items()\n",
    "    }\n",
    "\n",
    "    moa_payload = {\n",
    "        \"moa_id\": moa_id,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"tried_terms\": payload.get(\"tried_terms\") or [],\n",
    "        \"pmids\": pmids,\n",
    "        \"records\": condensed_records,\n",
    "    }\n",
    "\n",
    "    prompt = build_moa_mesh_prompt(moa_payload, candidate_terms)\n",
    "\n",
    "    token = moa_id\n",
    "    hash_id = moa_id\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for moa_id={moa_id}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    obj = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(obj, dict) or not obj:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse error moa_id={moa_id}, raw={text_response!r}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    chosen_term = obj.get(\"chosen_mesh_term\")\n",
    "    source_pmid = obj.get(\"source_pmid\")\n",
    "    rationale = obj.get(\"rationale\")\n",
    "\n",
    "    # HARD CHECK: chosen term\n",
    "    if not chosen_term or not isinstance(chosen_term, str):\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing or invalid chosen_mesh_term for moa_id={moa_id}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"coverage_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Special allowed sentinel for \"no good term\"\n",
    "    if chosen_term == \"[none]\":\n",
    "        # Accept even though it's not in candidate_terms\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "    else:\n",
    "        # For any real term, it MUST come from the JSON-derived candidate list\n",
    "        if chosen_term not in candidate_terms:\n",
    "            # DNE in JSON (hallucinated or modified term) → reject, do NOT save\n",
    "            print(\n",
    "                f\"⚠️ [{idx}/{total}] chosen_mesh_term not in JSON-derived candidate list \"\n",
    "                f\"for moa_id={moa_id}: {chosen_term!r}\"\n",
    "            )\n",
    "            with moa_counter_lock:\n",
    "                moa_counter[\"coverage_error\"] += 1\n",
    "            return\n",
    "\n",
    "        # Optional: source_pmid sanity check (must be one of pmids or None)\n",
    "        if source_pmid is not None and source_pmid not in pmids:\n",
    "            print(\n",
    "                f\"⚠️ [{idx}/{total}] source_pmid {source_pmid!r} not in pmids for moa_id={moa_id}; \"\n",
    "                f\"still accepting chosen_mesh_term\"\n",
    "            )\n",
    "\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "\n",
    "    # Save per-MOA JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (MOA_CHOICE_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_moa_lock:\n",
    "        master_moa_choices[moa_id] = mapped\n",
    "        MASTER_MOA_CHOICES_PATH.write_text(\n",
    "            json.dumps(master_moa_choices, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with moa_counter_lock:\n",
    "        moa_counter[\"processed\"] += 1\n",
    "        if moa_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {moa_counter['processed']} MOA entries...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN CONCURRENTLY OVER MOA PUBMED SEARCH FILES\n",
    "# -------------------------------------------------\n",
    "moa_files = sorted(MOA_PUBMED_DIR.glob(\"*.json\"))\n",
    "total_moa = len(moa_files)\n",
    "print(f\"Loaded {total_moa} MOA PubMed-search files for MeSH-term selection.\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_MOA) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_moa_file, fp, idx, total_moa): fp.name\n",
    "        for idx, fp in enumerate(moa_files, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        name = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error (MOA MeSH selection) file={name}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"MOA MeSH-term selection complete. \"\n",
    "    f\"processed={moa_counter['processed']}, \"\n",
    "    f\"skipped={moa_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={moa_counter['llm_error']}, \"\n",
    "    f\"parse_error={moa_counter['parse_error']}, \"\n",
    "    f\"coverage_error={moa_counter['coverage_error']}, \"\n",
    "    f\"no_candidates={moa_counter['no_candidates']}\"\n",
    ")\n",
    "print(f\"Chosen MOA directory: {MOA_CHOICE_DIR}\")\n",
    "print(f\"Log directory:        {MOA_CHOICE_LOG_DIR}\")\n",
    "print(f\"Master choices:       {MASTER_MOA_CHOICES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b762f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "IN_BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "PRODUCT_MASTER_BY_DID = BASE_DIR / \"product_id_master_by_did.json\"\n",
    "MOA_MASTER_PATH = BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "OUT_BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown_w_chosen_mechanisms.csv\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helpers\n",
    "# ----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse strings like \"['a','b']\" into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "def insert_after(df, col, newcol, values):\n",
    "    cols = list(df.columns)\n",
    "    if col not in cols:\n",
    "        df[newcol] = values\n",
    "        return\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "def is_none_term(s: str) -> bool:\n",
    "    \"\"\"True if the term represents '[none]' or equivalent.\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    s2 = s.strip().lower()\n",
    "    return s2 in (\"[none]\", \"none\", \"\")\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Simple normalization for name matching.\"\"\"\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load inputs\n",
    "# ----------------------------------------\n",
    "df = pd.read_csv(IN_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trial breakdown: {IN_BREAKDOWN_CSV}, shape={df.shape}\")\n",
    "\n",
    "if not PRODUCT_MASTER_BY_DID.exists():\n",
    "    raise FileNotFoundError(f\"No product master by did found at {PRODUCT_MASTER_BY_DID}\")\n",
    "if not MOA_MASTER_PATH.exists():\n",
    "    raise FileNotFoundError(f\"No MOA master file found at {MOA_MASTER_PATH}\")\n",
    "\n",
    "product_by_did = json.loads(PRODUCT_MASTER_BY_DID.read_text(encoding=\"utf-8\"))\n",
    "moa_master = json.loads(MOA_MASTER_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# ----------------------------------------\n",
    "# did -> chosen_mesh_term (exclude \"[none]\")\n",
    "# ----------------------------------------\n",
    "did_to_mesh = {}\n",
    "for did, rec in moa_master.items():\n",
    "    mesh = (rec.get(\"chosen_mesh_term\") or \"\").strip()\n",
    "    if mesh and not is_none_term(mesh):\n",
    "        did_to_mesh[did] = mesh\n",
    "\n",
    "print(f\"did_to_mesh (excluding [none]) entries: {len(did_to_mesh)}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Build trial_hash -> list of dids (only those with a chosen mesh term)\n",
    "# and did -> normalized name set for matching\n",
    "# ----------------------------------------\n",
    "trial_to_dids = {}\n",
    "did_to_names_norm = {}\n",
    "\n",
    "for did, rec in product_by_did.items():\n",
    "    if did not in did_to_mesh:\n",
    "        # If we don't have a chosen MeSH term, skip this did for mapping purposes\n",
    "        continue\n",
    "\n",
    "    # Collect all names / alt names\n",
    "    drug_names = rec.get(\"drug_names\", []) or []\n",
    "    alt_names = rec.get(\"alternative_names\", []) or []\n",
    "    all_names = set(drug_names) | set(alt_names)\n",
    "\n",
    "    names_norm = {norm_name(n) for n in all_names if str(n).strip()}\n",
    "    if not names_norm:\n",
    "        continue\n",
    "\n",
    "    did_to_names_norm[did] = names_norm\n",
    "\n",
    "    # Map each trial_hash to this did\n",
    "    trial_hashes = rec.get(\"trial_hashes\", []) or []\n",
    "    for th in trial_hashes:\n",
    "        th_str = str(th).strip()\n",
    "        if not th_str:\n",
    "            continue\n",
    "        trial_to_dids.setdefault(th_str, []).append(did)\n",
    "\n",
    "print(f\"trial_to_dids entries: {len(trial_to_dids)}\")\n",
    "print(f\"did_to_names_norm entries: {len(did_to_names_norm)}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# For each trial row, build chosen MeSH mechanism lists per role\n",
    "# based on matching product NAMES (not tt_drug_id) within that trial.\n",
    "# ----------------------------------------\n",
    "ROLE_NAME_SPECS = [\n",
    "    (\"investigational_products\", \"investigational_products_mechanism_mesh_terms\"),\n",
    "    (\"active_comparators\", \"active_comparators_mechanism_mesh_terms\"),\n",
    "    (\"standard_of_care\", \"standard_of_care_mechanism_mesh_terms\"),\n",
    "]\n",
    "\n",
    "# Prepare containers\n",
    "new_cols = {spec[1]: [] for spec in ROLE_NAME_SPECS}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    candidate_dids = trial_to_dids.get(trial_hash, [])\n",
    "\n",
    "    # Fast path: if no dids for this trial, all lists are empty\n",
    "    if not candidate_dids:\n",
    "        for _, new_col in ROLE_NAME_SPECS:\n",
    "            # But we still need correct length per role (one entry per product name)\n",
    "            base_col = [spec[0] for spec in ROLE_NAME_SPECS if spec[1] == new_col][0]\n",
    "            names_list = parse_listish(row.get(base_col, \"\"))\n",
    "            mesh_list = [\"\" for _ in names_list]\n",
    "            new_cols[new_col].append(str(mesh_list))\n",
    "        continue\n",
    "\n",
    "    # For each role, match by name\n",
    "    for base_col, new_col in ROLE_NAME_SPECS:\n",
    "        names_list = parse_listish(row.get(base_col, \"\"))\n",
    "        mesh_list = []\n",
    "\n",
    "        for prod_name in names_list:\n",
    "            name_norm = norm_name(prod_name)\n",
    "            if not name_norm:\n",
    "                mesh_list.append(\"\")\n",
    "                continue\n",
    "\n",
    "            meshes_for_this = set()\n",
    "\n",
    "            # Check each candidate did for this trial\n",
    "            for did in candidate_dids:\n",
    "                names_norm = did_to_names_norm.get(did, set())\n",
    "                if name_norm in names_norm:\n",
    "                    mesh = did_to_mesh.get(did, \"\")\n",
    "                    if mesh:\n",
    "                        meshes_for_this.add(mesh)\n",
    "\n",
    "            if not meshes_for_this:\n",
    "                mesh_list.append(\"\")\n",
    "            elif len(meshes_for_this) == 1:\n",
    "                mesh_list.append(next(iter(meshes_for_this)))\n",
    "            else:\n",
    "                mesh_list.append(\"; \".join(sorted(meshes_for_this)))\n",
    "\n",
    "        new_cols[new_col].append(str(mesh_list))\n",
    "\n",
    "# ----------------------------------------\n",
    "# Attach columns next to their name columns\n",
    "# ----------------------------------------\n",
    "for base_col, new_col in ROLE_NAME_SPECS:\n",
    "    insert_after(df, base_col, new_col, new_cols[new_col])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Save output\n",
    "# ----------------------------------------\n",
    "OUT_BREAKDOWN_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_BREAKDOWN_CSV, index=False)\n",
    "\n",
    "print(f\"✔️ Wrote trial breakdown with chosen MeSH mechanisms → {OUT_BREAKDOWN_CSV}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh\"\n",
    "OUT_DIR = Path(\"cache\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FILES = [\"desc2025.xml\", \"supp2025.xml\"]\n",
    "\n",
    "for fname in FILES:\n",
    "    url = f\"{BASE_URL}/{fname}\"\n",
    "    out_path = OUT_DIR / fname\n",
    "\n",
    "    # Skip if already downloaded\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        print(f\"Skipping {fname}, already exists.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"⬇Downloading {url} -> {out_path}\")\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    out_path.write_bytes(r.content)\n",
    "    print(f\"Downloaded {fname}\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, xml.etree.ElementTree as ET\n",
    "import html, unicodedata\n",
    "\n",
    "DESC_XML  = \"cache/desc2025.xml\"\n",
    "SUPP_XML  = \"cache/supp2025.xml\"\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\").replace(\"\\u2032\", \"'\").replace(\"\\u2033\", '\"')\n",
    "    t = t.replace(\"\\u201C\", '\"').replace(\"\\u201D\", '\"')\n",
    "    t = t.replace(\"\\u2010\", \"-\").replace(\"\\u2011\", \"-\").replace(\"\\u2012\", \"-\").replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    return \" \".join(t.strip().lower().split())\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Unicode-clean + collapse whitespace (preserve case).\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = t.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\").replace(\"\\u2032\", \"'\").replace(\"\\u2033\", '\"')\n",
    "    t = t.replace(\"\\u201C\", '\"').replace(\"\\u201D\", '\"')\n",
    "    t = t.replace(\"\\u2010\", \"-\").replace(\"\\u2011\", \"-\").replace(\"\\u2012\", \"-\").replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")\n",
    "    return \" \".join(t.strip().split())\n",
    "\n",
    "def _dedup(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "def _extract_scope_note_from_descriptor(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Prefer the ScopeNote of the PreferredConcept (PreferredConceptYN='Y'),\n",
    "    else fall back to the first ScopeNote present under any Concept.\n",
    "    \"\"\"\n",
    "    # Preferred concept first\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    # Any concept scope note as fallback\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def _extract_scope_note_from_supp(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    For SCRs, ScopeNote can also live under Concept.\n",
    "    Prefer the PreferredConcept (if flagged), else the first available.\n",
    "    \"\"\"\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def load_mesh_tree_and_id(desc_xml_fp: str, supp_xml_fp: str) -> dict[str, dict[str, list[str] | str]]:\n",
    "    # term_map[normalized_term] = {\"mesh_id\": <UI>, \"tree_numbers\": [..], \"scope_note\": <str>}\n",
    "    term_map: dict[str, dict[str, list[str] | str]] = {}\n",
    "\n",
    "    # Helper maps for fallbacks/joins\n",
    "    heading_to_tree: dict[str, list[str]] = {}\n",
    "    ui_to_tree: dict[str, list[str]] = {}\n",
    "    heading_to_scope: dict[str, str] = {}\n",
    "    ui_to_scope: dict[str, str] = {}\n",
    "\n",
    "    # --- Descriptors ---\n",
    "    if os.path.exists(desc_xml_fp):\n",
    "        root = ET.parse(desc_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//DescriptorRecord\"):\n",
    "            desc_ui = (rec.findtext(\"DescriptorUI\") or \"\").strip()\n",
    "            tree_numbers = _dedup([tn.text.strip() for tn in rec.findall(\".//TreeNumberList/TreeNumber\") if tn.text])\n",
    "\n",
    "            heading_raw = rec.findtext(\"DescriptorName/String\")\n",
    "            heading_norm = norm(heading_raw) if heading_raw else \"\"\n",
    "            scope_note = _extract_scope_note_from_descriptor(rec)\n",
    "\n",
    "            if heading_norm:\n",
    "                heading_to_tree[heading_norm] = tree_numbers\n",
    "                heading_to_scope[heading_norm] = scope_note\n",
    "            if desc_ui:\n",
    "                ui_to_tree[desc_ui] = tree_numbers\n",
    "                ui_to_scope[desc_ui] = scope_note\n",
    "\n",
    "            # Collect all terms mapped to this descriptor\n",
    "            terms = set()\n",
    "            if heading_raw:\n",
    "                terms.add(heading_norm)\n",
    "            for concept in rec.findall(\".//Concept\"):\n",
    "                for term in concept.findall(\".//Term\"):\n",
    "                    s = term.findtext(\"String\")\n",
    "                    if s:\n",
    "                        terms.add(norm(s))\n",
    "\n",
    "            for term in terms:\n",
    "                term_map[term] = {\n",
    "                    \"mesh_id\": desc_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": scope_note\n",
    "                }\n",
    "\n",
    "    # --- Supplementary (SCRs) ---\n",
    "    if os.path.exists(supp_xml_fp):\n",
    "        root = ET.parse(supp_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//SupplementalRecord\"):\n",
    "            supp_ui = (rec.findtext(\"SupplementalRecordUI\") or \"\").strip()\n",
    "\n",
    "            # Collect ALL names for this SCR\n",
    "            names = set()\n",
    "\n",
    "            for s in rec.findall(\".//SupplementalRecordName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/ConceptName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/TermList/Term/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            # Direct trees (often none for SCRs)\n",
    "            tree_numbers = [tn.text.strip() for tn in rec.findall(\".//TreeNumberList/TreeNumber\") if tn.text]\n",
    "\n",
    "            # SCR scope note (preferred concept first)\n",
    "            scr_scope_note = _extract_scope_note_from_supp(rec)\n",
    "\n",
    "            # Fallback via HeadingMappedTo (names and UIs)\n",
    "            mapped_scope_note = \"\"\n",
    "            if not tree_numbers:\n",
    "                # Try mapped names\n",
    "                mapped_names = [n.text.strip() for n in rec.findall(\".//HeadingMappedTo/DescriptorReferredTo/DescriptorName/String\") if n is not None and n.text]\n",
    "                for m in mapped_names:\n",
    "                    m_norm = norm(m)\n",
    "                    tns = heading_to_tree.get(m_norm)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if not mapped_scope_note and m_norm in heading_to_scope and heading_to_scope[m_norm]:\n",
    "                        mapped_scope_note = heading_to_scope[m_norm]\n",
    "\n",
    "                # Try mapped UIs\n",
    "                mapped_uis = [u.text.strip().lstrip(\"*\") for u in rec.findall(\".//HeadingMappedTo/DescriptorReferredTo/DescriptorUI\") if u is not None and u.text]\n",
    "                for mui in mapped_uis:\n",
    "                    tns = ui_to_tree.get(mui)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if not mapped_scope_note and mui in ui_to_scope and ui_to_scope[mui]:\n",
    "                        mapped_scope_note = ui_to_scope[mui]\n",
    "\n",
    "                tree_numbers = _dedup(tree_numbers)\n",
    "\n",
    "            final_scope_note = scr_scope_note or mapped_scope_note or \"\"\n",
    "\n",
    "            for name in names:\n",
    "                # Keep Descriptor mapping if already present for same term\n",
    "                if name in term_map and str(term_map[name].get(\"mesh_id\", \"\")).startswith(\"D\"):\n",
    "                    continue\n",
    "                term_map[name] = {\n",
    "                    \"mesh_id\": supp_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": final_scope_note\n",
    "                }\n",
    "\n",
    "    return term_map\n",
    "\n",
    "# -------- Run --------\n",
    "TREE_INDEX = load_mesh_tree_and_id(DESC_XML, SUPP_XML)\n",
    "print(f\"Loaded MeSH index terms: {len(TREE_INDEX):,} unique normalized terms\")\n",
    "\n",
    "# Quick checks\n",
    "for q in [\"sotatercept\", \"ACE-011\", \"winrevair\"]:\n",
    "    k = norm(q)\n",
    "    print(q, \"→\", TREE_INDEX.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "TRIALS_IN_PATH  = BASE_DIR / \"trial_product_breakdown_w_chosen_mechanisms.csv\"\n",
    "TRIALS_OUT_PATH = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# Sanity: TREE_INDEX and norm must already be loaded\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    TREE_INDEX\n",
    "except NameError:\n",
    "    raise RuntimeError(\"TREE_INDEX is not defined — run the MeSH loader cell first.\")\n",
    "\n",
    "try:\n",
    "    norm\n",
    "except NameError:\n",
    "    raise RuntimeError(\"norm() is not defined — ensure it is defined in the MeSH loader cell.\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------\n",
    "def parse_listish(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def mesh_heading_to_tree_numbers(chosen_mesh_term: str):\n",
    "    \"\"\"\n",
    "    Return *all* tree numbers for the chosen MeSH heading.\n",
    "    \"\"\"\n",
    "    if not chosen_mesh_term or chosen_mesh_term == \"[none]\":\n",
    "        return []\n",
    "    base = chosen_mesh_term.split(\" / \")[0].replace(\"*\", \"\").strip()\n",
    "    key = norm(base)\n",
    "    info = TREE_INDEX.get(key)\n",
    "    if not info:\n",
    "        return []\n",
    "    return info.get(\"tree_numbers\", []) or []\n",
    "\n",
    "\n",
    "# Clinical-pharmacology-ish heuristic for ONE primary tree number\n",
    "PRIORITY_PREFIXES = [\n",
    "    \"D12.\",  # Proteins: receptors, enzymes, cytokines, antibodies (biologics / targets)\n",
    "    \"D27.\",  # Chemical Actions and Uses: classic pharmacologic classes\n",
    "    \"D02.\",  # Organic Chemicals: small-molecule drugs\n",
    "    \"D09.\",  # Carbohydrates\n",
    "    \"D23.\",  # Immunologic Factors\n",
    "    \"D26.\",  # Biological Factors\n",
    "]\n",
    "\n",
    "def _depth(tn: str) -> int:\n",
    "    # fewer segments = higher-level class\n",
    "    return len(tn.split(\".\"))\n",
    "\n",
    "def choose_primary_tree(tree_numbers):\n",
    "    \"\"\"\n",
    "    Given a list of tree numbers for ONE MeSH term,\n",
    "    choose a single 'primary' tree that best reflects the\n",
    "    pharmacologic / target-level concept.\n",
    "    \"\"\"\n",
    "    if not tree_numbers:\n",
    "        return \"\"\n",
    "\n",
    "    tns = [t.strip() for t in tree_numbers if isinstance(t, str) and t.strip()]\n",
    "    if not tns:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Prefer specific high-value branches (by prefix)\n",
    "    for prefix in PRIORITY_PREFIXES:\n",
    "        candidates = [t for t in tns if t.startswith(prefix)]\n",
    "        if candidates:\n",
    "            # choose the highest-level (shortest depth) node in that branch\n",
    "            return max(candidates, key=_depth)\n",
    "\n",
    "    # 2) Else prefer any Chemicals & Drugs branch (D*)\n",
    "    d_candidates = [t for t in tns if t.startswith(\"D\")]\n",
    "    if d_candidates:\n",
    "        return max(d_candidates, key=_depth)\n",
    "\n",
    "    # 3) Fallback: shortest overall\n",
    "    return max(tns, key=_depth)\n",
    "\n",
    "\n",
    "def trees_for_mesh_term_list(mesh_term_list):\n",
    "    \"\"\"\n",
    "    Given a list of MeSH headings (already mapped mechanism terms),\n",
    "    return:\n",
    "      all_tree_lists : list of [list-of-tree-numbers] per term\n",
    "      primary_trees  : list of ONE chosen tree number per term (\"\" if none)\n",
    "    \"\"\"\n",
    "    all_tree_lists = []\n",
    "    primary_trees = []\n",
    "\n",
    "    for term in mesh_term_list:\n",
    "        term_str = (term or \"\").strip()\n",
    "        if not term_str:\n",
    "            all_tree_lists.append([])\n",
    "            primary_trees.append(\"\")\n",
    "            continue\n",
    "\n",
    "        all_trees = mesh_heading_to_tree_numbers(term_str)\n",
    "        all_tree_lists.append(all_trees)\n",
    "\n",
    "        primary = choose_primary_tree(all_trees)\n",
    "        primary_trees.append(primary)\n",
    "\n",
    "    return all_tree_lists, primary_trees\n",
    "\n",
    "\n",
    "def insert_after(df, col, newcol, values):\n",
    "    cols = list(df.columns)\n",
    "    if col not in cols:\n",
    "        # fallback: append\n",
    "        df[newcol] = values\n",
    "        return\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Load trial dataset\n",
    "# -----------------------------------------\n",
    "df = pd.read_csv(TRIALS_IN_PATH, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trials: {TRIALS_IN_PATH}, shape={df.shape}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Compute tree-number columns row-wise\n",
    "# -----------------------------------------\n",
    "inv_trees_all  = []\n",
    "inv_trees_primary = []\n",
    "\n",
    "ac_trees_all   = []\n",
    "ac_trees_primary = []\n",
    "\n",
    "soc_trees_all  = []\n",
    "soc_trees_primary = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inv_mesh_terms = parse_listish(row.get(\"investigational_products_mechanism_mesh_terms\"))\n",
    "    ac_mesh_terms  = parse_listish(row.get(\"active_comparators_mechanism_mesh_terms\"))\n",
    "    soc_mesh_terms = parse_listish(row.get(\"standard_of_care_mechanism_mesh_terms\"))\n",
    "\n",
    "    inv_all_t, inv_primary_t = trees_for_mesh_term_list(inv_mesh_terms)\n",
    "    ac_all_t,  ac_primary_t  = trees_for_mesh_term_list(ac_mesh_terms)\n",
    "    soc_all_t, soc_primary_t = trees_for_mesh_term_list(soc_mesh_terms)\n",
    "\n",
    "    inv_trees_all.append(inv_all_t)\n",
    "    inv_trees_primary.append(inv_primary_t)\n",
    "\n",
    "    ac_trees_all.append(ac_all_t)\n",
    "    ac_trees_primary.append(ac_primary_t)\n",
    "\n",
    "    soc_trees_all.append(soc_all_t)\n",
    "    soc_trees_primary.append(soc_primary_t)\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# Insert columns next to the mechanism_mesh_terms columns\n",
    "# -----------------------------------------\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_mesh_terms\",\n",
    "    \"investigational_products_mechanism_tree_numbers\",\n",
    "    inv_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_tree_numbers\",\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    inv_trees_primary,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_mesh_terms\",\n",
    "    \"active_comparators_mechanism_tree_numbers\",\n",
    "    ac_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_tree_numbers\",\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    ac_trees_primary,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_mesh_terms\",\n",
    "    \"standard_of_care_mechanism_tree_numbers\",\n",
    "    soc_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "    soc_trees_primary,\n",
    ")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Save output\n",
    "# -----------------------------------------\n",
    "TRIALS_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(TRIALS_OUT_PATH, index=False)\n",
    "print(f\"✅ Wrote: {TRIALS_OUT_PATH}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e09af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Columns with MeSH mechanism terms (already mapped)\n",
    "MESH_TERM_COLS = [\n",
    "    \"investigational_products_mechanism_mesh_terms\",\n",
    "    \"active_comparators_mechanism_mesh_terms\",\n",
    "    \"standard_of_care_mechanism_mesh_terms\",\n",
    "]\n",
    "\n",
    "# Columns with *primary* tree numbers (one tree per mechanism)\n",
    "TREE_COLS = [\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "]\n",
    "\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        return val if isinstance(val, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def is_none_term(s: str) -> bool:\n",
    "    \"\"\"Treat '[none]' / 'none' / empty as unusable.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return True\n",
    "    t = s.strip().lower()\n",
    "    return t in (\"\", \"[none]\", \"none\")\n",
    "\n",
    "tree_to_mesh_terms = defaultdict(list)\n",
    "pair_counter = Counter()   # (mesh_term, primary_tree_number) → count\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Parse lists for each mechanism category\n",
    "    mesh_term_lists   = [parse_list(row.get(c, \"[]\")) for c in MESH_TERM_COLS]\n",
    "    tree_number_lists = [parse_list(row.get(c, \"[]\")) for c in TREE_COLS]\n",
    "\n",
    "    # Iterate over the three mechanism categories in parallel\n",
    "    for mesh_terms, tree_nums in zip(mesh_term_lists, tree_number_lists):\n",
    "        for mesh_term, primary_tn in zip(mesh_terms, tree_nums):\n",
    "            if is_none_term(mesh_term):\n",
    "                continue\n",
    "            if not isinstance(primary_tn, str) or not primary_tn.strip():\n",
    "                continue\n",
    "\n",
    "            tn = primary_tn.strip()\n",
    "            mt = mesh_term.strip()\n",
    "\n",
    "            pair_counter[(mt, tn)] += 1\n",
    "            tree_to_mesh_terms[tn].append(mt)\n",
    "\n",
    "# Convert to DataFrame\n",
    "out_rows = [\n",
    "    {\n",
    "        \"mesh_term\": mesh_term,\n",
    "        \"tree_number\": tree_num,\n",
    "        \"count\": count,\n",
    "    }\n",
    "    for (mesh_term, tree_num), count in pair_counter.items()\n",
    "]\n",
    "\n",
    "out_df = pd.DataFrame(out_rows).sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved tree number + term breakdown → {OUTPUT_PATH}\")\n",
    "print(\"Top 20 combinations:\")\n",
    "print(out_df.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Define super-group labels (9 buckets)\n",
    "# -------------------------------------------------------------------\n",
    "G1 = \"cytokine_hormone_receptor_modulators\"\n",
    "G2 = \"immune_checkpoint_immune_modulation\"\n",
    "G3 = \"targeted_pathway_inhibitors\"\n",
    "G4 = \"classical_cytotoxic_chemotherapy\"\n",
    "G5 = \"biologic_antibodies_biologics\"\n",
    "G6 = \"small_molecule_immunomod_antiinflammatory\"\n",
    "G7 = \"metabolic_pathway_modulators\"\n",
    "G8 = \"vaccines_immune_biologics\"\n",
    "G9 = \"supportive_adjunctive_agents\"\n",
    "\n",
    "\n",
    "def classify_super_group(mesh_term: str, tree_number: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic mapping of mesh_term + tree_number to one of 9 MOA super-groups.\n",
    "    Think like a clinical pharmacologist, but keep it deterministic and simple.\n",
    "    \"\"\"\n",
    "    t = (mesh_term or \"\").lower()\n",
    "    tn = (tree_number or \"\").strip()\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 8: Vaccines & immune biologics (non-mAb)\n",
    "    # -----------------------\n",
    "    if \"vaccine\" in t:\n",
    "        return G8\n",
    "    if \"recombinant fusion proteins\" in t:\n",
    "        return G8\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 5: Biologic antibodies (mono / bispecific / CAR / fusion)\n",
    "    # -----------------------\n",
    "    if (\n",
    "        \"antibod\" in t\n",
    "        or \"immunoconjugate\" in t\n",
    "        or \"chimeric antigen\" in t\n",
    "    ):\n",
    "        return G5\n",
    "    # Core monoclonal antibody/fusion protein MeSH branches\n",
    "    if tn.startswith(\"D12.776.124.486.485.114\"):  # Antibodies, Monoclonal*\n",
    "        return G5\n",
    "    if tn.startswith(\"D12.776.124.790.651.114\"):  # Therapeutic mAbs under Immunologic Factors\n",
    "        return G5\n",
    "    if tn.startswith(\"D12.776.828.300\"):  # Recombinant Fusion Proteins\n",
    "        return G5\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 1: Cytokine & hormone receptor modulators\n",
    "    # (EPO-R, TPO-R, IL-2R, glucocorticoid receptor, Ca-sensing receptor, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"receptors, erythropoietin\",\n",
    "        \"erythropoietin\",\n",
    "        \"receptors, thrombopoietin\",\n",
    "        \"thrombopoietin\",\n",
    "        \"receptors, interleukin-2\",\n",
    "        \"interleukin-2 receptor alpha subunit\",\n",
    "        \"receptors, glucocorticoid\",\n",
    "        \"receptors, calcium-sensing\",\n",
    "    ]):\n",
    "        return G1\n",
    "    # Hematopoietic / cytokine receptor MeSH branches seen in your table\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.150\"):  # EPO-R agonists\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.610\"):  # TPO-R agonists\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.420.320\"):  # IL-2R\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.826.750.430\"):  # Glucocorticoid receptor\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.695.115\"):  # Ca-sensing receptor\n",
    "        return G1\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 2: Immune checkpoint & immune modulation\n",
    "    # (Immune checkpoint inhibitors, PD-1/PD-L1, TNF, CD47, IL-33, IL-1RA, lectins, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"immune checkpoint inhibitors\",\n",
    "        \"immune checkpoint inhibitor\",\n",
    "        \"immune checkpoint\",  # generic catch-all\n",
    "        \"programmed cell death 1 receptor\", \"pd-1\", \"pd1\",\n",
    "        \"pd-l1\", \"pdl1\",\n",
    "        \"tumor necrosis factor-alpha\",\n",
    "        \"tnf\",\n",
    "        \"tumor necrosis factor ligand superfamily member\",\n",
    "        \"cd47 antigen\",\n",
    "        \"lectins, c-type\",\n",
    "        \"interleukin-33\",\n",
    "        \"interleukin 1 receptor antagonist protein\",\n",
    "    ]):\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.543.750.705.222.875\"):  # PD-1 receptor\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.500.800\"):  # TNF-alpha\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.395.550.014\"):  # CD47 antigen\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.503.280\"):  # C-type lectins\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.750.720\"):  # TNF ligand superfamily member 15\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.465.850\"):  # IL-33\n",
    "        return G2\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 3: Targeted pathway inhibitors (RTK / JAK-STAT / mTOR, VEGF, HER2)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"vascular endothelial growth factor a\",\n",
    "        \"receptor, erbb-2\",\n",
    "        \"erbb2\",\n",
    "        \"vegf\",\n",
    "        \"janus kinase inhibitors\",\n",
    "        \"jak inhibitor\",\n",
    "        \"mtor inhibitors\",\n",
    "        \"tor serine-threonine kinases\",\n",
    "    ]):\n",
    "        return G3\n",
    "    # VEGF-A branch\n",
    "    if tn.startswith(\"D12.644.276.100.800.200\"):\n",
    "        return G3\n",
    "    # HER2 / ErbB-2 receptor branch\n",
    "    if tn.startswith(\"D12.776.543.750.750.400.074.400\"):\n",
    "        return G3\n",
    "    # JAK / mTOR live under D27.505.519.* but we key by text above.\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 4: Classical cytotoxic chemotherapy (antimetabolite / alkylator / tubulin / topo)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"antimetabolites, antineoplastic\",\n",
    "        \"antineoplastic agents, alkylating\",\n",
    "        \"topoisomerase i inhibitors\",\n",
    "        \"topoisomerase ii inhibitors\",\n",
    "        \"vinca alkaloids\",\n",
    "        \"vinblastine\",\n",
    "        \"taxoids\",\n",
    "        \"paclitaxel\",\n",
    "        \"tubulin modulators\",\n",
    "    ]):\n",
    "        return G4\n",
    "    # Classical chemo branches\n",
    "    if tn.startswith(\"D27.505.519.186\"):  # antimetabolites, antineoplastic\n",
    "        return G4\n",
    "    if tn.startswith(\"D27.505.519.124\"):  # alkylating agents\n",
    "        return G4\n",
    "    if tn.startswith(\"D27.505.519.593.249.500\"):  # tubulin modulators\n",
    "        return G4\n",
    "    # You can add explicit topo/anthracycline branches here if you see them later.\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 6: Small-molecule immunomodulators & anti-inflammatories\n",
    "    # (PDE4 inhibitors, COX inhibitors, glucocorticoids as drugs, antimalarials)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"phosphodiesterase 4 inhibitors\",\n",
    "        \"cyclooxygenase inhibitors\",\n",
    "        \"histamine h1 antagonists\",\n",
    "        \"glucocorticoids* / metabolism; pharmacology\",\n",
    "        \"glucocorticoids\",  # as a drug class\n",
    "        \"antimalarials\",\n",
    "        \"immunosuppressive agents\",\n",
    "        \"calcineurin inhibitors\",\n",
    "    ]):\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.519.625.375.425.400\"):  # H1 antagonists\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.696.663.850.014.040.500.500\"):  # COX inhibitors\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.519.389.735.374\"):  # PDE4 inhibitors\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.696.477.656\"):  # Immunosuppressive Agents*\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.954.122.250.100.085\"):  # Antimalarials\n",
    "        return G6\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 7: Metabolic pathway modulators\n",
    "    # (gluconeogenesis, metabolic enzymes, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"gluconeogenesis / drug effects\",\n",
    "        \"gluconeogenesis\",\n",
    "        \"biguanides\",\n",
    "        \"imp dehydrogenase\",\n",
    "        \"thymidylate synthase\",\n",
    "        \"thymidine phosphorylase\",\n",
    "    ]):\n",
    "        return G7\n",
    "    if tn.startswith(\"G02.111.158.500\"):  # Gluconeogenesis / drug effects*\n",
    "        return G7\n",
    "    if tn.startswith(\"D08.811.682.047.820.450\"):  # IMP dehydrogenase\n",
    "        return G7\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 9: Supportive / adjunctive agents\n",
    "    # (anion exchange resins, leucovorin, antithrombins, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"anion exchange resins\",\n",
    "        \"antithrombins\",\n",
    "        \"leucovorin\",\n",
    "    ]):\n",
    "        return G9\n",
    "    if tn.startswith(\"D27.720.470.420.050\"):  # Anion exchange resins\n",
    "        return G9\n",
    "    if tn.startswith(\"D27.505.519.389.745.800.449\"):  # Antithrombins / agonists\n",
    "        return G9\n",
    "    if \"leucovorin\" in t:\n",
    "        return G9\n",
    "\n",
    "    # -----------------------\n",
    "    # Fallbacks:\n",
    "    # - If D27.505.* and not otherwise classified → treat as metabolic/chemical other\n",
    "    # -----------------------\n",
    "    if tn.startswith(\"D27.505.\"):\n",
    "        return G7  # generic chemical/metabolic \"other\" rather than immuno\n",
    "\n",
    "    # Absolute default: call it supportive/other\n",
    "    return G9\n",
    "\n",
    "\n",
    "# Apply classifier\n",
    "out_df = df[[\"mesh_term\", \"tree_number\"]].copy()\n",
    "out_df[\"mechanism_super_group\"] = [\n",
    "    classify_super_group(m, tn) for m, tn in zip(out_df[\"mesh_term\"], out_df[\"tree_number\"])\n",
    "]\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved mechanism super-group mapping → {OUTPUT_PATH}\")\n",
    "print(out_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "MAP_PATH      = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "TRIALS_IN     = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "TRIALS_OUT    = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load mapping: tree_number → super_group\n",
    "# ---------------------------------------------------\n",
    "map_df = pd.read_csv(MAP_PATH)\n",
    "\n",
    "# Normalize tree_number a bit (strip whitespace)\n",
    "map_df[\"tree_number\"] = map_df[\"tree_number\"].astype(str).str.strip()\n",
    "\n",
    "# If there are duplicates, we just keep the first (should all agree anyway)\n",
    "mapping = {}\n",
    "for _, row in map_df.iterrows():\n",
    "    tn = str(row[\"tree_number\"]).strip()\n",
    "    sg = row[\"mechanism_super_group\"]\n",
    "    if tn and tn not in mapping:\n",
    "        mapping[tn] = sg\n",
    "\n",
    "print(f\"Loaded {len(mapping):,} tree_number → super_group mappings\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings (e.g. \"['a','b']\") into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        return []\n",
    "    # If it's a single scalar, wrap in list\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def build_super_group_list(mech_terms, mech_trees):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      mech_terms : list of MeSH headings (unused except for length)\n",
    "      mech_trees : list of primary tree numbers (strings)\n",
    "    Return:\n",
    "      list of mechanism_super_group strings (same length).\n",
    "    \"\"\"\n",
    "    # Ensure lists\n",
    "    mech_trees = mech_trees or []\n",
    "    mech_terms = mech_terms or []\n",
    "\n",
    "    n = max(len(mech_terms), len(mech_trees))\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tn = (mech_trees[i] if i < len(mech_trees) else \"\") or \"\"\n",
    "        tn = tn.strip()\n",
    "        if not tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        out.append(mapping.get(tn, \"\"))  # \"\" if not found\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_super_group_list_with_fallback(\n",
    "    mech_terms,\n",
    "    mech_trees,\n",
    "    fallback_terms,\n",
    "    fallback_trees,\n",
    "):\n",
    "    \"\"\"\n",
    "    For investigational products:\n",
    "    - First try mechanism-based MeSH mapping (mech_terms/mech_trees).\n",
    "    - If the mechanism term is missing / '[none]' / empty, fall back to\n",
    "      the investigational product MeSH mapping (fallback_terms/fallback_trees).\n",
    "\n",
    "    Mapping itself is done ONLY on tree_number.\n",
    "    \"\"\"\n",
    "    # Ensure all are lists\n",
    "    mech_terms     = mech_terms or []\n",
    "    mech_trees     = mech_trees or []\n",
    "    fallback_terms = fallback_terms or []\n",
    "    fallback_trees = fallback_trees or []\n",
    "\n",
    "    n = max(len(mech_terms), len(mech_trees), len(fallback_terms), len(fallback_trees))\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Primary (mechanism-based)\n",
    "        term_mech = (mech_terms[i] if i < len(mech_terms) else \"\") or \"\"\n",
    "        tn_mech   = (mech_trees[i] if i < len(mech_trees) else \"\") or \"\"\n",
    "\n",
    "        term_mech = term_mech.strip()\n",
    "        tn_mech   = tn_mech.strip()\n",
    "\n",
    "        # Fallback (drug-based)\n",
    "        term_fb = (fallback_terms[i] if i < len(fallback_terms) else \"\") or \"\"\n",
    "        tn_fb   = (fallback_trees[i] if i < len(fallback_trees) else \"\") or \"\"\n",
    "\n",
    "        term_fb = term_fb.strip()\n",
    "        tn_fb   = tn_fb.strip()\n",
    "\n",
    "        # Decide which tree number to use\n",
    "        chosen_tn = \"\"\n",
    "        # 1) Use mechanism term if present and not [none]\n",
    "        if term_mech and term_mech != \"[none]\" and tn_mech:\n",
    "            chosen_tn = tn_mech\n",
    "        # 2) Else fall back to investigational product MeSH term\n",
    "        elif term_fb and term_fb != \"[none]\" and tn_fb:\n",
    "            chosen_tn = tn_fb\n",
    "\n",
    "        chosen_tn = chosen_tn.strip()\n",
    "        if not chosen_tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "\n",
    "        out.append(mapping.get(chosen_tn, \"\"))  # \"\" if no mapping\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Load trials and build super-group columns\n",
    "# ---------------------------------------------------\n",
    "df = pd.read_csv(TRIALS_IN)\n",
    "\n",
    "inv_sg_list = []\n",
    "ac_sg_list  = []\n",
    "soc_sg_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # -----------------------------\n",
    "    # INVESTIGATIONAL PRODUCTS\n",
    "    # -----------------------------\n",
    "    # Mechanism-based mapping\n",
    "    inv_mech_terms = parse_list(row.get(\"investigational_products_mechanism_mesh_terms\"))\n",
    "    inv_mech_tn    = parse_list(row.get(\"investigational_products_mechanism_primary_tree_numbers\"))\n",
    "\n",
    "    # Fallback: investigational product MeSH mapping\n",
    "    inv_prod_terms = parse_list(row.get(\"investigational_products_mapped\"))\n",
    "    inv_prod_tn    = parse_list(row.get(\"investigational_products_primary_tree_numbers\"))\n",
    "\n",
    "    inv_sg_list.append(\n",
    "        build_super_group_list_with_fallback(\n",
    "            inv_mech_terms,\n",
    "            inv_mech_tn,\n",
    "            inv_prod_terms,\n",
    "            inv_prod_tn,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # ACTIVE COMPARATORS\n",
    "    # (no fallback requested)\n",
    "    # -----------------------------\n",
    "    ac_terms = parse_list(row.get(\"active_comparators_mechanism_mesh_terms\"))\n",
    "    ac_tn    = parse_list(row.get(\"active_comparators_mechanism_primary_tree_numbers\"))\n",
    "    ac_sg_list.append(build_super_group_list(ac_terms, ac_tn))\n",
    "\n",
    "    # -----------------------------\n",
    "    # STANDARD OF CARE\n",
    "    # (no fallback requested)\n",
    "    # -----------------------------\n",
    "    soc_terms = parse_list(row.get(\"standard_of_care_mechanism_mesh_terms\"))\n",
    "    soc_tn    = parse_list(row.get(\"standard_of_care_mechanism_primary_tree_numbers\"))\n",
    "    soc_sg_list.append(build_super_group_list(soc_terms, soc_tn))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Insert new columns next to the primary_tree_numbers\n",
    "# ---------------------------------------------------\n",
    "def insert_after(df, col, newcol, values):\n",
    "    cols = list(df.columns)\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"investigational_products_mechanism_super_group\",\n",
    "    inv_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_super_group\",\n",
    "    ac_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_super_group\",\n",
    "    soc_sg_list,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Save final CSV\n",
    "# ---------------------------------------------------\n",
    "df.to_csv(TRIALS_OUT, index=False)\n",
    "print(f\"Wrote final trial-level file with super-groups → {TRIALS_OUT}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d17c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_super_group_distribution.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "def parse_list(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def first_non_empty_str(lst):\n",
    "    \"\"\"Return the first non-empty string from a list, or '' if none.\"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return \"\"\n",
    "    for v in lst:\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "chosen_super_groups = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inv_sg_list = parse_list(row.get(\"investigational_products_mechanism_super_group\", \"[]\"))\n",
    "    ac_sg_list  = parse_list(row.get(\"active_comparators_mechanism_super_group\", \"[]\"))\n",
    "    soc_sg_list = parse_list(row.get(\"standard_of_care_mechanism_super_group\", \"[]\"))\n",
    "\n",
    "    # Priority 1 — investigational product supergroup\n",
    "    chosen = first_non_empty_str(inv_sg_list)\n",
    "\n",
    "    # Priority 2 — active comparator supergroup\n",
    "    if not chosen:\n",
    "        chosen = first_non_empty_str(ac_sg_list)\n",
    "\n",
    "    # Priority 3 — fallback to SOC\n",
    "    if not chosen:\n",
    "        chosen = first_non_empty_str(soc_sg_list)\n",
    "\n",
    "    chosen_super_groups.append(chosen)\n",
    "\n",
    "# Add per-trial chosen super-group (optional but useful)\n",
    "df[\"trial_mechanism_super_group\"] = chosen_super_groups\n",
    "\n",
    "# Count distribution (exclude empty)\n",
    "dist = Counter(sg for sg in chosen_super_groups if sg)\n",
    "\n",
    "dist_df = (\n",
    "    pd.DataFrame(\n",
    "        [{\"mechanism_super_group\": sg, \"count\": count}\n",
    "         for sg, count in dist.items()]\n",
    "    )\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save distribution\n",
    "dist_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved distribution → {OUTPUT_PATH}\")\n",
    "print(\"Top categories:\\n\")\n",
    "print(dist_df.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deae989",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e05ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache\")\n",
    "OUT_DIR = Path(\"output\")\n",
    "\n",
    "CLASS_PATH   = BASE_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "MECH_PATH    = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "TRIALS_PATH  = BASE_DIR / \"raw_trials_with_hash.csv\"\n",
    "OUTPUT_PATH  = OUT_DIR / \"trial_results_table.csv\"\n",
    "\n",
    "# ---------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def join_plus(lst):\n",
    "    \"\"\"Join non-empty strings with '+'.\"\"\"\n",
    "    cleaned = [str(x).strip() for x in lst if str(x).strip()]\n",
    "    return \"+\".join(cleaned)\n",
    "\n",
    "def join_unique_plus(lst):\n",
    "    \"\"\"Join unique, non-empty strings with '+' (order-preserving).\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        s = str(x).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return \"+\".join(out)\n",
    "\n",
    "def normalize_innovation(val: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize to canonical title-case:\n",
    "      - Innovative\n",
    "      - Generic\n",
    "      - Biosimilar\n",
    "    Anything else is title-cased as a fallback.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    v = str(val).strip()\n",
    "    if not v:\n",
    "        return \"\"\n",
    "    low = v.lower()\n",
    "    if \"innov\" in low:\n",
    "        return \"Innovative\"\n",
    "    if \"bio\" in low:\n",
    "        return \"Biosimilar\"\n",
    "    if \"gener\" in low:\n",
    "        return \"Generic\"\n",
    "    # Fallback: just title-case whatever it is.\n",
    "    return v.title()\n",
    "\n",
    "# Remove all parenthetical segments from drug_name\n",
    "def strip_parentheses(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return re.sub(r\"\\s*\\([^)]*\\)\", \"\", s).strip()\n",
    "\n",
    "# NEW — clean MOA:\n",
    "#   - split by '+'\n",
    "#   - for each piece, drop everything after '/'\n",
    "#   - remove '*'\n",
    "#   - strip\n",
    "#   - rejoin by '+'\n",
    "def clean_moa(moa: str) -> str:\n",
    "    if not isinstance(moa, str):\n",
    "        return \"\"\n",
    "    parts = moa.split(\"+\")\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        s = part.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # keep only text before first '/'\n",
    "        if \"/\" in s:\n",
    "            s = s.split(\"/\", 1)[0]\n",
    "        # remove '*' and strip again\n",
    "        s = s.replace(\"*\", \"\").strip()\n",
    "        if s:\n",
    "            cleaned_parts.append(s)\n",
    "    return \"+\".join(cleaned_parts)\n",
    "\n",
    "# ---------------------------------\n",
    "# Load inputs\n",
    "# ---------------------------------\n",
    "df_class  = pd.read_csv(CLASS_PATH)\n",
    "df_mech   = pd.read_csv(MECH_PATH)\n",
    "df_titles = pd.read_csv(TRIALS_PATH, dtype=str)[[\"trial_hash\", \"title\"]]\n",
    "\n",
    "# ---------------------------------\n",
    "# Prepare classification info\n",
    "# ---------------------------------\n",
    "df_class[\"drug_name_list\"] = df_class[\"investigational_products\"].apply(parse_listish)\n",
    "df_class[\"innovation_list\"] = df_class[\"investigational_products_classifications\"].apply(parse_listish)\n",
    "\n",
    "df_class[\"drug_name_joined\"] = df_class[\"drug_name_list\"].apply(join_plus)\n",
    "df_class[\"innovation_joined\"] = df_class[\"innovation_list\"].apply(\n",
    "    lambda lst: join_plus([normalize_innovation(v) for v in lst])\n",
    ")\n",
    "\n",
    "df_class_slim = df_class[[\"trial_hash\", \"drug_name_joined\", \"innovation_joined\"]].copy()\n",
    "\n",
    "# ---------------------------------\n",
    "# Prepare mechanism / category info\n",
    "# ---------------------------------\n",
    "df_mech[\"inv_drug_name_joined\"] = df_mech[\"investigational_products\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"inv_moa_joined\"] = df_mech[\"investigational_products_mechanism_mesh_terms\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"inv_category_joined\"] = df_mech[\"investigational_products_mechanism_super_group\"].apply(\n",
    "    lambda x: join_unique_plus(parse_listish(x))\n",
    ")\n",
    "\n",
    "df_mech[\"soc_drug_name_joined\"] = df_mech[\"standard_of_care\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"soc_moa_joined\"] = df_mech[\"standard_of_care_mechanism_mesh_terms\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"soc_category_joined\"] = df_mech[\"standard_of_care_mechanism_super_group\"].apply(\n",
    "    lambda x: join_unique_plus(parse_listish(x))\n",
    ")\n",
    "\n",
    "df_mech_slim = df_mech[\n",
    "    [\n",
    "        \"trial_hash\",\n",
    "        \"inv_drug_name_joined\",\n",
    "        \"inv_moa_joined\",\n",
    "        \"inv_category_joined\",\n",
    "        \"soc_drug_name_joined\",\n",
    "        \"soc_moa_joined\",\n",
    "        \"soc_category_joined\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# ---------------------------------\n",
    "# Merge on trial_hash\n",
    "# ---------------------------------\n",
    "merged = pd.merge(\n",
    "    df_mech_slim,\n",
    "    df_class_slim,\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# ---------------------------------\n",
    "# Build final table\n",
    "# ---------------------------------\n",
    "def build_final_row(row):\n",
    "    inv_drug = (row.get(\"inv_drug_name_joined\") or \"\").strip()\n",
    "    soc_drug = (row.get(\"soc_drug_name_joined\") or \"\").strip()\n",
    "\n",
    "    if inv_drug:\n",
    "        drug_name  = inv_drug\n",
    "        moa        = (row.get(\"inv_moa_joined\") or \"\").strip()\n",
    "        innovation = (row.get(\"innovation_joined\") or \"\").strip()\n",
    "        category   = (row.get(\"inv_category_joined\") or \"\").strip()\n",
    "        if not category:\n",
    "            category = (row.get(\"soc_category_joined\") or \"\").strip()\n",
    "\n",
    "    elif soc_drug:\n",
    "        drug_name  = soc_drug\n",
    "        moa        = (row.get(\"soc_moa_joined\") or \"\").strip()\n",
    "        category   = (row.get(\"soc_category_joined\") or \"\").strip()\n",
    "        # Standard-of-care only: treat as Generic (canonical title-case)\n",
    "        innovation = \"Generic\"\n",
    "\n",
    "    else:\n",
    "        drug_name = \"\"\n",
    "        moa = \"\"\n",
    "        innovation = \"\"\n",
    "        category = \"\"\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"drug_name\": drug_name,\n",
    "            \"moa\": moa,\n",
    "            \"innovation_generic_biosimilar\": innovation,\n",
    "            \"category\": category,\n",
    "        }\n",
    "    )\n",
    "\n",
    "final_cols = merged.apply(build_final_row, axis=1)\n",
    "final = pd.concat([merged[[\"trial_hash\"]], final_cols], axis=1)\n",
    "\n",
    "# ---------------------------------\n",
    "# Attach titles\n",
    "# ---------------------------------\n",
    "final = final.merge(df_titles, on=\"trial_hash\", how=\"left\")\n",
    "final[\"trial_title\"] = final[\"title\"].fillna(final[\"trial_hash\"])\n",
    "final.drop(columns=[\"title\"], inplace=True)\n",
    "\n",
    "# ---------------------------------\n",
    "# REMOVE PARENTHETICAL TEXT FROM drug_name\n",
    "# ---------------------------------\n",
    "final[\"drug_name\"] = final[\"drug_name\"].apply(strip_parentheses)\n",
    "\n",
    "# ---------------------------------\n",
    "# CLEAN MOA FIELD\n",
    "# ---------------------------------\n",
    "final[\"moa\"] = final[\"moa\"].apply(clean_moa)\n",
    "\n",
    "# Ensure innovation is always canonical (in case anything slipped through)\n",
    "final[\"innovation_generic_biosimilar\"] = final[\"innovation_generic_biosimilar\"].apply(\n",
    "    lambda v: \"+\".join(\n",
    "        normalize_innovation(part) for part in str(v).split(\"+\") if str(part).strip()\n",
    "    ) if pd.notna(v) and str(v).strip() else \"\"\n",
    ")\n",
    "\n",
    "results = final[[\"trial_title\", \"drug_name\", \"moa\", \"innovation_generic_biosimilar\", \"category\"]].copy()\n",
    "\n",
    "# Save\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved results table → {OUTPUT_PATH}\")\n",
    "print(results.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Find rows with missing values\n",
    "# ---------------------------------\n",
    "\n",
    "# Treat \"\" as missing for easier filtering\n",
    "cols_to_check = [\"trial_title\", \"drug_name\", \"moa\", \"innovation_generic_biosimilar\", \"category\"]\n",
    "\n",
    "def is_missing(x):\n",
    "    return (pd.isna(x)) or (str(x).strip() == \"\")\n",
    "\n",
    "# Modern replacement for applymap\n",
    "mask_missing = results[cols_to_check].map(is_missing).any(axis=1)\n",
    "\n",
    "missing_rows = results[mask_missing].copy()\n",
    "\n",
    "print(f\"Found {len(missing_rows)} rows with at least one missing value.\")\n",
    "\n",
    "# Save for debugging\n",
    "MISSING_OUTPUT_PATH = BASE_DIR / \"trial_results_table_missing_rows.csv\"\n",
    "missing_rows.to_csv(MISSING_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"❗ Missing rows saved to → {MISSING_OUTPUT_PATH}\")\n",
    "print(missing_rows.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036c24e",
   "metadata": {},
   "source": [
    "discovered for two drugs \"601\" and \"Inetetamab\" citline mapped them to the wrong drug / drug_id\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "That is inotuzumab ozogamicin (Besponsa):\n",
    "- Target: CD22\n",
    "- Indication: B-cell ALL, etc.\n",
    "- Mechanism: antibody–drug conjugate / DNA damaging.\n",
    "\n",
    "This has nothing to do with:\n",
    "- HER2\n",
    "- breast cancer neoadjuvant\n",
    "- Inetetamab / Inituzumab / Ceputin\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "AER-601 (Aerami / Dance Biopharm GLP-1 analogue)\n",
    "- Target: GLP-1 receptor\n",
    "- Indications: Type 2 diabetes, obesity, appetite/weight control\n",
    "- Mechanism: GLP-1 receptor agonist, incretin mimetic, insulin secretagogue\n",
    "\n",
    "This has nothing to do with:\n",
    "- VEGF-A or VEGF receptors\n",
    "- Intravitreal ophthalmic anti-VEGF biologics\n",
    "- Pathological myopic choroidal neovascularization (pmCNV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
