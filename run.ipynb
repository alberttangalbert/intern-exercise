{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff4ae34",
   "metadata": {},
   "source": [
    "#### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97784b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load raw data from provided CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv(\"data/raw_trials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e73a1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b69802af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                 |   0 |\n",
      "|:--------------------------------|----:|\n",
      "| title                           |   0 |\n",
      "| objective                       |   3 |\n",
      "| outcome_details                 | 146 |\n",
      "| phase                           |   0 |\n",
      "| primary_completion_date         |  61 |\n",
      "| primary_endpoints_reported_date | 161 |\n",
      "| prior_concurrent_therapy        | 184 |\n",
      "| start_date                      |  45 |\n",
      "| study_design                    |  16 |\n",
      "| treatment_plan                  |   1 |\n",
      "| record_type                     |   0 |\n",
      "| patients_per_site_per_month     | 119 |\n",
      "| primary_endpoint_json           |   0 |\n",
      "| other_endpoint_json             |   0 |\n",
      "| associated_cro_json             |   0 |\n",
      "| notes_json                      |   0 |\n",
      "| outcomes_json                   |   0 |\n",
      "| patient_dispositions_json       |   0 |\n",
      "| results_json                    |   0 |\n",
      "| study_keywords_json             |   0 |\n",
      "| tags_json                       |   0 |\n",
      "| primary_drugs_tested_json       |   0 |\n",
      "| other_drugs_tested_json         |   0 |\n",
      "| therapeutic_areas_json          |   0 |\n",
      "| bmt_other_drugs_tested_json     |   0 |\n",
      "| bmt_primary_drugs_tested_json   |   0 |\n",
      "| ct_gov_listed_locations_json    |   0 |\n",
      "| ct_gov_mesh_terms_json          |   0 |\n",
      "Shape: (184, 28)\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum().to_markdown())\n",
    "print(\"Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6897de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw trials from: data/raw_trials.csv\n",
      "Generating trial_hash values ...\n",
      "Data columns: Index(['trial_hash', 'title', 'objective', 'outcome_details', 'phase',\n",
      "       'primary_completion_date', 'primary_endpoints_reported_date',\n",
      "       'prior_concurrent_therapy', 'start_date', 'study_design',\n",
      "       'treatment_plan', 'record_type', 'patients_per_site_per_month',\n",
      "       'primary_endpoint_json', 'other_endpoint_json', 'associated_cro_json',\n",
      "       'notes_json', 'outcomes_json', 'patient_dispositions_json',\n",
      "       'results_json', 'study_keywords_json', 'tags_json',\n",
      "       'primary_drugs_tested_json', 'other_drugs_tested_json',\n",
      "       'therapeutic_areas_json', 'bmt_other_drugs_tested_json',\n",
      "       'bmt_primary_drugs_tested_json', 'ct_gov_listed_locations_json',\n",
      "       'ct_gov_mesh_terms_json'],\n",
      "      dtype='object')\n",
      "Data shape: (184, 29)\n",
      "Saved to cache/data_preprocess/raw_trials_with_hash.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate a unique, deterministic trial hash for each clinical trial and save an\n",
    "augmented CSV.\n",
    "\n",
    "Inputs:\n",
    "- CSV file: data/raw_trials.csv\n",
    "    Must contain at least:\n",
    "    • \"title\"\n",
    "    • \"start_date\"\n",
    "    • \"phase\"\n",
    "\n",
    "Process:\n",
    "- Load the raw trials into a DataFrame.\n",
    "- For each row, build a small JSON payload from (title, start_date, phase).\n",
    "- Compute an MD5 hash of the payload and prefix with \"tid_\" to form\n",
    "  a deterministic trial identifier.\n",
    "- Insert \"trial_hash\" as the first column.\n",
    "\n",
    "Outputs:\n",
    "- CSV written to:\n",
    "      cache/data_preprocess/raw_trials_with_hash.csv\n",
    "  containing all original columns plus the leading \"trial_hash\" column.\n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "INPUT_PATH = Path(\"data/raw_trials.csv\")\n",
    "OUTPUT_PATH = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "print(f\"Loading raw trials from: {INPUT_PATH}\")\n",
    "data = pd.read_csv(INPUT_PATH, dtype=str).fillna(\"\")\n",
    "\n",
    "print(\"Generating trial_hash values ...\")\n",
    "\n",
    "def make_trial_hash(row):\n",
    "    \"\"\"Deterministic hash for a trial based on stable fields.\"\"\"\n",
    "    payload = {\n",
    "        \"title\": row.get(\"title\", \"\"),\n",
    "        \"start_date\": row.get(\"start_date\", \"\"),\n",
    "        \"phase\": row.get(\"phase\", \"\"),\n",
    "    }\n",
    "    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False)\n",
    "    return \"tid_\" + hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Create trial_hash column\n",
    "data[\"trial_hash\"] = data.apply(make_trial_hash, axis=1)\n",
    "\n",
    "# Move trial_hash to first column\n",
    "cols = [\"trial_hash\"] + [c for c in data.columns if c != \"trial_hash\"]\n",
    "data = data[cols]\n",
    "\n",
    "print(\"Data columns:\", data.columns)\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Export\n",
    "data.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14351eb6",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efb4f1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 184 trials from cache/data_preprocess/raw_trials_with_hash.csv\n",
      "Complete. processed=1, skipped=183, llm_error=0, parse_error=0\n",
      "Roles directory: cache/task_1/trial_drug_roles\n",
      "Log directory:   cache/task_1/trial_drug_roles_log\n",
      "Master roles:    cache/task_1/trial_drug_roles_master.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use a chatbot to extract structured drug-role metadata for each clinical trial.\n",
    "\n",
    "Inputs:\n",
    "- `cache/data_preprocess/raw_trials_with_hash.csv`\n",
    "    One row per trial, including:\n",
    "    • trial_hash (unique ID)\n",
    "    • title, objective, treatment_plan\n",
    "    • *_drugs_tested_json fields\n",
    "    • other structured or semi-structured metadata used to identify interventions.\n",
    "\n",
    "Process:\n",
    "- For each trial, build an LLM prompt using selected columns.\n",
    "- Ask the model to identify all distinct interventions and classify them.\n",
    "- For each drug:\n",
    "    • Assign role (Investigational Product, Active Comparator, Placebo, SOC)\n",
    "    • List alternative names / synonyms\n",
    "    • Identify molecular target and mechanism (if known)\n",
    "    • Assign tt_drug_id and bmt_drug_id only when matchable with high confidence.\n",
    "- Runs in parallel using ThreadPoolExecutor.\n",
    "- Skips writing output for trials that already have saved results.\n",
    "- Tracks processed, skipped, LLM errors, and JSON-parse errors.\n",
    "\n",
    "Outputs:\n",
    "- Per-trial mapped interventions:\n",
    "      `cache/task_1/trial_drug_roles/{trial_hash}.json`\n",
    "- Per-trial log files (prompt + raw response + cost):\n",
    "      `cache/task_1/trial_drug_roles_log/{trial_hash}.json`\n",
    "- Aggregated master index of all mappings:\n",
    "      `cache/task_1/trial_drug_roles_master.json`\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "BASE_DIR = Path(\"cache\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"task_1\" / \"trial_drug_roles\"\n",
    "DRUG_ROLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DRUG_ROLE_LOG_DIR = BASE_DIR / \"task_1\" / \"trial_drug_roles_log\"\n",
    "DRUG_ROLE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_ROLES_PATH = BASE_DIR / \"task_1\" / \"trial_drug_roles_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def build_prompt(trial_payload: dict) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM to:\n",
    "    - Extract drug names\n",
    "    - Canonicalize names by removing company/manufacturer/location qualifiers\n",
    "    - Deduplicate synonymous names\n",
    "    - For each canonical drug, return a dict with:\n",
    "        * role (Investigational Product / Placebo / Active Comparator / Standard of Care)\n",
    "        * alternative_names (list)\n",
    "        * molecular_target\n",
    "        * mechanism\n",
    "        * tt_drug_id (TrialTrove/PharmaProjects drugId as string)\n",
    "        * bmt_drug_id (BioMedTracker bmtDrugId as string)\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and interpretation expert.\n",
    "\n",
    "You are given structured information about a clinical trial, including:\n",
    "- Title and objective\n",
    "- Study design and treatment plan\n",
    "- JSON fields listing drugs tested in the study:\n",
    "  - primary_drugs_tested_json\n",
    "  - other_drugs_tested_json\n",
    "  - bmt_other_drugs_tested_json\n",
    "  - bmt_primary_drugs_tested_json\n",
    "- These JSON fields may also contain metadata such as\n",
    "  drugApprovalStatus (Approved / Unapproved), mechanisms, synonyms, etc.\n",
    "- In the TrialTrove/PharmaProjects JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"drugId\".\n",
    "- In the BioMedTracker JSON blocks, the unique drug identifier\n",
    "  is usually under a key like \"bmtDrugId\".\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Identify all DISTINCT physical drug entities explicitly used in the study.\n",
    "   - Strings in the *_drugs_tested_json fields are drug-name candidates.\n",
    "   - If these fields contain structured JSON, infer names from keys such as\n",
    "     \"name\", \"drug_name\", \"drugName\", \"drugPrimaryName\", \"preferred_name\",\n",
    "     \"label\", etc.\n",
    "\n",
    "2. Canonicalize each drug name:\n",
    "   Remove company names, manufacturer qualifiers, geographic qualifiers,\n",
    "   dosage-form qualifiers, or parenthetical descriptors that do NOT change\n",
    "   the name of the underlying drug.\n",
    "\n",
    "   Examples:\n",
    "   - \"AlphaBlocker (CompanyX)\" → \"AlphaBlocker\"\n",
    "   - \"Recombinant Growth Factor (rgf)\" → \"Recombinant Growth Factor\"\n",
    "   - \"DrugX citrate (RegionY)\" → \"DrugX citrate\"\n",
    "   - \"BrandName (compound-42, MakerCorp)\" → \"BrandName\"\n",
    "\n",
    "3. Deduplicate synonymous names referring to the SAME drug.\n",
    "   - Prefer the simplest, most standard canonical name.\n",
    "   - Collect all other variations under alternative_names.\n",
    "\n",
    "4. For EACH distinct drug, build an object with SIX fields:\n",
    "\n",
    "   • \"role\": one of:\n",
    "       - \"Investigational Product\"\n",
    "       - \"Placebo\"\n",
    "       - \"Active Comparator\"\n",
    "       - \"Standard of Care\"\n",
    "\n",
    "     ROLE ASSIGNMENT RULES (SUMMARY):\n",
    "\n",
    "     - \"Investigational Product\":\n",
    "       * Sponsor's novel or proprietary product, or a regimen whose key component\n",
    "         is a novel/proprietary or clearly unapproved pipeline agent.\n",
    "       * The trial objective explicitly focuses on evaluating this new product\n",
    "         for safety/efficacy, dose-finding, first-in-human, or proof-of-concept.\n",
    "\n",
    "     - \"Standard of Care\":\n",
    "       * Approved, widely used therapies or regimens that represent background,\n",
    "         conventional, or standard treatment.\n",
    "       * IMPORTANT: If ALL drugs in a regimen are already approved therapies\n",
    "         and the trial is mainly about treatment strategy, regimen choice,\n",
    "         imaging-guided regimen selection, dosing/scheduling, or algorithm\n",
    "         optimization (rather than developing a NEW drug entity), then:\n",
    "             → Classify ALL actively dosed drugs as \"Standard of Care\".\n",
    "             → Do NOT create any \"Investigational Product\" entry for that trial.\n",
    "       * This includes combinations of standard agents (e.g., docetaxel,\n",
    "         cisplatin, cyclophosphamide, ifosfamide, paclitaxel, fluorouracil,\n",
    "         and similar approved drugs) when no new molecular entity is being tested.\n",
    "\n",
    "     - \"Active Comparator\":\n",
    "       * A non-placebo comparator arm explicitly contrasted with another\n",
    "         investigational product in the protocol (e.g., \"experimental vs\n",
    "         active control\").\n",
    "\n",
    "     - \"Placebo\":\n",
    "       * Inert control preparations.\n",
    "\n",
    "   • \"alternative_names\": list of synonymous variants.\n",
    "   • \"molecular_target\": e.g., \"CD20\", \"VEGF-A\". If unknown or not disclosed, use \"\".\n",
    "   • \"mechanism\": e.g., \"EGFR inhibitor\", \"Anti-PD-1 antibody\", \"JAK inhibitor\".\n",
    "       - If the ONLY information is that it is a \"small molecule\", \"biologic\",\n",
    "         \"small molecule; mechanism of action not identified\", \"mechanism unknown\",\n",
    "         \"not identified\", \"not determined\", \"not disclosed\", or similar,\n",
    "         then treat the mechanism as UNKNOWN and set \"mechanism\" = \"\".\n",
    "       - Do NOT copy meta-statements such as:\n",
    "           • \"Small molecule; mechanism of action not identified\"\n",
    "           • \"Mechanism of action unknown/not identified/not yet determined\"\n",
    "         into the mechanism field. Use \"\" instead in these cases.\n",
    "   • \"tt_drug_id\": STRING. If not confidently matchable, \"\".\n",
    "   • \"bmt_drug_id\": STRING. If not confidently matchable, \"\".\n",
    "\n",
    "5. ID MISMATCH SAFETY RULE:\n",
    "   - Do NOT assign tt_drug_id or bmt_drug_id if they clearly belong to a different drug\n",
    "     (different target, mechanism, indication, modality, or obviously mismatched name).\n",
    "   - If there is ANY doubt about the correctness of an ID:\n",
    "       → Set BOTH \"tt_drug_id\" and \"bmt_drug_id\" to \"\".\n",
    "\n",
    "Output format (IMPORTANT):\n",
    "Return ONLY a valid JSON object with:\n",
    "  - keys   = canonical drug names\n",
    "  - values = objects with EXACTLY:\n",
    "        * \"role\"\n",
    "        * \"alternative_names\"\n",
    "        * \"molecular_target\"\n",
    "        * \"mechanism\"\n",
    "        * \"tt_drug_id\"\n",
    "        * \"bmt_drug_id\"\n",
    "\n",
    "Example:\n",
    "{{\n",
    "  \"ABC-123\": {{\n",
    "    \"role\": \"Investigational Product\",\n",
    "    \"alternative_names\": [\"ABC123\", \"Compound-ABC\"],\n",
    "    \"molecular_target\": \"Receptor-Z\",\n",
    "    \"mechanism\": \"Bispecific antibody\",\n",
    "    \"tt_drug_id\": \"123456\",\n",
    "    \"bmt_drug_id\": \"78901\"\n",
    "  }},\n",
    "  \"DrugX\": {{\n",
    "    \"role\": \"Standard of Care\",\n",
    "    \"alternative_names\": [\"GenericX\", \"ChemX\"],\n",
    "    \"molecular_target\": \"Enzyme-A\",\n",
    "    \"mechanism\": \"Antimetabolite\",\n",
    "    \"tt_drug_id\": \"\",\n",
    "    \"bmt_drug_id\": \"\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Input JSON:\n",
    "{payload_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "counter = {\"processed\": 0, \"skipped_existing\": 0, \"llm_error\": 0, \"parse_error\": 0}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "master_roles: dict[str, dict] = {}\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_trial(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one trial: call LLM, validate output, save role JSON and log.\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"[{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = DRUG_ROLE_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_prompt(trial_payload)\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM error for {trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    drug_roles = extract_json_object(text_response)\n",
    "    if not isinstance(drug_roles, dict) or not drug_roles:\n",
    "        print(f\"JSON parse error trial_hash={trial_hash}\")\n",
    "        with counter_lock:\n",
    "            counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"title\": row.get(\"title\"),\n",
    "        \"drug_roles\": drug_roles,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    log_payload = {\n",
    "        \"token\": trial_hash,\n",
    "        \"hash_id\": trial_hash,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (DRUG_ROLE_LOG_DIR / f\"{trial_hash}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    with master_lock:\n",
    "        master_roles[trial_hash] = mapped\n",
    "        MASTER_ROLES_PATH.write_text(\n",
    "            json.dumps(master_roles, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        counter[\"processed\"] += 1\n",
    "        if counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Processed {counter['processed']} trials...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "rows = df_trials.to_dict(orient=\"records\")\n",
    "total_trials = len(rows)\n",
    "print(f\"Loaded {total_trials} trials from {TRIALS_WITH_HASH_CSV}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_trial, row, idx, total_trials): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Worker error: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Complete. processed={counter['processed']}, \"\n",
    "    f\"skipped={counter['skipped_existing']}, \"\n",
    "    f\"llm_error={counter['llm_error']}, \"\n",
    "    f\"parse_error={counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Roles directory: {DRUG_ROLE_DIR}\")\n",
    "print(f\"Log directory:   {DRUG_ROLE_LOG_DIR}\")\n",
    "print(f\"Master roles:    {MASTER_ROLES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce191b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LLM COST SUMMARY ==========\n",
      "Total LLM cost:             $4.3950\n",
      "Number of logged trials:     184\n",
      "Average cost per trial:      $0.0239\n",
      "\n",
      "Top 10 most expensive trials:\n",
      "  tid_1158b3369546dc4b16dc21c8c026b619.json: $0.0546\n",
      "  tid_28a767e788d4d9a4e65b3c10d10585c2.json: $0.0546\n",
      "  tid_763e3011bc90e46c88c7a2953a39ed2a.json: $0.0518\n",
      "  tid_6e821d7fbd8539bae7baf3a668d6d080.json: $0.0462\n",
      "  tid_d372a5464ccae4cf39f41537506a78c0.json: $0.0454\n",
      "  tid_ff64edc14f04fb1d81451cc7475488fe.json: $0.0448\n",
      "  tid_8b4d60a5fddc078962af34399d7e342c.json: $0.0436\n",
      "  tid_e0a77c4ecf93cf781f04cc467c974511.json: $0.0428\n",
      "  tid_7e80effdd579ba535ef686ac50dcc4bc.json: $0.0416\n",
      "  tid_837737698a5271d314ea8208addb2d72.json: $0.0410\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarize total LLM usage cost for previous cell by reading all per-trial log files.\n",
    "\n",
    "Inputs:\n",
    "- Directory: cache/task_1/trial_drug_roles_log/\n",
    "    Each log JSON contains:\n",
    "        • total_cost (float)\n",
    "        • other metadata (prompt, raw response, timing, etc.)\n",
    "\n",
    "Process:\n",
    "- Load each log file and extract its total_cost value.\n",
    "- Aggregate total cost, count entries, and compute average cost per trial.\n",
    "- Sort trials by cost to identify the most expensive prompts.\n",
    "\n",
    "Outputs:\n",
    "- Console summary including:\n",
    "    • Total cost\n",
    "    • Number of logged trials\n",
    "    • Average cost per trial\n",
    "    • Top 10 highest-cost trials (filename + cost)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/task_1/trial_drug_roles_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e313110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trial product breakdown to cache/task_1/trial_product_breakdown.csv\n",
      "|     | trial_hash                           | investigational_products                           | investigational_products_alternative_names                                                                                                                                                                                                          | investigational_products_molecular_target   | investigational_products_mechanism                                         | investigational_products_tt_drug_id   | investigational_products_bmt_drug_id   | active_comparators   | active_comparators_alternative_names   | active_comparators_molecular_target   | active_comparators_mechanism   | active_comparators_tt_drug_id   | active_comparators_bmt_drug_id   | placebos   | placebos_alternative_names   | placebos_molecular_target   | placebos_mechanism   | standard_of_care   | standard_of_care_alternative_names   | standard_of_care_molecular_target   | standard_of_care_mechanism   | standard_of_care_tt_drug_id   | standard_of_care_bmt_drug_id   |\n",
      "|----:|:-------------------------------------|:---------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:---------------------------------------------------------------------------|:--------------------------------------|:---------------------------------------|:---------------------|:---------------------------------------|:--------------------------------------|:-------------------------------|:--------------------------------|:---------------------------------|:-----------|:-----------------------------|:----------------------------|:---------------------|:-------------------|:-------------------------------------|:------------------------------------|:-----------------------------|:------------------------------|:-------------------------------|\n",
      "|  90 | tid_0541995757b10e613a42173d6b8ddc09 | ['cinacalcet hydrochloride']                       | [['cinacalcet HCl', 'cinacalcet', 'cinacalcet HCl, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride tablet']] | ['Calcium-sensing receptor (CaSR)']         | ['Calcimimetic; positive allosteric modulator of CaSR']                    | ['194454']                            | ['']                                   | []                   | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "|   1 | tid_0d6e9b2f3f57c17c0e93610e28853f0c | ['Xenopax']                                        | [['daclizumab', 'daclizumab, Shanghai CP Guojian', 'transplantation MAb, Shanghai', 'recombinant anti-CD25 humanized monoclonal antibody injection']]                                                                                               | ['CD25 (IL2RA)']                            | ['Humanized monoclonal antibody; interleukin-2 receptor alpha antagonist'] | ['40098']                             | ['']                                   | []                   | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "|  67 | tid_0da20e863cfc5f3e369868462bff74e0 | ['recombinant erythropoiesis-stimulating protein'] | [['NuPIAO', 'nupiao', 'rESP', 'recombinant erythropoietin stimulating protein', 'recombinant erythropoietin stimulating protein, 3SBio', 'recombinant erythropoiesis-stimulating protein injection (CHO cells)', 'SSS-06', 'SSS06', 'SSS 06']]      | ['Erythropoietin receptor (EPOR)']          | ['Erythropoietin receptor agonist']                                        | ['40640']                             | ['19694']                              | []                   | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| 165 | tid_0e698ee5065c49d23fcf57516957a273 | ['SB8']                                            | [['SB 8', 'SB-8', 'Aybintio', 'Onbevzi', 'bevacizumab, Samsung Bioepis', '615', 'SB8 Anti-VEGF antibody']]                                                                                                                                          | ['VEGF-A']                                  | ['Anti-VEGF-A monoclonal antibody (angiogenesis inhibitor)']               | ['113881']                            | ['27338']                              | []                   | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| 140 | tid_0e8fa21079f928135dfc6164a15285f8 | ['SSS-17']                                         | [['SSS17', 'SSS 17', '[14C] SSS17', '[14C]-SSS17', '[14C]SSS17', 'HIF 117', 'HIF-117', 'HIF117', '[14C]HIF-117']]                                                                                                                                   | ['HIF']                                     | ['Hypoxia-inducible factor antagonist']                                    | ['130313']                            | ['']                                   | []                   | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                   | []                                  | []                           | []                            | []                             |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate per-trial drug-role JSONs into a wide trial-level product breakdown CSV.\n",
    "\n",
    "Inputs:\n",
    "- Directory: cache/task_1/trial_drug_roles/\n",
    "    Each file: {trial_hash}.json with structure:\n",
    "        {\n",
    "          \"trial_hash\": \"<tid_...>\",\n",
    "          \"title\": \"...\",\n",
    "          \"drug_roles\": {\n",
    "            \"<drug_name>\": {\n",
    "              \"role\": \"Investigational Product\" | \"Active Comparator\" | \"Placebo\" | \"Standard of Care\",\n",
    "              \"alternative_names\": [...],\n",
    "              \"molecular_target\": \"...\",\n",
    "              \"mechanism\": \"...\",\n",
    "              \"tt_drug_id\": \"...\",\n",
    "              \"bmt_drug_id\": \"...\"\n",
    "            },\n",
    "            ...\n",
    "          }\n",
    "        }\n",
    "\n",
    "Process:\n",
    "- Iterate over all JSONs in cache/task_1/trial_drug_roles/.\n",
    "- For each trial:\n",
    "    • Partition drugs into four buckets: investigational, active comparator, placebo, standard of care.\n",
    "    • Collect, per role:\n",
    "        - canonical names\n",
    "        - alternative_names (as list-of-lists)\n",
    "        - molecular_target\n",
    "        - mechanism\n",
    "        - tt_drug_id / bmt_drug_id where applicable.\n",
    "- Build one row per trial with list-valued columns for each role.\n",
    "\n",
    "Outputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "    One row per trial, columns:\n",
    "        trial_hash\n",
    "        investigational_products, investigational_products_alternative_names, ...\n",
    "        active_comparators, ...\n",
    "        placebos, ...\n",
    "        standard_of_care, ...\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Base directory for task_1 cache + input/output\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "\n",
    "# Directory that contains per-trial drug-role JSONs\n",
    "DRUG_ROLE_DIR = BASE_DIR / \"trial_drug_roles\"\n",
    "\n",
    "# Output CSV path\n",
    "OUT_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "rows = []\n",
    "\n",
    "for fp in DRUG_ROLE_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    drug_roles = obj.get(\"drug_roles\") or {}\n",
    "    if not isinstance(drug_roles, dict):\n",
    "        print(f\"drug_roles not dict in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    # Containers\n",
    "    inv_names = []\n",
    "    inv_alt_names = []          # list of lists\n",
    "    inv_targets = []\n",
    "    inv_mechanisms = []\n",
    "    inv_tt_ids = []\n",
    "    inv_bmt_ids = []\n",
    "\n",
    "    ac_names = []\n",
    "    ac_alt_names = []           # list of lists\n",
    "    ac_targets = []\n",
    "    ac_mechanisms = []\n",
    "    ac_tt_ids = []\n",
    "    ac_bmt_ids = []\n",
    "\n",
    "    plc_names = []\n",
    "    plc_alt_names = []          # list of lists\n",
    "    plc_targets = []\n",
    "    plc_mechanisms = []\n",
    "\n",
    "    soc_names = []\n",
    "    soc_alt_names = []          # list of lists\n",
    "    soc_targets = []\n",
    "    soc_mechanisms = []\n",
    "    soc_tt_ids = []\n",
    "    soc_bmt_ids = []\n",
    "\n",
    "    for drug_name, meta in drug_roles.items():\n",
    "        if not isinstance(meta, dict):\n",
    "            continue\n",
    "\n",
    "        role = (meta.get(\"role\") or \"\").strip()\n",
    "        role_norm = role.lower()\n",
    "\n",
    "        alt_names = meta.get(\"alternative_names\") or []\n",
    "        if not isinstance(alt_names, list):\n",
    "            alt_names = [str(alt_names)]\n",
    "\n",
    "        molecular_target = meta.get(\"molecular_target\") or \"\"\n",
    "        mechanism = meta.get(\"mechanism\") or \"\"\n",
    "\n",
    "        # IDs are always stored as strings in the LLM output, but be defensive\n",
    "        tt_id = str(meta.get(\"tt_drug_id\") or \"\")\n",
    "        bmt_id = str(meta.get(\"bmt_drug_id\") or \"\")\n",
    "\n",
    "        if role_norm == \"investigational product\":\n",
    "            inv_names.append(drug_name)\n",
    "            inv_alt_names.append(alt_names)\n",
    "            inv_targets.append(molecular_target)\n",
    "            inv_mechanisms.append(mechanism)\n",
    "            inv_tt_ids.append(tt_id)\n",
    "            inv_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"active comparator\":\n",
    "            ac_names.append(drug_name)\n",
    "            ac_alt_names.append(alt_names)\n",
    "            ac_targets.append(molecular_target)\n",
    "            ac_mechanisms.append(mechanism)\n",
    "            ac_tt_ids.append(tt_id)\n",
    "            ac_bmt_ids.append(bmt_id)\n",
    "\n",
    "        elif role_norm == \"placebo\":\n",
    "            plc_names.append(drug_name)\n",
    "            plc_alt_names.append(alt_names)\n",
    "            plc_targets.append(molecular_target)\n",
    "            plc_mechanisms.append(mechanism)\n",
    "\n",
    "        elif role_norm == \"standard of care\":\n",
    "            soc_names.append(drug_name)\n",
    "            soc_alt_names.append(alt_names)\n",
    "            soc_targets.append(molecular_target)\n",
    "            soc_mechanisms.append(mechanism)\n",
    "            soc_tt_ids.append(tt_id)\n",
    "            soc_bmt_ids.append(bmt_id)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "\n",
    "            \"investigational_products\": inv_names,\n",
    "            \"investigational_products_alternative_names\": inv_alt_names,\n",
    "            \"investigational_products_molecular_target\": inv_targets,\n",
    "            \"investigational_products_mechanism\": inv_mechanisms,\n",
    "            \"investigational_products_tt_drug_id\": inv_tt_ids,\n",
    "            \"investigational_products_bmt_drug_id\": inv_bmt_ids,\n",
    "\n",
    "            \"active_comparators\": ac_names,\n",
    "            \"active_comparators_alternative_names\": ac_alt_names,\n",
    "            \"active_comparators_molecular_target\": ac_targets,\n",
    "            \"active_comparators_mechanism\": ac_mechanisms,\n",
    "            \"active_comparators_tt_drug_id\": ac_tt_ids,\n",
    "            \"active_comparators_bmt_drug_id\": ac_bmt_ids,\n",
    "\n",
    "            \"placebos\": plc_names,\n",
    "            \"placebos_alternative_names\": plc_alt_names,\n",
    "            \"placebos_molecular_target\": plc_targets,\n",
    "            \"placebos_mechanism\": plc_mechanisms,\n",
    "\n",
    "            \"standard_of_care\": soc_names,\n",
    "            \"standard_of_care_alternative_names\": soc_alt_names,\n",
    "            \"standard_of_care_molecular_target\": soc_targets,\n",
    "            \"standard_of_care_mechanism\": soc_mechanisms,\n",
    "            \"standard_of_care_tt_drug_id\": soc_tt_ids,\n",
    "            \"standard_of_care_bmt_drug_id\": soc_bmt_ids,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved trial product breakdown to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca77eab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NO investigational products: 5 / 184\n",
      "| trial_hash                           | investigational_products   |\n",
      "|:-------------------------------------|:---------------------------|\n",
      "| tid_4c45730f6411aa1e5a38bb1223d66988 | []                         |\n",
      "| tid_67de51bf9728e056a6fb42c76e4b0212 | []                         |\n",
      "| tid_8cab7b7177fcb0d10255bced8b0633ee | []                         |\n",
      "| tid_bb1e0571142dde8a49976632c349593c | []                         |\n",
      "| tid_e9e01f51b6680ba4f467ac191bb307c5 | []                         |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Identify trials with no investigational products.\n",
    "\n",
    "Purpose:\n",
    "- Load the trial_product_breakdown.csv file.\n",
    "- Parse the stringified list column \"investigational_products\" into real Python lists.\n",
    "- Flag all trials where the parsed list is empty (i.e., no investigational product identified).\n",
    "- Print summary statistics and display rows missing investigational products.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "\n",
    "Outputs:\n",
    "- Console summary of how many trials lack investigational products.\n",
    "- Markdown preview of example rows with missing investigational products.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "def parse_listish(s: str):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = s.strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "# Parse the investigational_products column into real lists\n",
    "df[\"investigational_products_parsed\"] = df[\"investigational_products\"].apply(parse_listish)\n",
    "\n",
    "# Flag rows with no investigational products\n",
    "no_inv_mask = df[\"investigational_products_parsed\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "num_no_inv = int(no_inv_mask.sum())\n",
    "total = len(df)\n",
    "\n",
    "print(f\"Rows with NO investigational products: {num_no_inv} / {total}\")\n",
    "\n",
    "print(\n",
    "    df.loc[no_inv_mask, [\"trial_hash\", \"investigational_products\"]]\n",
    "      .head(20)\n",
    "      .to_markdown(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ab97f",
   "metadata": {},
   "source": [
    "Manual checks \n",
    "- tid_4c45730f6411aa1e5a38bb1223d66988\n",
    "    - This trial is combining three standard-of-care agents into a regimen “DCF”\n",
    "- tid_67de51bf9728e056a6fb42c76e4b0212\n",
    "    - Even though they administer Yisaipu in a structured way, it is an approved drug and not being tested for regulatory approval.\n",
    "- tid_8cab7b7177fcb0d10255bced8b0633ee\n",
    "    - The trial is studying treatment strategies, regimens, algorithms, imaging-guided regimen selection, or dosing, using only approved standard therapies.\n",
    "- tid_bb1e0571142dde8a49976632c349593c\n",
    "    - The trial's focus is on optimizing regimen selection (e.g., TIPy or TCbIPy) via imaging, rather than testing a new drug entity.\n",
    "- tid_e9e01f51b6680ba4f467ac191bb307c5\n",
    "    - All drugs are approved, marketed standard-of-care therapies that the sponsor does not own\n",
    "\n",
    "these are all confirmed generics biosimilars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebdc7bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated by tt_drug_id (first 10 rows):\n",
      "|   tt_drug_id | drug_names                           | alternative_names                                                                                                                                                                                                                                                                                                                         | molecular_targets                                                                                                                                   | product_mechanisms                                                                                                                                 | trial_hashes                                                                                                                                                                                                                                     |\n",
      "|-------------:|:-------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|       104351 | ['recombinant human thrombopoietin'] | ['TPIAO', 'human thrombopoietin', 'recombinant thrombopoietin', 'rhTPO', 'thrombopoietin']                                                                                                                                                                                                                                                | ['MPL (TPO receptor)']                                                                                                                              | ['Thrombopoietin receptor agonist']                                                                                                                | ['tid_58a84007ff09f957ed7e2275b22a07e6']                                                                                                                                                                                                         |\n",
      "|       104513 | ['recombinant human thrombopoietin'] | ['TPIAO', 'recombinant human TPO', 'rhTPO', 'thrombopoietin']                                                                                                                                                                                                                                                                             | ['MPL (TPO receptor)']                                                                                                                              | ['Thrombopoietin receptor agonist (recombinant protein)']                                                                                          | ['tid_1c2e944946f54731a88b019db7abcb63']                                                                                                                                                                                                         |\n",
      "|       113881 | ['SB8']                              | ['615', 'Aybintio', 'Onbevzi', 'SB 8', 'SB-8', 'SB8 Anti-VEGF antibody', 'bevacizumab, Samsung Bioepis']                                                                                                                                                                                                                                  | ['VEGF-A']                                                                                                                                          | ['Anti-VEGF-A monoclonal antibody (angiogenesis inhibitor)']                                                                                       | ['tid_0e698ee5065c49d23fcf57516957a273']                                                                                                                                                                                                         |\n",
      "|        11947 | ['Methotrexate', 'methotrexate']     | ['Brimexate', 'Emtexate', 'Emthexate', 'Farmitrexat', 'Farmotrex', 'Imeth', 'Lantarel', 'Ledertrexate', 'Ledertrexato', 'MTX', 'Matrex', 'Maxtrex', 'Methotrexat', 'Methotrexate-Faulding', 'Metotreksat', 'Metotressato', 'Metotrexato', 'Metrexan', 'Novatrex', 'Oncotrex', 'Rheumatrex', 'Tremetex', 'Trexall', 'methotrexate (oral)'] | ['DHFR', 'Dihydrofolate reductase (DHFR)']                                                                                                          | ['Antimetabolite; DHFR inhibitor', 'Antimetabolite; dihydrofolate reductase inhibitor', 'DHFR inhibitor (antimetabolite)']                         | ['tid_8cab7b7177fcb0d10255bced8b0633ee', 'tid_9dc4ace9308864c9f8a619d6abe32011', 'tid_fc93655913a5e1b233a8077e9fc758c6']                                                                                                                         |\n",
      "|       122198 | ['Bevacizumab']                      | ['601 t', '601-t', '601t', 'bevacizumab, 3SBio', 'bevacizumab, Shanghai CP Guojian']                                                                                                                                                                                                                                                      | ['VEGF-A']                                                                                                                                          | ['Anti-VEGF-A monoclonal antibody']                                                                                                                | ['tid_ffaa31cb9bc69b1f30ef420bf9395efd']                                                                                                                                                                                                         |\n",
      "|       122500 | ['Fexofenadine hydrochloride']       | ['Fexofenadine', 'Fexofenadine (Wansheng candidate)', 'Fexofenadine HCl', 'Fexofenadine hydrochloride tablets', 'Lai Duofei', 'fexofenadine hydrochloride, Zhejiang Wansheng']                                                                                                                                                            | ['Histamine H1 receptor', 'Histamine H1 receptor (HRH1)']                                                                                           | ['Histamine H1 receptor antagonist']                                                                                                               | ['tid_7cbe7f647635abbe0ccccf9233a6e375', 'tid_8c76966ccc6c54153d53dc85c07f1d4b']                                                                                                                                                                 |\n",
      "|        12359 | ['mycophenolate mofetil']            | ['Cellcept', 'MMF', 'Munoloc', 'R-99', 'R99', 'RS-61443', 'Renodapt', 'mycophenolate mofetil hydrochloride']                                                                                                                                                                                                                              | ['IMPDH']                                                                                                                                           | ['Inosine monophosphate dehydrogenase inhibitor']                                                                                                  | ['tid_28a767e788d4d9a4e65b3c10d10585c2']                                                                                                                                                                                                         |\n",
      "|       125796 | ['Apremilast', 'apremilast']         | ['AP-506', 'AP506', 'apremilast, 3SBio']                                                                                                                                                                                                                                                                                                  | ['PDE4']                                                                                                                                            | ['Phosphodiesterase 4 inhibitor']                                                                                                                  | ['tid_30e06d0602ca191ee72282e5070e1a2c', 'tid_888909319365ceead4be875847493bb0']                                                                                                                                                                 |\n",
      "|       126134 | ['toripalimab']                      | ['JS-001', 'JS001', 'Loqtorzi', 'TAB-001', 'Tuoyi', 'toripalimab-tpzi']                                                                                                                                                                                                                                                                   | ['PD-1 (PDCD1)']                                                                                                                                    | ['Anti-PD-1 antibody']                                                                                                                             | ['tid_e9e01f51b6680ba4f467ac191bb307c5']                                                                                                                                                                                                         |\n",
      "|       130313 | ['SSS-17', 'SSS17']                  | ['HIF 117', 'HIF-117', 'HIF-117 capsules', 'HIF-117Capsules', 'HIF117', 'SSS 17', 'SSS-17', 'SSS17', '[14C] SSS17', '[14C]-SSS17', '[14C]HIF-117', '[14C]SSS17', '[¹⁴C] SSS17', '[¹⁴C]-SSS17', '[¹⁴C]SSS17']                                                                                                                              | ['HIF', 'HIF prolyl hydroxylase (PHD)', 'HIF prolyl hydroxylases (HIF-PH; PHD1/2/3)', 'Hypoxia-inducible factor', 'Hypoxia-inducible factor (HIF)'] | ['Hypoxia-inducible factor antagonist', 'Small-molecule HIF prolyl hydroxylase inhibitor', 'Small-molecule HIF-PH (prolyl hydroxylase) inhibitor'] | ['tid_0e8fa21079f928135dfc6164a15285f8', 'tid_232704536c29e97119c451c906d641f0', 'tid_394f064db860c7cebcdd82c40cad1d9d', 'tid_6dc666a780607d3dcb99598f22cb5210', 'tid_b31c0cb67a9379f28e0393dc632f56d2', 'tid_e7a7c80dc675e3ddaf7b4f73690f0015'] |\n",
      "Saved aggregated tt_drug_id table → cache/task_1/product_id_master_by_tt.csv\n",
      "Rows missing molecular_targets AND product_mechanisms:\n",
      "|   tt_drug_id | drug_names                                                 | alternative_names                                            | molecular_targets   | product_mechanisms   | trial_hashes                                                                     |\n",
      "|-------------:|:-----------------------------------------------------------|:-------------------------------------------------------------|:--------------------|:---------------------|:---------------------------------------------------------------------------------|\n",
      "|       131499 | ['SSS-24']                                                 | ['SSS 24', 'SSS24']                                          | []                  | []                   | ['tid_d094fcbc52c756f274a0b0cbb0f37bcb']                                         |\n",
      "|       182230 | ['narfurine hydrochloride']                                | ['narfurine hydrochloride orally disintegrating tablets']    | []                  | []                   | ['tid_302158af0c6b2664f2a682e42b6de1e8']                                         |\n",
      "|        57757 | ['Undisclosed chemotherapy', 'undisclosed - chemotherapy'] | ['chemotherapy (unspecified)', 'undisclosed - chemotherapy'] | []                  | []                   | ['tid_277c0971f8acb4acb4d7104050fa0231', 'tid_f48b01eb433692a187251a3a10fa9923'] |\n",
      "Saved → cache/task_1/product_id_missing_targets_or_mechs.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate trial-level product data by TrialTrove tt_drug_id and identify products\n",
    "missing both molecular targets and mechanisms.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_1/trial_product_breakdown.csv\n",
    "    Contains per-trial lists of products and their tt_drug_id, targets, and mechanisms.\n",
    "\n",
    "Process:\n",
    "- Parse list-like columns from strings into Python lists.\n",
    "- Aggregate across all trials, keyed by tt_drug_id, collecting:\n",
    "    • drug_names\n",
    "    • alternative_names\n",
    "    • molecular_targets\n",
    "    • product_mechanisms\n",
    "    • trial_hashes where each product appears.\n",
    "- Build a product-level master table (one row per tt_drug_id).\n",
    "- Identify tt_drug_id entries that are missing BOTH molecular_targets\n",
    "  and product_mechanisms.\n",
    "\n",
    "Outputs:\n",
    "- Product master table:\n",
    "      cache/task_1/product_id_master_by_tt.csv\n",
    "- Table of products missing both targets and mechanisms:\n",
    "      cache/task_1/product_id_missing_targets_or_mechs.csv\n",
    "- Console preview of the first 10 aggregated rows and all missing rows.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "OUT_AGG = BASE_DIR / \"product_id_master_by_tt.csv\"\n",
    "OUT_MISSING = BASE_DIR / \"product_id_missing_targets_or_mechs.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Load trial-level breakdown\n",
    "df = pd.read_csv(IN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Aggregate everything keyed by tt_drug_id\n",
    "agg = {}  # tt_id -> {\"names\": set(), \"alt_names\": set(), \"targets\": set(), \"mechs\": set(), \"trials\": set()}\n",
    "\n",
    "ROLE_PAIRS = [\n",
    "    (\"investigational_products\", \"investigational_products_tt_drug_id\"),\n",
    "    (\"active_comparators\", \"active_comparators_tt_drug_id\"),\n",
    "    (\"standard_of_care\", \"standard_of_care_tt_drug_id\"),\n",
    "]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "\n",
    "    for base_col, tt_col in ROLE_PAIRS:\n",
    "        # aligned lists\n",
    "        names_list   = parse_listish(row.get(base_col, \"\"))\n",
    "        alts_list    = parse_listish(row.get(f\"{base_col}_alternative_names\", \"\"))\n",
    "        targets_list = parse_listish(row.get(f\"{base_col}_molecular_target\", \"\"))\n",
    "        mechs_list   = parse_listish(row.get(f\"{base_col}_mechanism\", \"\"))\n",
    "        tt_ids       = parse_listish(row.get(tt_col, \"\"))\n",
    "\n",
    "        # iterate by index over tt_ids (they define the products)\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "            if not tt_id:\n",
    "                continue\n",
    "\n",
    "            # init aggregate bucket if needed\n",
    "            if tt_id not in agg:\n",
    "                agg[tt_id] = {\n",
    "                    \"names\": set(),\n",
    "                    \"alt_names\": set(),\n",
    "                    \"targets\": set(),\n",
    "                    \"mechs\": set(),\n",
    "                    \"trials\": set(),\n",
    "                }\n",
    "\n",
    "            # record trial hash if available\n",
    "            if trial_hash:\n",
    "                agg[tt_id][\"trials\"].add(trial_hash)\n",
    "\n",
    "            # name\n",
    "            if i < len(names_list):\n",
    "                name = str(names_list[i]).strip()\n",
    "                if name:\n",
    "                    agg[tt_id][\"names\"].add(name)\n",
    "\n",
    "            # alternative names (may be nested lists)\n",
    "            if i < len(alts_list):\n",
    "                alt_entry = alts_list[i]\n",
    "                if isinstance(alt_entry, list):\n",
    "                    for a in alt_entry:\n",
    "                        a_str = str(a).strip()\n",
    "                        if a_str:\n",
    "                            agg[tt_id][\"alt_names\"].add(a_str)\n",
    "                else:\n",
    "                    a_str = str(alt_entry).strip()\n",
    "                    if a_str:\n",
    "                        agg[tt_id][\"alt_names\"].add(a_str)\n",
    "\n",
    "            # target\n",
    "            if i < len(targets_list):\n",
    "                tgt = str(targets_list[i]).strip()\n",
    "                if tgt:\n",
    "                    agg[tt_id][\"targets\"].add(tgt)\n",
    "\n",
    "            # mechanism\n",
    "            if i < len(mechs_list):\n",
    "                mech = str(mechs_list[i]).strip()\n",
    "                if mech:\n",
    "                    agg[tt_id][\"mechs\"].add(mech)\n",
    "\n",
    "# Build aggregated DataFrame\n",
    "rows_out = []\n",
    "for tt_id, payload in agg.items():\n",
    "    rows_out.append(\n",
    "        {\n",
    "            \"tt_drug_id\": tt_id,\n",
    "            \"drug_names\": sorted(payload[\"names\"]),\n",
    "            \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "            \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "            \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "            \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "grouped_df = pd.DataFrame(rows_out).sort_values(\"tt_drug_id\")\n",
    "\n",
    "print(\"Aggregated by tt_drug_id (first 10 rows):\")\n",
    "print(grouped_df.head(10).to_markdown(index=False))\n",
    "\n",
    "grouped_df.to_csv(OUT_AGG, index=False)\n",
    "print(f\"Saved aggregated tt_drug_id table → {OUT_AGG}\")\n",
    "\n",
    "# Identify rows missing both targets AND mechanisms\n",
    "missing_mask = grouped_df[\"molecular_targets\"].apply(lambda x: len(x) == 0) & \\\n",
    "               grouped_df[\"product_mechanisms\"].apply(lambda x: len(x) == 0)\n",
    "\n",
    "missing_df = grouped_df[missing_mask].copy()\n",
    "\n",
    "print(\"Rows missing molecular_targets AND product_mechanisms:\")\n",
    "if missing_df.empty:\n",
    "    print(\"No missing values — every tt_drug_id has at least one target or mechanism.\")\n",
    "else:\n",
    "    print(missing_df.to_markdown(index=False))\n",
    "\n",
    "missing_df.to_csv(OUT_MISSING, index=False)\n",
    "print(f\"Saved → {OUT_MISSING}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d86691e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 products missing targets/mechanisms from cache/task_1/product_id_missing_targets_or_mechs.csv\n",
      "Product mechanism inference complete. processed=0, skipped=3, llm_error=0, parse_error=0\n",
      "Per-product directory: cache/task_1/product_mechanism_inference\n",
      "Log directory:        cache/task_1/product_mechanism_inference_log\n",
      "Master file:          cache/task_1/product_mechanism_inference_master.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Infer molecular targets and mechanisms of action for products missing both fields,\n",
    "using LLM + web search on trial context.\n",
    "\n",
    "Inputs:\n",
    "- Product-level CSV of products missing targets/mechanisms:\n",
    "      cache/task_1/product_id_missing_targets_or_mechs.csv\n",
    "    (one row per tt_drug_id; columns include tt_drug_id, drug_names,\n",
    "     alternative_names, trial_hashes, etc.)\n",
    "- Trial metadata CSV:\n",
    "      cache/data_preprocess/raw_trials_with_hash.csv\n",
    "    (one row per trial; must include trial_hash and core text/JSON fields).\n",
    "\n",
    "Process:\n",
    "- Load the \"missing products\" table from disk.\n",
    "- Build an index of trial_hash → full trial metadata.\n",
    "- For each tt_drug_id in the missing-products table:\n",
    "    • Parse trial_hashes and gather associated trials.\n",
    "    • Build a prompt with all known drug names and trial context.\n",
    "    • Call the LLM with web_search tools to infer:\n",
    "        - molecular_target\n",
    "        - mechanism\n",
    "      If the target/mechanism is not publicly disclosed, both fields should be \"\".\n",
    "- Save a per-product JSON file and a structured log.\n",
    "- Maintain a rolling master JSON of all inferred product mechanisms.\n",
    "\n",
    "Outputs:\n",
    "- Per-product mechanism JSON:\n",
    "      cache/task_1/product_mechanism_inference/{tt_drug_id}.json\n",
    "- Per-product log JSON:\n",
    "      cache/task_1/product_mechanism_inference_log/{tt_drug_id}.json\n",
    "- Master mapping:\n",
    "      cache/task_1/product_mechanism_inference_master.json\n",
    "- Console summary of processed / skipped / error counts.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# Base dir for this task's outputs\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "\n",
    "# Trial metadata (shared across tasks)\n",
    "RAW_TRIALS_CSV = Path(\"cache/data_preprocess/raw_trials_with_hash.csv\")\n",
    "\n",
    "# Missing-products table (from previous aggregation cell)\n",
    "MISSING_PRODUCTS_CSV = BASE_DIR / \"product_id_missing_targets_or_mechs.csv\"\n",
    "\n",
    "# Output dirs/files for product mechanism inference\n",
    "PRODUCT_MECH_DIR = BASE_DIR / \"product_mechanism_inference\"\n",
    "PRODUCT_MECH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRODUCT_MECH_LOG_DIR = BASE_DIR / \"product_mechanism_inference_log\"\n",
    "PRODUCT_MECH_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def safe_parse_listish(val):\n",
    "    \"\"\"\n",
    "    Parse list-like strings back into Python lists, if needed.\n",
    "    If already a list, return as-is.\n",
    "    \"\"\"\n",
    "    if isinstance(val, list):\n",
    "        return val\n",
    "    if val is None:\n",
    "        return []\n",
    "    s = str(val).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def build_product_prompt(row: dict, trial_context: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt asking the LLM (with web_search) to infer\n",
    "    molecular_target and mechanism for a product, based on:\n",
    "      - drug names / alternative names\n",
    "      - full metadata for associated trials\n",
    "    \"\"\"\n",
    "    drug_names = safe_parse_listish(row.get(\"drug_names\", [])) or []\n",
    "\n",
    "    # Ensure lists are JSON-serializable\n",
    "    try:\n",
    "        drug_names_json = json.dumps(drug_names, ensure_ascii=False)\n",
    "    except TypeError:\n",
    "        drug_names_json = json.dumps([str(x) for x in drug_names], ensure_ascii=False)\n",
    "\n",
    "    # Trials context JSON\n",
    "    trials_json = json.dumps(trial_context, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a pharmacology expert with access to web search.\n",
    "\n",
    "You are given:\n",
    "- A drug name (and aliases).\n",
    "- Full metadata for one or more clinical trials in which this drug appears (JSON objects).\n",
    "\n",
    "Your goal:\n",
    "Using web search and your domain knowledge, determine:\n",
    "1. The primary molecular target(s) of the drug (e.g., EGFR, VEGFR2, TNF, CD20, JAK1/2).\n",
    "2. A concise, standard mechanism of action label (e.g., \"EGFR inhibitor\", \"Anti-PD-1 antibody\", \n",
    "   \"JAK inhibitor\", \"DNA-damaging cytotoxic\", etc.).\n",
    "\n",
    "Rules:\n",
    "- Try searching for the drug using all known names or aliases.\n",
    "- If no molecular target or mechanism of action has been publicly disclosed,\n",
    "  then return empty strings for BOTH fields.\n",
    "\n",
    "INPUT\n",
    "-----\n",
    "drug_name: {drug_names_json}\n",
    "trial_metadata:\n",
    "{trials_json}\n",
    "\n",
    "OUTPUT (JSON only)\n",
    "------------------\n",
    "{{\n",
    "  \"molecular_target\": \"\",\n",
    "  \"mechanism\": \"\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Shared counters & master mapping\n",
    "product_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "}\n",
    "product_counter_lock = threading.Lock()\n",
    "\n",
    "product_master: dict[str, dict] = {}\n",
    "product_master_lock = threading.Lock()\n",
    "\n",
    "# Load existing master if present\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    try:\n",
    "        product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        product_master = {}\n",
    "\n",
    "# Load trial metadata and build index\n",
    "df_trials = pd.read_csv(RAW_TRIALS_CSV, dtype=str).fillna(\"\")\n",
    "trials_index: dict[str, dict] = {\n",
    "    str(row[\"trial_hash\"]).strip(): row.to_dict()\n",
    "    for _, row in df_trials.iterrows()\n",
    "}\n",
    "\n",
    "\n",
    "def process_product(row: dict, idx: int, total: int) -> None:\n",
    "    \"\"\"Process one tt_drug_id: call LLM+web_search with trial context, save output & log.\"\"\"\n",
    "    tt_drug_id = str(row.get(\"tt_drug_id\", \"\")).strip()\n",
    "    if not tt_drug_id:\n",
    "        print(f\"[{idx}/{total}] Missing tt_drug_id, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = PRODUCT_MECH_DIR / f\"{tt_drug_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # Get associated trial hashes and build trial context list\n",
    "    trial_hashes_raw = row.get(\"trial_hashes\", [])\n",
    "    trial_hashes = safe_parse_listish(trial_hashes_raw)\n",
    "\n",
    "    trial_context = []\n",
    "    for th in trial_hashes:\n",
    "        th_key = str(th).strip()\n",
    "        if not th_key:\n",
    "            continue\n",
    "        trial_row = trials_index.get(th_key)\n",
    "        if trial_row:\n",
    "            trial_context.append(trial_row)\n",
    "\n",
    "    prompt = build_product_prompt(row, trial_context)\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM with web_search tool\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(\n",
    "            prompt=prompt,\n",
    "            model=MODEL,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "        )\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}/{total}] LLM error for tt_drug_id={tt_drug_id}: {e}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    mech_obj = extract_json_object(text_response)\n",
    "\n",
    "    # Expect a dict with the two keys\n",
    "    if not isinstance(mech_obj, dict) or not mech_obj:\n",
    "        print(f\"[{idx}/{total}] JSON parse/validity error tt_drug_id={tt_drug_id}, raw={text_response!r}\")\n",
    "        with product_counter_lock:\n",
    "            product_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    molecular_target = str(mech_obj.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "    mechanism = str(mech_obj.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "    mapped = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"drug_names\": safe_parse_listish(row.get(\"drug_names\", [])),\n",
    "        \"alternative_names\": safe_parse_listish(row.get(\"alternative_names\", [])),\n",
    "        \"trial_hashes\": trial_hashes,\n",
    "        \"molecular_target\": molecular_target,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"source\": \"llm_web_search\",\n",
    "    }\n",
    "\n",
    "    # Save per-product JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"tt_drug_id\": tt_drug_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (PRODUCT_MECH_LOG_DIR / f\"{tt_drug_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with product_master_lock:\n",
    "        product_master[tt_drug_id] = mapped\n",
    "        MASTER_PRODUCT_MECH_PATH.write_text(\n",
    "            json.dumps(product_master, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with product_counter_lock:\n",
    "        product_counter[\"processed\"] += 1\n",
    "        if product_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {product_counter['processed']} products...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "missing_df = pd.read_csv(MISSING_PRODUCTS_CSV, dtype=str).fillna(\"\")\n",
    "missing_rows = missing_df.to_dict(orient=\"records\")\n",
    "total_missing = len(missing_rows)\n",
    "print(f\"Loaded {total_missing} products missing targets/mechanisms from {MISSING_PRODUCTS_CSV}\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_product, row, idx, total_missing): row.get(\"tt_drug_id\")\n",
    "        for idx, row in enumerate(missing_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        tid = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Worker error tt_drug_id={tid}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Product mechanism inference complete. \"\n",
    "    f\"processed={product_counter['processed']}, \"\n",
    "    f\"skipped={product_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={product_counter['llm_error']}, \"\n",
    "    f\"parse_error={product_counter['parse_error']}\"\n",
    ")\n",
    "print(f\"Per-product directory: {PRODUCT_MECH_DIR}\")\n",
    "print(f\"Log directory:        {PRODUCT_MECH_LOG_DIR}\")\n",
    "print(f\"Master file:          {MASTER_PRODUCT_MECH_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90e85a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trial breakdown: cache/task_1/trial_product_breakdown.csv, shape=(184, 23)\n",
      "Aggregated 114 distinct tt_drug_id entries.\n",
      "Aggregated 15 products without tt_drug_id but with target/mechanism.\n",
      "Saved did-keyed drug master JSON → cache/task_1/product_id_master_by_did.json\n",
      "Total drugs: 129\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build a did-keyed master product dictionary by merging trial-level product info\n",
    "with (optional) LLM-inferred mechanisms.\n",
    "\n",
    "Inputs:\n",
    "- Trial product breakdown CSV:\n",
    "      cache/task_1/trial_product_breakdown.csv\n",
    "  Contains, per trial_hash:\n",
    "      • investigational_products / active_comparators / standard_of_care\n",
    "      • *_alternative_names\n",
    "      • *_molecular_target\n",
    "      • *_mechanism\n",
    "      • *_tt_drug_id\n",
    "- Optional LLM mechanism master:\n",
    "      cache/task_1/product_mechanism_inference_master.json\n",
    "  Maps tt_drug_id → inferred molecular_target and mechanism.\n",
    "\n",
    "Process:\n",
    "- For each role (investigational_products, active_comparators, standard_of_care):\n",
    "    • Parse list-like columns into Python lists.\n",
    "    • Aggregate by tt_drug_id:\n",
    "        - collect all names, alt_names, targets, mechanisms, trial_hashes.\n",
    "        - fill missing targets/mechanisms from product_mechanism_inference_master.json\n",
    "    • For entries without tt_drug_id but with target/mechanism:\n",
    "        - create synthetic product entries keyed by (role, name, target, mechanism).\n",
    "- Generate a stable deterministic \"did_*\" ID:\n",
    "    • For known tt_drug_id: hash of tt_drug_id.\n",
    "    • For unknown products: hash of composite key (role + data).\n",
    "\n",
    "Outputs:\n",
    "- did-keyed JSON:\n",
    "      cache/task_1/product_id_master_by_did.json\n",
    "  Structure:\n",
    "      {\n",
    "        \"did_<hash>\": {\n",
    "          \"did\": \"did_<hash>\",\n",
    "          \"tt_drug_id\": \"<tt_id or ''>\",\n",
    "          \"drug_names\": [...],\n",
    "          \"alternative_names\": [...],\n",
    "          \"molecular_targets\": [...],\n",
    "          \"product_mechanisms\": [...],\n",
    "          \"trial_hashes\": [...]\n",
    "        },\n",
    "        ...\n",
    "      }\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "import ast\n",
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_1\")\n",
    "IN_BREAKDOWN_CSV = BASE_DIR / \"trial_product_breakdown.csv\"\n",
    "MASTER_PRODUCT_MECH_PATH = BASE_DIR / \"product_mechanism_inference_master.json\"\n",
    "OUT_JSON = BASE_DIR / \"product_id_master_by_did.json\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string (e.g. \"['a','b']\") into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def pad_to_length(lst, n):\n",
    "    \"\"\"Pad list with empty strings so len(lst) >= n.\"\"\"\n",
    "    lst = list(lst)\n",
    "    while len(lst) < n:\n",
    "        lst.append(\"\")\n",
    "    return lst\n",
    "\n",
    "\n",
    "def make_did_from_tt(tt_drug_id: str) -> str:\n",
    "    \"\"\"Deterministic drug hash ID based on tt_drug_id (for known IDs).\"\"\"\n",
    "    h = hashlib.md5(tt_drug_id.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"did_{h}\"\n",
    "\n",
    "\n",
    "def make_did_for_unknown(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic drug hash ID for products without tt_drug_id.\n",
    "    Key can be any composite string (e.g., role + name + target + mechanism).\n",
    "    \"\"\"\n",
    "    h = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"did_{h}\"\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------\n",
    "# Load inputs\n",
    "df = pd.read_csv(IN_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trial breakdown: {IN_BREAKDOWN_CSV}, shape={df.shape}\")\n",
    "\n",
    "if MASTER_PRODUCT_MECH_PATH.exists():\n",
    "    product_master = json.loads(MASTER_PRODUCT_MECH_PATH.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    product_master = {}\n",
    "    print(f\"No product master mech file found at {MASTER_PRODUCT_MECH_PATH}\")\n",
    "\n",
    "# role → (base_name_col, tt_id_col, target_col, mech_col)\n",
    "ROLE_SPECS = [\n",
    "    (\n",
    "        \"investigational_products\",\n",
    "        \"investigational_products_tt_drug_id\",\n",
    "        \"investigational_products_molecular_target\",\n",
    "        \"investigational_products_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"active_comparators\",\n",
    "        \"active_comparators_tt_drug_id\",\n",
    "        \"active_comparators_molecular_target\",\n",
    "        \"active_comparators_mechanism\",\n",
    "    ),\n",
    "    (\n",
    "        \"standard_of_care\",\n",
    "        \"standard_of_care_tt_drug_id\",\n",
    "        \"standard_of_care_molecular_target\",\n",
    "        \"standard_of_care_mechanism\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Aggregate per tt_drug_id and per \"unknown but has mech/target\"\n",
    "agg_tt = {}       # tt_id -> {...}\n",
    "agg_unknown = {}  # composite_key -> {...}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "\n",
    "    for base_col, tt_col, tgt_col, mech_col in ROLE_SPECS:\n",
    "        # Skip if any required column is missing\n",
    "        if tt_col not in df.columns or tgt_col not in df.columns or mech_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Base name + alt-name columns\n",
    "        names_list = parse_listish(row.get(base_col, \"\"))\n",
    "        alt_list   = parse_listish(row.get(f\"{base_col}_alternative_names\", \"\"))\n",
    "\n",
    "        tt_ids   = parse_listish(row.get(tt_col, \"\"))\n",
    "        targets  = parse_listish(row.get(tgt_col, \"\"))\n",
    "        mechs    = parse_listish(row.get(mech_col, \"\"))\n",
    "\n",
    "        # Align target/mech lists to tt_ids length\n",
    "        targets = pad_to_length(targets, len(tt_ids))\n",
    "        mechs   = pad_to_length(mechs, len(tt_ids))\n",
    "\n",
    "        for i, raw_tt in enumerate(tt_ids):\n",
    "            tt_id = str(raw_tt).strip()\n",
    "\n",
    "            # Name (by position if available)\n",
    "            name = \"\"\n",
    "            if i < len(names_list):\n",
    "                name = str(names_list[i]).strip()\n",
    "\n",
    "            # Alternative names (can be list-of-lists or flat)\n",
    "            alt_names_for_this = []\n",
    "            if i < len(alt_list):\n",
    "                alt_entry = alt_list[i]\n",
    "                if isinstance(alt_entry, list):\n",
    "                    for a in alt_entry:\n",
    "                        a_str = str(a).strip()\n",
    "                        if a_str:\n",
    "                            alt_names_for_this.append(a_str)\n",
    "                else:\n",
    "                    a_str = str(alt_entry).strip()\n",
    "                    if a_str:\n",
    "                        alt_names_for_this.append(a_str)\n",
    "\n",
    "            # Existing target/mechanism from CSV\n",
    "            csv_target = str(targets[i]).strip()\n",
    "            csv_mech   = str(mechs[i]).strip()\n",
    "\n",
    "            # Case 1: Have a tt_drug_id → normal aggregation\n",
    "            if tt_id:\n",
    "                if tt_id not in agg_tt:\n",
    "                    agg_tt[tt_id] = {\n",
    "                        \"names\": set(),\n",
    "                        \"alt_names\": set(),\n",
    "                        \"targets\": set(),\n",
    "                        \"mechs\": set(),\n",
    "                        \"trials\": set(),\n",
    "                    }\n",
    "\n",
    "                # Record trial hash\n",
    "                if trial_hash:\n",
    "                    agg_tt[tt_id][\"trials\"].add(trial_hash)\n",
    "\n",
    "                # Names\n",
    "                if name:\n",
    "                    agg_tt[tt_id][\"names\"].add(name)\n",
    "\n",
    "                for a_str in alt_names_for_this:\n",
    "                    agg_tt[tt_id][\"alt_names\"].add(a_str)\n",
    "\n",
    "                # LLM-inferred target/mechanism (if available)\n",
    "                info = product_master.get(tt_id) or {}\n",
    "                inferred_target = str(info.get(\"molecular_target\", \"\") or \"\").strip()\n",
    "                inferred_mech   = str(info.get(\"mechanism\", \"\") or \"\").strip()\n",
    "\n",
    "                # Final chosen values for this (trial, index, tt_id)\n",
    "                final_target = csv_target or inferred_target\n",
    "                final_mech   = csv_mech   or inferred_mech\n",
    "\n",
    "                if final_target:\n",
    "                    agg_tt[tt_id][\"targets\"].add(final_target)\n",
    "                if final_mech:\n",
    "                    agg_tt[tt_id][\"mechs\"].add(final_mech)\n",
    "\n",
    "            # Case 2: NO tt_drug_id, but we have target or mechanism\n",
    "            # → create a synthetic product entry with empty tt_drug_id\n",
    "            else:\n",
    "                # If we have no name and no mechanistic info, skip\n",
    "                if not (name or csv_target or csv_mech):\n",
    "                    continue\n",
    "\n",
    "                # Only create unknown entry if there is mechanistic info\n",
    "                if not (csv_target or csv_mech):\n",
    "                    continue\n",
    "\n",
    "                # Build a composite key to deduplicate unknown products\n",
    "                composite_key = f\"{base_col}||{name}||{csv_target}||{csv_mech}\"\n",
    "\n",
    "                if composite_key not in agg_unknown:\n",
    "                    agg_unknown[composite_key] = {\n",
    "                        \"names\": set(),\n",
    "                        \"alt_names\": set(),\n",
    "                        \"targets\": set(),\n",
    "                        \"mechs\": set(),\n",
    "                        \"trials\": set(),\n",
    "                    }\n",
    "\n",
    "                if trial_hash:\n",
    "                    agg_unknown[composite_key][\"trials\"].add(trial_hash)\n",
    "\n",
    "                if name:\n",
    "                    agg_unknown[composite_key][\"names\"].add(name)\n",
    "\n",
    "                for a_str in alt_names_for_this:\n",
    "                    agg_unknown[composite_key][\"alt_names\"].add(a_str)\n",
    "\n",
    "                if csv_target:\n",
    "                    agg_unknown[composite_key][\"targets\"].add(csv_target)\n",
    "                if csv_mech:\n",
    "                    agg_unknown[composite_key][\"mechs\"].add(csv_mech)\n",
    "\n",
    "print(f\"Aggregated {len(agg_tt)} distinct tt_drug_id entries.\")\n",
    "print(f\"Aggregated {len(agg_unknown)} products without tt_drug_id but with target/mechanism.\")\n",
    "\n",
    "# Build did-keyed JSON structure\n",
    "drug_master_by_did = {}\n",
    "\n",
    "# 1) Entries with real tt_drug_id\n",
    "for tt_id, payload in agg_tt.items():\n",
    "    did = make_did_from_tt(tt_id)\n",
    "    drug_master_by_did[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": tt_id,\n",
    "        \"drug_names\": sorted(payload[\"names\"]),\n",
    "        \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "        \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "        \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "        \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "    }\n",
    "\n",
    "# 2) Entries without tt_drug_id (tt_drug_id = \"\")\n",
    "for composite_key, payload in agg_unknown.items():\n",
    "    did = make_did_for_unknown(composite_key)\n",
    "    drug_master_by_did[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": \"\",  # explicitly empty as requested\n",
    "        \"drug_names\": sorted(payload[\"names\"]),\n",
    "        \"alternative_names\": sorted(payload[\"alt_names\"]),\n",
    "        \"molecular_targets\": sorted(payload[\"targets\"]),\n",
    "        \"product_mechanisms\": sorted(payload[\"mechs\"]),\n",
    "        \"trial_hashes\": sorted(payload[\"trials\"]),\n",
    "    }\n",
    "\n",
    "# Save JSON\n",
    "OUT_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSON.write_text(\n",
    "    json.dumps(drug_master_by_did, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(f\"Saved did-keyed drug master JSON → {OUT_JSON}\")\n",
    "print(f\"Total drugs: {len(drug_master_by_did)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709c055",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77190ce",
   "metadata": {},
   "source": [
    "Identify whether the drugs are innovative or/generic biosimilars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52a5c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 trials with investigational products for innovation-status classification.\n",
      "Progress: processed 50 trials for innovation status...\n",
      "Progress: processed 100 trials for innovation status...\n",
      "Progress: processed 150 trials for innovation status...\n",
      "Trial investigational-drug innovation classification complete. processed=179, skipped=0, llm_error=0, parse_error=0, coverage_error=0\n",
      "Classifications directory: cache/task_2/trial_investigational_drugs_classifications\n",
      "Log directory:             cache/task_2/trial_investigational_drugs_classifications_log\n",
      "Master classifications:    cache/task_2/trial_investigational_drugs_classifications_master.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classify investigational drugs in each trial as Innovative, Generic, or Biosimilar.\n",
    "\n",
    "Inputs (from cache/task_1/):\n",
    "- raw_trials_with_hash.csv\n",
    "    • Per-trial metadata (must include trial_hash and RELEVANT_COLS)\n",
    "- trial_product_breakdown.csv\n",
    "    • Per-trial drug-role breakdown (investigational_products, *_tt_drug_id, etc.)\n",
    "- product_id_master_by_did.json\n",
    "    • did-keyed product master including:\n",
    "        - tt_drug_id\n",
    "        - drug_names\n",
    "        - alternative_names\n",
    "        - molecular_targets\n",
    "        - product_mechanisms\n",
    "\n",
    "Process:\n",
    "- Build tt_drug_id → product metadata from product_id_master_by_did.json.\n",
    "- Merge trial metadata with trial_product_breakdown on trial_hash.\n",
    "- For each trial that has investigational products:\n",
    "    • Build per-drug context (names, alt names, targets, mechanisms, tt_drug_id).\n",
    "    • Build an LLM prompt with trial text + all breakdown columns.\n",
    "    • Ask the LLM to classify each investigational product as:\n",
    "          \"Innovative\", \"Generic\", or \"Biosimilar\",\n",
    "      and provide a one-sentence explanation + tt_drug_id.\n",
    "    • Validate JSON, coverage, and required fields.\n",
    "    • Save per-trial classification JSON, a log JSON, and update a master JSON.\n",
    "\n",
    "Outputs (to cache/task_2/):\n",
    "- trial_investigational_drugs_classifications/{trial_hash}.json\n",
    "- trial_investigational_drugs_classifications_log/{trial_hash}.json\n",
    "- trial_investigational_drugs_classifications_master.json\n",
    "- Console summary of processed / skipped / error counts.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "IN_BASE_DIR_PRE = Path(\"cache/data_preprocess\")\n",
    "IN_BASE_DIR_TASK1 = Path(\"cache/task_1\")\n",
    "\n",
    "TRIALS_WITH_HASH_CSV  = IN_BASE_DIR_PRE / \"raw_trials_with_hash.csv\"\n",
    "PRODUCT_BREAKDOWN_CSV = IN_BASE_DIR_TASK1 / \"trial_product_breakdown.csv\"\n",
    "PRODUCT_BY_DID_JSON   = IN_BASE_DIR_TASK1 / \"product_id_master_by_did.json\"\n",
    "\n",
    "OUT_BASE_DIR = Path(\"cache/task_2\")\n",
    "OUT_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INNOV_DIR = OUT_BASE_DIR / \"trial_investigational_drugs_classifications\"\n",
    "INNOV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INNOV_LOG_DIR = OUT_BASE_DIR / \"trial_investigational_drugs_classifications_log\"\n",
    "INNOV_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INNOV_PATH = OUT_BASE_DIR / \"trial_investigational_drugs_classifications_master.json\"\n",
    "\n",
    "RELEVANT_COLS = [\n",
    "    \"title\",\n",
    "    \"objective\",\n",
    "    \"outcome_details\",\n",
    "    \"treatment_plan\",\n",
    "    \"notes_json\",\n",
    "    \"results_json\",\n",
    "    \"primary_drugs_tested_json\",\n",
    "    \"other_drugs_tested_json\",\n",
    "    \"therapeutic_areas_json\",\n",
    "    \"bmt_other_drugs_tested_json\",\n",
    "    \"bmt_primary_drugs_tested_json\",\n",
    "    \"ct_gov_mesh_terms_json\",\n",
    "]\n",
    "\n",
    "MAX_WORKERS_INNOV = 8\n",
    "\n",
    "MODEL = \"gpt-5\"\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def load_master_innov() -> dict:\n",
    "    \"\"\"Load the master innovation classification JSON, or return empty dict if missing/invalid.\"\"\"\n",
    "    if not MASTER_INNOV_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_INNOV_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def parse_listish(s):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    # Common empty-list cases\n",
    "    if s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        # If it's something else, treat as a single non-empty token\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        # Fallback: treat non-empty string as a single element\n",
    "        return [s]\n",
    "\n",
    "\n",
    "def pad_to_length(lst, n):\n",
    "    \"\"\"Pad list with empty strings so len(lst) >= n.\"\"\"\n",
    "    lst = list(lst)\n",
    "    while len(lst) < n:\n",
    "        lst.append(\"\")\n",
    "    return lst\n",
    "\n",
    "\n",
    "def build_innovation_prompt(trial_payload: dict, drug_contexts: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt to classify each investigational product as\n",
    "    Innovative / Generic / Biosimilar, with one-sentence explanation,\n",
    "    using extra context about each drug (names, alt names, targets, mechanisms).\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(trial_payload, ensure_ascii=False, indent=2)\n",
    "    drugs_json   = json.dumps(drug_contexts, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are a clinical trial design and drug development expert.\n",
    "\n",
    "You are given:\n",
    "1) Structured information about a clinical trial (title, objective, results, etc.).\n",
    "2) A list of investigational products used in the trial, with extra metadata for each.\n",
    "   Each investigational drug entry has:\n",
    "   - \"name\": the canonical investigational product name in this trial\n",
    "   - \"tt_drug_id\": the TrialTrove/PharmaProjects drugId as a string (if known)\n",
    "   - \"drug_names\": list of names for this drug\n",
    "   - \"alternative_names\": list of alternative or synonym names\n",
    "   - \"molecular_targets\": list of known molecular targets\n",
    "   - \"product_mechanisms\": list of known mechanisms of action\n",
    "\n",
    "Your task: For EACH investigational product (by its \"name\"), classify whether it is:\n",
    "- \"Innovative\"\n",
    "- \"Generic\"\n",
    "- \"Biosimilar\"\n",
    "\n",
    "and provide:\n",
    "- a one-sentence concise explanation for your classification\n",
    "- the tt_drug_id (string; use \"\" if unknown).\n",
    "\n",
    "DEFINITIONS / GUIDANCE\n",
    "----------------------\n",
    "\n",
    "Innovative:\n",
    "- A novel or proprietary drug (new or sponsor-specific product).\n",
    "- New mechanism of action OR new molecular entity OR clearly the sponsor's lead product.\n",
    "- Often associated with efficacy or superiority language:\n",
    "  - \"evaluate efficacy\", \"vs placebo\", \"improve outcomes\", etc.\n",
    "- Not a copy of an already-approved product.\n",
    "\n",
    "Generic:\n",
    "- A small-molecule copy of an already-approved branded drug.\n",
    "- Same active ingredient, strength, dosage form, and route.\n",
    "- Often explicitly described as generic or equivalent.\n",
    "\n",
    "Biosimilar:\n",
    "- A biologic product that is highly similar to an already-approved reference biologic.\n",
    "- Same target and mechanism as a branded biologic.\n",
    "- Strong clues:\n",
    "  - \"equivalence\", \"non-inferiority\", \"no clinically meaningful differences\",\n",
    "  - direct comparison to a specific branded reference biologic.\n",
    "\n",
    "You MUST choose ONE of the three labels (\"Innovative\", \"Generic\", \"Biosimilar\") for each drug.\n",
    "If you are uncertain, you may say so in the one-sentence explanation, but still pick a label.\n",
    "\n",
    "Use all available information:\n",
    "- Trial text and design\n",
    "- Investigational vs comparator roles\n",
    "- Known targets/mechanisms from the drug metadata.\n",
    "\n",
    "OUTPUT FORMAT (IMPORTANT)\n",
    "-------------------------\n",
    "\n",
    "Return ONLY a valid JSON object, with:\n",
    "- KEYS   = exactly the investigational product \"name\" values given below\n",
    "- VALUES = an object with exactly three fields:\n",
    "    - \"classification\": one of \"Innovative\", \"Generic\", \"Biosimilar\"\n",
    "    - \"explanation\": a single, concise sentence explaining your reasoning\n",
    "    - \"tt_drug_id\": the string tt_drug_id for this drug (\"\" if unknown)\n",
    "\n",
    "Example output:\n",
    "{{\n",
    "  \"DrugA\": {{\n",
    "    \"classification\": \"Innovative\",\n",
    "    \"explanation\": \"DrugA is a novel monoclonal antibody targeting a new receptor and is the sponsor's lead product.\",\n",
    "    \"tt_drug_id\": \"123456\"\n",
    "  }},\n",
    "  \"DrugB\": {{\n",
    "    \"classification\": \"Biosimilar\",\n",
    "    \"explanation\": \"DrugB is tested for equivalence compared to the branded biologic with the same active ingredient.\",\n",
    "    \"tt_drug_id\": \"789012\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "TRIAL PAYLOAD (includes trial text and all drug-role breakdown columns):\n",
    "{payload_json}\n",
    "\n",
    "INVESTIGATIONAL DRUG CONTEXTS (you MUST classify EACH by its 'name' key):\n",
    "{drugs_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Shared master + counters\n",
    "master_innov = load_master_innov()\n",
    "master_lock = threading.Lock()\n",
    "\n",
    "innov_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,\n",
    "}\n",
    "counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_innov_row(row: dict, idx: int, total: int, breakdown_cols: list[str], tt_to_drug_meta: dict) -> None:\n",
    "    \"\"\"Process a single trial with investigational products and classify them.\"\"\"\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    if not trial_hash:\n",
    "        print(f\"⚠️ [{idx}/{total}] Missing trial_hash, skipping\")\n",
    "        return\n",
    "\n",
    "    # Names as used in the trial\n",
    "    investigational_products = row.get(\"investigational_products_parsed\") or []\n",
    "    investigational_products = [str(x).strip() for x in investigational_products if str(x).strip()]\n",
    "\n",
    "    if not investigational_products:\n",
    "        # Shouldn't happen due to filtering, but be safe\n",
    "        return\n",
    "\n",
    "    out_fp = INNOV_DIR / f\"{trial_hash}.json\"\n",
    "    if out_fp.exists():\n",
    "        with counter_lock:\n",
    "            innov_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Build per-drug context from did JSON\n",
    "    # ------------------------------------\n",
    "    inv_tt_raw = row.get(\"investigational_products_tt_drug_id\", \"\")\n",
    "    inv_tt_ids = parse_listish(inv_tt_raw)\n",
    "    inv_tt_ids = pad_to_length(inv_tt_ids, len(investigational_products))\n",
    "\n",
    "    drug_contexts = []\n",
    "    for i, name in enumerate(investigational_products):\n",
    "        tt_id = str(inv_tt_ids[i]).strip() if i < len(inv_tt_ids) else \"\"\n",
    "        meta = tt_to_drug_meta.get(tt_id, {}) if tt_id else {}\n",
    "\n",
    "        drug_contexts.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"tt_drug_id\": tt_id,\n",
    "                \"drug_names\": meta.get(\"drug_names\", []),\n",
    "                \"alternative_names\": meta.get(\"alternative_names\", []),\n",
    "                \"molecular_targets\": meta.get(\"molecular_targets\", []),\n",
    "                \"product_mechanisms\": meta.get(\"product_mechanisms\", []),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Build payload from selected columns\n",
    "    # ------------------------------------\n",
    "    trial_payload = {\"trial_hash\": trial_hash}\n",
    "\n",
    "    # 1) Trial-level textual fields from raw_trials_with_hash.csv\n",
    "    for col in RELEVANT_COLS:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    # 2) ALL columns from trial_product_breakdown.csv\n",
    "    for col in breakdown_cols:\n",
    "        trial_payload[col] = row.get(col, \"\")\n",
    "\n",
    "    prompt = build_innovation_prompt(trial_payload, drug_contexts)\n",
    "\n",
    "    token = trial_hash\n",
    "    hash_id = trial_hash\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ [{idx}/{total}] LLM error for trial_hash={trial_hash}: {e}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    classifications = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(classifications, dict) or not classifications:\n",
    "        print(f\"⚠️ [{idx}/{total}] JSON parse error trial_hash={trial_hash}, raw={text_response!r}\")\n",
    "        with counter_lock:\n",
    "            innov_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Check coverage: every investigational product must be present as a key\n",
    "    missing = [d for d in investigational_products if d not in classifications]\n",
    "    if missing:\n",
    "        print(\n",
    "            f\"⚠️ [{idx}/{total}] Coverage error for trial_hash={trial_hash}: \"\n",
    "            f\"missing classifications for {missing}\"\n",
    "        )\n",
    "        with counter_lock:\n",
    "            innov_counter[\"coverage_error\"] += 1\n",
    "        # DO NOT save this trial so it can be re-run next time\n",
    "        return\n",
    "\n",
    "    # Sanity check: each value has classification, explanation, tt_drug_id\n",
    "    for d in investigational_products:\n",
    "        meta = classifications.get(d, {})\n",
    "        if not isinstance(meta, dict):\n",
    "            print(f\"⚠️ [{idx}/{total}] Invalid meta for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "        if (\"classification\" not in meta) or (\"explanation\" not in meta) or (\"tt_drug_id\" not in meta):\n",
    "            print(f\"⚠️ [{idx}/{total}] Missing fields for {d} in trial_hash={trial_hash}\")\n",
    "            with counter_lock:\n",
    "                innov_counter[\"parse_error\"] += 1\n",
    "            return\n",
    "\n",
    "    mapped = {\n",
    "        \"trial_hash\": trial_hash,\n",
    "        \"investigational_products\": investigational_products,\n",
    "        \"classifications\": classifications,\n",
    "        \"source\": \"llm\",\n",
    "    }\n",
    "\n",
    "    # Save per-trial JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (INNOV_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_lock:\n",
    "        master_innov[trial_hash] = mapped\n",
    "        MASTER_INNOV_PATH.write_text(\n",
    "            json.dumps(master_innov, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    with counter_lock:\n",
    "        innov_counter[\"processed\"] += 1\n",
    "        if innov_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {innov_counter['processed']} trials for innovation status...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "# Load per-drug metadata (product_id_master_by_did.json) and build tt_drug_id → metadata\n",
    "if PRODUCT_BY_DID_JSON.exists():\n",
    "    product_by_did = json.loads(PRODUCT_BY_DID_JSON.read_text(encoding=\"utf-8\"))\n",
    "else:\n",
    "    product_by_did = {}\n",
    "    print(f\"⚠️ No drug master JSON found at {PRODUCT_BY_DID_JSON}\")\n",
    "\n",
    "tt_to_drug_meta: dict[str, dict] = {}\n",
    "for did, rec in product_by_did.items():\n",
    "    tt = str(rec.get(\"tt_drug_id\", \"\")).strip()\n",
    "    if tt and tt not in tt_to_drug_meta:\n",
    "        tt_to_drug_meta[tt] = rec\n",
    "\n",
    "# Load breakdown (investigational products + all drug-role cols)\n",
    "df_breakdown = pd.read_csv(PRODUCT_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "df_breakdown[\"investigational_products_parsed\"] = df_breakdown[\"investigational_products\"].apply(parse_listish)\n",
    "mask_has_inv = df_breakdown[\"investigational_products_parsed\"].apply(lambda x: len(x) > 0)\n",
    "\n",
    "# Restrict to rows with investigational products\n",
    "df_breakdown_sub = df_breakdown.loc[mask_has_inv].copy()\n",
    "\n",
    "# All columns from trial_product_breakdown.csv except trial_hash (which is already separate)\n",
    "BREAKDOWN_COLS = [c for c in df_breakdown_sub.columns if c != \"trial_hash\"]\n",
    "\n",
    "# Load raw trials (for RELEVANT_COLS)\n",
    "df_trials = pd.read_csv(TRIALS_WITH_HASH_CSV, dtype=str).fillna(\"\")\n",
    "\n",
    "# Merge on trial_hash; keep all breakdown columns + investigational_products_parsed + RELEVANT_COLS\n",
    "df_merged = df_breakdown_sub.merge(\n",
    "    df_trials[[\"trial_hash\"] + RELEVANT_COLS],\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "innov_rows = df_merged.to_dict(orient=\"records\")\n",
    "total_innov = len(innov_rows)\n",
    "print(f\"Loaded {total_innov} trials with investigational products for innovation-status classification.\")\n",
    "\n",
    "# Run concurrently\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_INNOV) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_innov_row, row, idx, total_innov, BREAKDOWN_COLS, tt_to_drug_meta): row.get(\"trial_hash\")\n",
    "        for idx, row in enumerate(innov_rows, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        th = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Worker error (innovation) trial_hash={th}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"Trial investigational-drug innovation classification complete. \"\n",
    "    f\"processed={innov_counter['processed']}, \"\n",
    "    f\"skipped={innov_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={innov_counter['llm_error']}, \"\n",
    "    f\"parse_error={innov_counter['parse_error']}, \"\n",
    "    f\"coverage_error={innov_counter['coverage_error']}\"\n",
    ")\n",
    "print(f\"Classifications directory: {INNOV_DIR}\")\n",
    "print(f\"Log directory:             {INNOV_LOG_DIR}\")\n",
    "print(f\"Master classifications:    {MASTER_INNOV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c10a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LLM COST SUMMARY ==========\n",
      "Total LLM cost:             $1.8837\n",
      "Number of logged trials:     179\n",
      "Average cost per trial:      $0.0105\n",
      "\n",
      "Top 10 most expensive trials:\n",
      "  tid_9727cefa81bf0a9c341273bce42d3346.json: $0.0330\n",
      "  tid_8b4d60a5fddc078962af34399d7e342c.json: $0.0301\n",
      "  tid_99fac3ebe48aad5ebc1077142f61d5eb.json: $0.0218\n",
      "  tid_28a767e788d4d9a4e65b3c10d10585c2.json: $0.0215\n",
      "  tid_43635104c2d64be16c8882a500dd5181.json: $0.0205\n",
      "  tid_456995e1db18e20bcddc2bdf2938fac3.json: $0.0167\n",
      "  tid_b29013cdbc706b95776d47be1d6e98e6.json: $0.0167\n",
      "  tid_a50324f4d36f5cc93b795ec7f8b7005b.json: $0.0165\n",
      "  tid_69d3a93a71b9ed6021c817d9afa127fa.json: $0.0163\n",
      "  tid_f48b01eb433692a187251a3a10fa9923.json: $0.0161\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarize total LLM usage cost for previous cell by reading all per-trial log files.\n",
    "\n",
    "Inputs:\n",
    "- Directory: cache/task_2/trial_investigational_drugs_classifications\n",
    "    Each log JSON contains:\n",
    "        • total_cost (float)\n",
    "        • other metadata (prompt, raw response, timing, etc.)\n",
    "\n",
    "Process:\n",
    "- Load each log file and extract its total_cost value.\n",
    "- Aggregate total cost, count entries, and compute average cost per trial.\n",
    "- Sort trials by cost to identify the most expensive prompts.\n",
    "\n",
    "Outputs:\n",
    "- Console summary including:\n",
    "    • Total cost\n",
    "    • Number of logged trials\n",
    "    • Average cost per trial\n",
    "    • Top 10 highest-cost trials (filename + cost)\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_DIR = Path(\"cache/task_2/trial_investigational_drugs_classifications_log\")\n",
    "\n",
    "total_cost = 0.0\n",
    "num_entries = 0\n",
    "costs = []\n",
    "\n",
    "for fp in LOG_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        log = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "        c = float(log.get(\"total_cost\") or 0.0)\n",
    "        total_cost += c\n",
    "        costs.append((fp.name, c))\n",
    "        num_entries += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fp.name}: {e}\")\n",
    "\n",
    "# Sort descending by cost\n",
    "costs_sorted = sorted(costs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"========== LLM COST SUMMARY ==========\")\n",
    "print(f\"Total LLM cost:             ${total_cost:,.4f}\")\n",
    "print(f\"Number of logged trials:     {num_entries}\")\n",
    "if num_entries > 0:\n",
    "    print(f\"Average cost per trial:      ${total_cost / num_entries:,.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Top 10 most expensive trials:\")\n",
    "for name, c in costs_sorted[:10]:\n",
    "    print(f\"  {name}: ${c:,.4f}\")\n",
    "\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bebf824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved investigational drug classifications to cache/task_2/trial_investigational_drugs_classifications.csv\n",
      "| trial_hash                           | investigational_products                           | investigational_products_classifications   |\n",
      "|:-------------------------------------|:---------------------------------------------------|:-------------------------------------------|\n",
      "| tid_0541995757b10e613a42173d6b8ddc09 | [\"cinacalcet hydrochloride\"]                       | [\"Generic\"]                                |\n",
      "| tid_0d6e9b2f3f57c17c0e93610e28853f0c | [\"Xenopax\"]                                        | [\"Biosimilar\"]                             |\n",
      "| tid_0da20e863cfc5f3e369868462bff74e0 | [\"recombinant erythropoiesis-stimulating protein\"] | [\"Innovative\"]                             |\n",
      "| tid_0e698ee5065c49d23fcf57516957a273 | [\"SB8\"]                                            | [\"Biosimilar\"]                             |\n",
      "| tid_0e8fa21079f928135dfc6164a15285f8 | [\"SSS-17\"]                                         | [\"Innovative\"]                             |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Flatten per-trial investigational drug innovation JSON into a single CSV.\n",
    "\n",
    "Inputs (from cache/task_2/):\n",
    "- trial_investigational_drugs_classifications/{trial_hash}.json\n",
    "    Each JSON has:\n",
    "        • trial_hash\n",
    "        • investigational_products (list of raw drug strings)\n",
    "        • classifications: { raw_drug_name_or_key: {classification, explanation, tt_drug_id} }\n",
    "\n",
    "Process:\n",
    "- For each trial:\n",
    "    • For each raw investigational product entry:\n",
    "        - Parse it as a list if it's a stringified list (e.g. \"['inetetamab', 'toripalimab']\").\n",
    "        - Look up its classification using the original key first,\n",
    "          then fall back to each parsed name.\n",
    "        - Expand to flat aligned lists: one row-level list of product names,\n",
    "          and one row-level list of classifications.\n",
    "- Build a DataFrame with one row per trial:\n",
    "    • trial_hash\n",
    "    • investigational_products (JSON stringified flat list of names)\n",
    "    • investigational_products_classifications (JSON stringified flat list of labels)\n",
    "\n",
    "Outputs (to cache/task_2/):\n",
    "- trial_investigational_drugs_classifications.csv\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_2\")\n",
    "\n",
    "INNOV_DIR = BASE_DIR / \"trial_investigational_drugs_classifications\"\n",
    "OUT_CSV   = BASE_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def parse_listish(s):\n",
    "    \"\"\"\n",
    "    Parse a stringified list like \"['A', 'B']\" into a Python list.\n",
    "    If parsing fails or the cell is empty, return [].\n",
    "    \"\"\"\n",
    "    if isinstance(s, list):\n",
    "        return s\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, list):\n",
    "            return val\n",
    "        # If it's something else, treat as a single non-empty token\n",
    "        return [val]\n",
    "    except Exception:\n",
    "        # Fallback: treat non-empty string as a single element\n",
    "        return [s]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for fp in INNOV_DIR.glob(\"*.json\"):\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fp.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    trial_hash = obj.get(\"trial_hash\")\n",
    "    if not trial_hash:\n",
    "        print(f\"Missing trial_hash in {fp.name}, skipping\")\n",
    "        continue\n",
    "\n",
    "    inv_products_raw = obj.get(\"investigational_products\") or []\n",
    "    classifications_map = obj.get(\"classifications\") or {}\n",
    "\n",
    "    flat_products = []\n",
    "    flat_classifications = []\n",
    "\n",
    "    for drug_raw in inv_products_raw:\n",
    "        # drug_raw might be \"['inetetamab', 'toripalimab']\" or just \"SSGJ-707\"\n",
    "        if isinstance(drug_raw, str):\n",
    "            parsed_names = parse_listish(drug_raw)\n",
    "        else:\n",
    "            parsed_names = [drug_raw]\n",
    "\n",
    "        # Prefer classification using the exact key that was sent to the model\n",
    "        meta = classifications_map.get(drug_raw, {})\n",
    "        cls = meta.get(\"classification\", \"\")\n",
    "\n",
    "        # If not found, try each parsed name as a key\n",
    "        if not cls:\n",
    "            for name in parsed_names:\n",
    "                meta_n = classifications_map.get(name, {})\n",
    "                if \"classification\" in meta_n:\n",
    "                    cls = meta_n.get(\"classification\", \"\")\n",
    "                    break\n",
    "\n",
    "        if not cls:\n",
    "            print(\n",
    "                f\"Missing classification for raw drug {drug_raw!r} in \"\n",
    "                f\"trial_hash={trial_hash}, file={fp.name}\"\n",
    "            )\n",
    "\n",
    "        # Add one entry per parsed name so both lists are flat and aligned\n",
    "        for name in parsed_names:\n",
    "            flat_products.append(name)\n",
    "            flat_classifications.append(cls)\n",
    "\n",
    "    # Sanity check: lengths must match\n",
    "    if len(flat_products) != len(flat_classifications):\n",
    "        print(\n",
    "            f\"Length mismatch for trial_hash={trial_hash}: \"\n",
    "            f\"{len(flat_products)} products vs {len(flat_classifications)} classifications\"\n",
    "        )\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"trial_hash\": trial_hash,\n",
    "            # store as JSON stringified flat lists\n",
    "            \"investigational_products\": json.dumps(flat_products, ensure_ascii=False),\n",
    "            \"investigational_products_classifications\": json.dumps(flat_classifications, ensure_ascii=False),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_out = pd.DataFrame(rows).sort_values(\"trial_hash\")\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"Saved investigational drug classifications to {OUT_CSV}\")\n",
    "print(df_out.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96291d",
   "metadata": {},
   "source": [
    "#### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24ce1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 drug entries (did_*) to process\n",
      "Empty mechanism list for did=did_624e50d2bca9c6314160403c2f83bc0c, skipping\n",
      "Processed 50/129…\n",
      "Empty mechanism list for did=did_14f5219735c03c9b814c4b99a887d5f5, skipping\n",
      "Processed 100/129…\n",
      "Completed 127 drug entries with at least one PubMed hit. Files written to cache/task_3/investigational_drug_moa_pubmed_search\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PubMed search for each drug's mechanism of action (MOA), one JSON per did.\n",
    "\n",
    "Inputs (from cache/task_1/):\n",
    "- product_id_master_by_did.json\n",
    "    Keys: did_*\n",
    "    Values: {\n",
    "        \"tt_drug_id\": str,\n",
    "        \"drug_names\": [...],\n",
    "        \"alternative_names\": [...],\n",
    "        \"molecular_targets\": [...],\n",
    "        \"product_mechanisms\": [...]\n",
    "    }\n",
    "\n",
    "Process:\n",
    "- For each did_* entry:\n",
    "    • Build candidate PubMed search terms from:\n",
    "        - product_mechanisms (MOA strings; split on ';')\n",
    "        - molecular_targets\n",
    "    • Query PubMed (NCBI E-utilities) via:\n",
    "        - esearch: get top PMIDs by relevance\n",
    "        - efetch : fetch titles, abstracts, MeSH terms, and substances\n",
    "    • If no hits from raw mechanisms, use an LLM to refine MOA\n",
    "      into a more canonical search term and re-try.\n",
    "    • Aggregate all PMIDs and records per did and store:\n",
    "        - per-term search breakdown\n",
    "        - overall union of articles and first-hit term.\n",
    "\n",
    "Outputs (to cache/task_3/):\n",
    "- investigational_drug_moa_pubmed_search/{did}.json\n",
    "    {\n",
    "      \"type\": \"drug_moa_pubmed_search\",\n",
    "      \"did\": ...,\n",
    "      \"tt_drug_id\": ...,\n",
    "      \"drug_names\": [...],\n",
    "      \"alternative_names\": [...],\n",
    "      \"molecular_targets\": [...],\n",
    "      \"product_mechanisms\": [...],\n",
    "      \"mechanism_combined\": \"...\",\n",
    "      \"mechanism_key\": \"...\",\n",
    "      \"tried_terms\": [...],\n",
    "      \"llm_refined_mechanism\": \"... or null\",\n",
    "      \"mechanism_search\": { term -> {pmids, records} },\n",
    "      \"target_search\": { term -> {pmids, records} },\n",
    "      \"match\": {\n",
    "        \"term\": first_hit_term,\n",
    "        \"pmids\": [...],\n",
    "        \"records\": { pmid -> {title, abstract, mesh_terms, substances} }\n",
    "      }\n",
    "    }\n",
    "\n",
    "- investigational_drug_moa_pubmed_index.json\n",
    "    Lightweight summary index keyed by did_* with:\n",
    "        \"pmids\", \"matched_term\", \"json_path\", etc.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import html\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "import requests\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper\n",
    "\n",
    "BASE_IN_DIR = Path(\"cache/task_1\")\n",
    "BASE_OUT_DIR = Path(\"cache/task_3\")\n",
    "\n",
    "PRODUCT_MASTER_PATH = BASE_IN_DIR / \"product_id_master_by_did.json\"\n",
    "\n",
    "OUT_DIR = BASE_OUT_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MASTER_INDEX_PATH = BASE_OUT_DIR / \"investigational_drug_moa_pubmed_index.json\"\n",
    "\n",
    "EUTILS = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
    "API_KEY = os.getenv(\"NCBI_API_KEY\") or None\n",
    "EMAIL = os.getenv(\"NCBI_EMAIL\") or None\n",
    "SLEEP = 0.25\n",
    "RETRY_MAX = 3\n",
    "RETRY_WAIT = 1.0\n",
    "\n",
    "# LLM config (for MOA refinement)\n",
    "MODEL = \"gpt-5-mini\"  # adjust if needed\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "NAN_STRINGS = {\"nan\", \"none\", \"null\", \"\"}\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def _clean(s):\n",
    "    \"\"\"Normalize string; treat various 'nan' / empty-like tokens as empty.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s_str = str(s).strip()\n",
    "    return \"\" if s_str.lower() in NAN_STRINGS else s_str\n",
    "\n",
    "\n",
    "def norm_text(s: str) -> str:\n",
    "    \"\"\"Lowercase, strip, normalize Unicode and whitespace; return '' if nan-like.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = \" \".join(t.strip().lower().split())\n",
    "    return \"\" if t in NAN_STRINGS else t\n",
    "\n",
    "\n",
    "def _http_get_with_retry(url: str, params: dict, timeout: int) -> requests.Response:\n",
    "    \"\"\"HTTP GET with basic retry logic.\"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "\n",
    "def esearch_ids(term: str, n: int = 3) -> list[str]:\n",
    "    \"\"\"Run PubMed esearch for a term, return up to n PMIDs.\"\"\"\n",
    "    term = _clean(term)\n",
    "    if not term:\n",
    "        return []\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": term,\n",
    "        \"retmode\": \"json\",\n",
    "        \"retmax\": n,\n",
    "        \"sort\": \"relevance\",\n",
    "    }\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/esearch.fcgi\", params=params, timeout=30)\n",
    "    return r.json().get(\"esearchresult\", {}).get(\"idlist\", []) or []\n",
    "\n",
    "\n",
    "def _parse_xml_with_retry(text: str) -> ET.Element:\n",
    "    \"\"\"Parse XML with retry in case of transient parse errors.\"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY_MAX + 1):\n",
    "        try:\n",
    "            return ET.fromstring(text)\n",
    "        except ET.ParseError as e:\n",
    "            last_err = e\n",
    "            if attempt < RETRY_MAX:\n",
    "                time.sleep(RETRY_WAIT)\n",
    "            else:\n",
    "                raise last_err\n",
    "\n",
    "\n",
    "def efetch_details(pmids: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch article details (title, abstract, MeSH, substances) for a list of PMIDs.\n",
    "    Returns dict pmid -> {title, abstract, mesh_terms, substances}.\n",
    "    \"\"\"\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    params = {\"db\": \"pubmed\", \"id\": \",\".join(pmids), \"retmode\": \"xml\"}\n",
    "    if API_KEY:\n",
    "        params[\"api_key\"] = API_KEY\n",
    "    if EMAIL:\n",
    "        params[\"email\"] = EMAIL\n",
    "    r = _http_get_with_retry(f\"{EUTILS}/efetch.fcgi\", params=params, timeout=60)\n",
    "    root = _parse_xml_with_retry(r.text)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    def text_from_el(el):\n",
    "        return \"\".join(el.itertext()).strip() if el is not None else \"\"\n",
    "\n",
    "    def join_abstract(abs_parent):\n",
    "        parts = []\n",
    "        for t in abs_parent.findall(\"AbstractText\"):\n",
    "            label = t.attrib.get(\"Label\")\n",
    "            txt = text_from_el(t)\n",
    "            if txt:\n",
    "                parts.append(f\"{label}: {txt}\" if label else txt)\n",
    "        return \"\\n\".join(parts).strip()\n",
    "\n",
    "    for art in root.findall(\".//PubmedArticle\"):\n",
    "        pmid_el = art.find(\".//MedlineCitation/PMID\")\n",
    "        if pmid_el is None or not (pmid_el.text or \"\").strip():\n",
    "            continue\n",
    "        pmid = pmid_el.text.strip()\n",
    "\n",
    "        title = text_from_el(art.find(\".//Article/ArticleTitle\"))\n",
    "        abs_parent = art.find(\".//Article/Abstract\")\n",
    "        abstract = join_abstract(abs_parent) if abs_parent is not None else \"\"\n",
    "\n",
    "        mesh_terms = []\n",
    "        for mh in art.findall(\".//MedlineCitation/MeshHeadingList/MeshHeading\"):\n",
    "            desc = mh.find(\"DescriptorName\")\n",
    "            if desc is None or not (desc.text or \"\").strip():\n",
    "                continue\n",
    "            d_text = desc.text.strip()\n",
    "            d_major = desc.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "            d_str = f\"{d_text}{'*' if d_major else ''}\"\n",
    "\n",
    "            quals = []\n",
    "            for q in mh.findall(\"QualifierName\"):\n",
    "                q_text = (q.text or \"\").strip()\n",
    "                if q_text:\n",
    "                    q_major = q.attrib.get(\"MajorTopicYN\") == \"Y\"\n",
    "                    quals.append(f\"{q_text}{'*' if q_major else ''}\")\n",
    "\n",
    "            mesh_terms.append(d_str if not quals else d_str + \" / \" + \"; \".join(quals))\n",
    "\n",
    "        substances = []\n",
    "        for chem in art.findall(\".//Chemical\"):\n",
    "            nm_el = chem.find(\"NameOfSubstance\")\n",
    "            rn_el = chem.find(\"RegistryNumber\")\n",
    "            nm = nm_el.text.strip() if nm_el is not None else \"\"\n",
    "            rn = rn_el.text.strip() if rn_el is not None else \"\"\n",
    "            if nm and rn and rn != \"0\":\n",
    "                substances.append(f\"{nm} [RN:{rn}]\")\n",
    "            elif nm:\n",
    "                substances.append(nm)\n",
    "            elif rn and rn != \"0\":\n",
    "                substances.append(f\"[RN:{rn}]\")\n",
    "\n",
    "        def uniq(xs):\n",
    "            seen, out_local = set(), []\n",
    "            for x in xs:\n",
    "                if x and x not in seen:\n",
    "                    seen.add(x)\n",
    "                    out_local.append(x)\n",
    "            return out_local\n",
    "\n",
    "        out[pmid] = {\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"mesh_terms\": uniq(mesh_terms),\n",
    "            \"substances\": uniq(substances),\n",
    "        }\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    \"\"\"Write JSON to disk with UTF-8 + pretty indent.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_json_or_empty(path: Path) -> dict:\n",
    "    \"\"\"Load JSON or return empty dict on failure / missing file.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def split_terms(s: str):\n",
    "    \"\"\"\n",
    "    For MOA strings like:\n",
    "      'Thrombopoietin receptor agonist (recombinant growth factor); PEGylated recombinant human EPO'\n",
    "    split on ';' and treat each piece as a candidate search term.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    raw = [t.strip() for t in str(s).split(\";\")]\n",
    "    return [t for t in raw if t and t.lower() not in NAN_STRINGS]\n",
    "\n",
    "\n",
    "# --------------- LLM refinement helpers ---------------\n",
    "def build_moa_refinement_prompt(mechanism: str) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the chatbot to turn a free-text MOA into a concise, canonical\n",
    "    mechanism-of-action phrase suitable for PubMed search.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an expert clinical pharmacologist and mechanisms-of-action classifier.\n",
    "\n",
    "Given the following mechanism-of-action (MOA) description from a drug development database:\n",
    "\n",
    "\\\"\\\"\\\"{mechanism}\\\"\\\"\\\"\n",
    "\n",
    "Rewrite or condense it into a SHORT, CANONICAL mechanism-of-action term that would work well as a PubMed search term.\n",
    "\n",
    "Rules:\n",
    "- Output a concise mechanism class or well-recognized pharmacologic concept, not a full sentence.\n",
    "- Prefer standard pharmacologic/mechanistic classes (e.g. \"Ion Exchange Resins\", \"Immunocytokines\",\n",
    "  \"Kinase Inhibitors\", \"Antibodies, Monoclonal\", \"Immune Checkpoint Inhibitors\").\n",
    "- Do NOT include long target listings or extra explanation.\n",
    "- If the original MOA is already an appropriate concise search term, you may return it unchanged.\n",
    "\n",
    "Return ONLY the refined mechanism phrase, with no additional explanation or formatting.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def refine_mechanism_with_llm(mechanism: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Use the OpenAIWrapper .query() interface to get a refined mechanism phrase.\n",
    "    Returns the refined phrase or None on failure.\n",
    "    \"\"\"\n",
    "    mech_clean = _clean(mechanism)\n",
    "    if not mech_clean:\n",
    "        return None\n",
    "\n",
    "    prompt = build_moa_refinement_prompt(mech_clean)\n",
    "\n",
    "    try:\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        text = (res.get(\"text_response\") or \"\").strip()\n",
    "        # Strip surrounding quotes if the model adds them\n",
    "        text = text.strip().strip('\"').strip(\"'\")\n",
    "        refined = _clean(text)\n",
    "        return refined or None\n",
    "    except Exception as e:\n",
    "        print(f\"LLM refinement failed for mechanism='{mech_clean[:80]}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "# Load product master (by did) from task_1\n",
    "product_master_by_did = load_json_or_empty(PRODUCT_MASTER_PATH)\n",
    "if not product_master_by_did:\n",
    "    raise RuntimeError(f\"No product entries found in {PRODUCT_MASTER_PATH}\")\n",
    "\n",
    "# Load existing PubMed index (from task_3)\n",
    "master_index = load_json_or_empty(MASTER_INDEX_PATH) or {}\n",
    "\n",
    "total = len(product_master_by_did)\n",
    "print(f\"{total} drug entries (did_*) to process\")\n",
    "processed = 0\n",
    "\n",
    "# Main loop: one PubMed search per DRUG (by did)\n",
    "for did, rec in product_master_by_did.items():\n",
    "    # Skip if already indexed\n",
    "    if did in master_index:\n",
    "        mech_list = rec.get(\"product_mechanisms\", []) or []\n",
    "        mech_preview = \"; \".join(mech_list)[:60]\n",
    "        print(f\"{mech_preview} || already processed for {did}\")\n",
    "        processed += 1\n",
    "        continue\n",
    "\n",
    "    # product_mechanisms is a list; join into a single string for a \"combo\" key,\n",
    "    # but we'll search EACH mechanism (and its ';'-split pieces) separately.\n",
    "    mech_list = rec.get(\"product_mechanisms\", []) or []\n",
    "    mechanism = _clean(\"; \".join(mech_list))\n",
    "    if not mechanism:\n",
    "        print(f\"Empty mechanism list for did={did}, skipping\")\n",
    "        continue\n",
    "\n",
    "    mech_key = norm_text(mechanism)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build candidate search terms\n",
    "    # -----------------------------\n",
    "    # For MOAs: search EACH mechanism string (and each ';'-split subterm).\n",
    "    mechanism_terms: list[str] = []\n",
    "    for mech in mech_list:\n",
    "        mech = _clean(mech)\n",
    "        if not mech:\n",
    "            continue\n",
    "        subterms = split_terms(mech)\n",
    "        if not subterms:\n",
    "            subterms = [mech]\n",
    "        for t in subterms:\n",
    "            t_clean = _clean(t)\n",
    "            if t_clean and t_clean not in mechanism_terms:\n",
    "                mechanism_terms.append(t_clean)\n",
    "\n",
    "    # For molecular targets: direct terms\n",
    "    target_terms: list[str] = []\n",
    "    for tgt in rec.get(\"molecular_targets\", []) or []:\n",
    "        t_clean = _clean(tgt)\n",
    "        if t_clean and t_clean not in target_terms:\n",
    "            target_terms.append(t_clean)\n",
    "\n",
    "    if not mechanism_terms and not target_terms:\n",
    "        print(f\"No usable mechanism or target terms for did={did}, skipping\")\n",
    "        continue\n",
    "\n",
    "    tried_terms: list[str] = []\n",
    "    first_hit_term: str | None = None\n",
    "    llm_refined: str | None = None\n",
    "\n",
    "    # Detailed per-term results\n",
    "    mechanism_search: dict[str, dict] = {}\n",
    "    target_search: dict[str, dict] = {}\n",
    "\n",
    "    # Aggregate across all searches for summary\n",
    "    all_pmids: set[str] = set()\n",
    "    all_records: dict[str, dict] = {}\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Mechanism term searches\n",
    "    # -----------------------------\n",
    "    for term in mechanism_terms:\n",
    "        tried_terms.append(term)\n",
    "        query = f\"\\\"{term}\\\"\"\n",
    "        try:\n",
    "            pmids = esearch_ids(query, n=5)\n",
    "        except Exception:\n",
    "            pmids = []\n",
    "\n",
    "        records = {}\n",
    "        if pmids:\n",
    "            try:\n",
    "                records = efetch_details(pmids)\n",
    "            except Exception as e:\n",
    "                records = {\"_error\": str(e)}\n",
    "\n",
    "            # Track first term that hits\n",
    "            if first_hit_term is None and pmids:\n",
    "                first_hit_term = term\n",
    "\n",
    "            for p in pmids:\n",
    "                all_pmids.add(p)\n",
    "                if p not in all_records and p in records:\n",
    "                    all_records[p] = records[p]\n",
    "\n",
    "        mechanism_search[term] = {\n",
    "            \"pmids\": pmids,\n",
    "            \"records\": records,\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) LLM refinement if NO mechanism hits\n",
    "    # -----------------------------\n",
    "    if not all_pmids:\n",
    "        llm_refined = refine_mechanism_with_llm(mechanism)\n",
    "        if llm_refined:\n",
    "            llm_term_key = llm_refined  # store as-is\n",
    "            tried_terms.append(llm_refined + \" [LLM]\")\n",
    "            query = f\"\\\"{llm_refined}\\\"\"\n",
    "            try:\n",
    "                pmids = esearch_ids(query, n=5)\n",
    "            except Exception:\n",
    "                pmids = []\n",
    "\n",
    "            records = {}\n",
    "            if pmids:\n",
    "                try:\n",
    "                    records = efetch_details(pmids)\n",
    "                except Exception as e:\n",
    "                    records = {\"_error\": str(e)}\n",
    "\n",
    "                if first_hit_term is None and pmids:\n",
    "                    first_hit_term = llm_refined\n",
    "\n",
    "                for p in pmids:\n",
    "                    all_pmids.add(p)\n",
    "                    if p not in all_records and p in records:\n",
    "                        all_records[p] = records[p]\n",
    "\n",
    "            mechanism_search[llm_term_key] = {\n",
    "                \"pmids\": pmids,\n",
    "                \"records\": records,\n",
    "                \"llm_refined\": True,\n",
    "            }\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Molecular target term searches (10 PMIDs each)\n",
    "    # -----------------------------\n",
    "    for term in target_terms:\n",
    "        tried_terms.append(term)\n",
    "        query = f\"\\\"{term}\\\"\"\n",
    "        try:\n",
    "            pmids = esearch_ids(query, n=10)  # ← 10 studies per target term\n",
    "        except Exception:\n",
    "            pmids = []\n",
    "\n",
    "        records = {}\n",
    "        if pmids:\n",
    "            try:\n",
    "                records = efetch_details(pmids)\n",
    "            except Exception as e:\n",
    "                records = {\"_error\": str(e)}\n",
    "\n",
    "            if first_hit_term is None and pmids:\n",
    "                first_hit_term = term\n",
    "\n",
    "            for p in pmids:\n",
    "                all_pmids.add(p)\n",
    "                if p not in all_records and p in records:\n",
    "                    all_records[p] = records[p]\n",
    "\n",
    "        target_search[term] = {\n",
    "            \"pmids\": pmids,\n",
    "            \"records\": records,\n",
    "        }\n",
    "\n",
    "    # 4) If STILL no PMIDs at all, skip saving (so you can rerun later)\n",
    "    if not all_pmids:\n",
    "        print(f\"No PubMed hits for did={did} after mechanisms + targets + LLM, skipping\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # HASH-BASED OUTPUT (BY did)\n",
    "    # -----------------------------\n",
    "    fname = f\"{did}.json\"\n",
    "    out_path = OUT_DIR / fname\n",
    "\n",
    "    payload = {\n",
    "        \"type\": \"drug_moa_pubmed_search\",\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": rec.get(\"tt_drug_id\"),\n",
    "        \"drug_names\": rec.get(\"drug_names\", []),\n",
    "        \"alternative_names\": rec.get(\"alternative_names\", []),\n",
    "        \"molecular_targets\": rec.get(\"molecular_targets\", []),\n",
    "        \"product_mechanisms\": mech_list,\n",
    "        \"mechanism_combined\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"tried_terms\": tried_terms,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "        # Detailed breakdowns:\n",
    "        \"mechanism_search\": mechanism_search,\n",
    "        \"target_search\": target_search,\n",
    "        # Backward-compatible summary:\n",
    "        \"match\": {\n",
    "            \"term\": first_hit_term,\n",
    "            \"pmids\": sorted(all_pmids),\n",
    "            \"records\": all_records,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    save_json(out_path, payload)\n",
    "\n",
    "    # Index entry keyed by did (summary only)\n",
    "    master_index[did] = {\n",
    "        \"did\": did,\n",
    "        \"tt_drug_id\": rec.get(\"tt_drug_id\"),\n",
    "        \"drug_names\": rec.get(\"drug_names\", []),\n",
    "        \"product_mechanisms\": mech_list,\n",
    "        \"mechanism_combined\": mechanism,\n",
    "        \"mechanism_key\": mech_key,\n",
    "        \"json_path\": f\"{OUT_DIR.name}/{fname}\",\n",
    "        \"pmids\": sorted(all_pmids),\n",
    "        \"matched_term\": first_hit_term,\n",
    "        \"llm_refined_mechanism\": llm_refined,\n",
    "    }\n",
    "    save_json(MASTER_INDEX_PATH, master_index)\n",
    "\n",
    "    processed += 1\n",
    "    if processed % 50 == 0:\n",
    "        print(f\"Processed {processed}/{total}…\")\n",
    "\n",
    "    time.sleep(SLEEP)\n",
    "\n",
    "save_json(MASTER_INDEX_PATH, master_index)\n",
    "print(\n",
    "    f\"Completed {processed} drug entries with at least one PubMed hit. \"\n",
    "    f\"Files written to {OUT_DIR}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66fdab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 127 MOA PubMed-search files for MeSH-term selection.\n",
      "Progress: processed 50 MOA entries...\n",
      "Progress: processed 100 MOA entries...\n",
      "MOA MeSH-term selection complete. processed=127, skipped=0, llm_error=0, parse_error=0, coverage_error=0, no_candidates=0\n",
      "Chosen MOA directory: cache/task_3/investigational_drug_moa_chosen\n",
      "Log directory:        cache/task_3/investigational_drug_moa_chosen_log\n",
      "Master choices:       cache/task_3/investigational_drug_moa_chosen_master.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Select a single, mechanistically appropriate MeSH term for each drug MOA, using PubMed search results.\n",
    "\n",
    "Inputs (from cache/task_3/):\n",
    "- investigational_drug_moa_pubmed_search/{did}.json\n",
    "    Each JSON (one per did_*) should contain:\n",
    "        • did / moa_id\n",
    "        • product_mechanisms (list of MOA strings)\n",
    "        • mechanism_combined (joined MOA string)\n",
    "        • tried_terms (list of PubMed search terms used)\n",
    "        • match:\n",
    "            - pmids: list of PubMed IDs\n",
    "            - records: {\n",
    "                  pmid: {\n",
    "                      \"title\": str,\n",
    "                      \"abstract\": str,\n",
    "                      \"mesh_terms\": [list of MeSH descriptor/qualifier strings],\n",
    "                      \"substances\": [optional list of chemical/substance labels]\n",
    "                  },\n",
    "                  ...\n",
    "              }\n",
    "\n",
    "Process:\n",
    "- For each MOA PubMed-search JSON:\n",
    "    • Derive a moa_id (prefer payload[\"moa_id\"], then payload[\"did\"], then filename stem).\n",
    "    • Collect *candidate* MeSH terms by unioning all `mesh_terms` across the matched PMIDs.\n",
    "    • If there are no candidate MeSH terms, skip that MOA.\n",
    "    • Build a condensed payload (mechanism text, tried terms, pmids, and titles + mesh_terms only).\n",
    "    • Call the LLM with a constrained prompt that:\n",
    "        - Chooses EXACTLY one MeSH term from the candidate list, OR\n",
    "        - Returns \"[none]\" if no term meaningfully represents the MOA.\n",
    "    • Enforce that any non-\"[none]\" choice must be exactly in the candidate list.\n",
    "    • Persist:\n",
    "        - Per-MOA choice to investigational_drug_moa_chosen/{moa_id}.json\n",
    "        - A log entry with prompt + raw response to investigational_drug_moa_chosen_log/{moa_id}.json\n",
    "        - A master index of all MOA choices in investigational_drug_moa_chosen_master.json\n",
    "\n",
    "Outputs (to cache/task_3/):\n",
    "- investigational_drug_moa_chosen/{moa_id}.json\n",
    "    {\n",
    "      \"moa_id\": \"...\",\n",
    "      \"mechanism\": \"...\",\n",
    "      \"candidate_mesh_terms\": [...],\n",
    "      \"chosen_mesh_term\": \"<one term from candidates or '[none]'>\",\n",
    "      \"source_pmid\": \"<pmid or null>\",\n",
    "      \"rationale\": \"One concise sentence\",\n",
    "      \"source\": \"llm\"\n",
    "    }\n",
    "\n",
    "- investigational_drug_moa_chosen_log/{moa_id}.json\n",
    "    (debug/log record with prompt, raw_response, timing, and cost)\n",
    "\n",
    "- investigational_drug_moa_chosen_master.json\n",
    "    {\n",
    "      \"<moa_id>\": { ...same structure as per-MOA JSON... },\n",
    "      ...\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from services.openai_wrapper import OpenAIWrapper  # your wrapper\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "\n",
    "MOA_PUBMED_DIR          = BASE_DIR / \"investigational_drug_moa_pubmed_search\"\n",
    "MOA_CHOICE_DIR          = BASE_DIR / \"investigational_drug_moa_chosen\"\n",
    "MOA_CHOICE_LOG_DIR      = BASE_DIR / \"investigational_drug_moa_chosen_log\"\n",
    "MASTER_MOA_CHOICES_PATH = BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "\n",
    "MOA_CHOICE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MOA_CHOICE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_WORKERS_MOA = 8\n",
    "MODEL = \"gpt-5\"\n",
    "\n",
    "client = OpenAIWrapper()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def extract_json_object(text: str) -> dict:\n",
    "    \"\"\"Extract first valid JSON object from model output.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return {}\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return {}\n",
    "\n",
    "    # Direct parse first\n",
    "    try:\n",
    "        obj = json.loads(text)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: first {...} region\n",
    "    m = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    try:\n",
    "        obj = json.loads(m.group(0))\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "    return {}\n",
    "\n",
    "def load_master_moa_choices() -> dict:\n",
    "    \"\"\"Load the existing master MOA → MeSH choice index, or {} if missing/invalid.\"\"\"\n",
    "    if not MASTER_MOA_CHOICES_PATH.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(MASTER_MOA_CHOICES_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def build_moa_mesh_prompt(moa_payload: dict, candidate_mesh_terms: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Prompt the LLM to choose the best MeSH term that represents the mechanism of action.\n",
    "    If no suitable MeSH term exists, the model MUST return \"[none]\".\n",
    "    \"\"\"\n",
    "    payload_json = json.dumps(moa_payload, ensure_ascii=False, indent=2)\n",
    "    mesh_json    = json.dumps(candidate_mesh_terms, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert pharmacologist and MeSH annotation specialist.\n",
    "\n",
    "You are given:\n",
    "1) A mechanism-of-action (MOA) text string describing how a drug works.\n",
    "2) A set of PubMed-derived MeSH terms (candidate list).\n",
    "3) Condensed PubMed records used for MOA search.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 1 — Select the Best MeSH Term\n",
    "------------------------------------------------------------\n",
    "Choose EXACTLY ONE MeSH term that best represents the mechanism of action.\n",
    "\n",
    "Rules:\n",
    "- You MUST select a term *only* from the candidate list.\n",
    "- Choose the most mechanistic/specific pharmacologic concept available\n",
    "  (e.g., \"Receptor Antagonists\", \"Antibodies, Monoclonal\", \"Kinase Inhibitors\").\n",
    "- Avoid generic terms (\"Humans\", \"Adult\", \"Neoplasms\") unless absolutely no mechanistic term exists.\n",
    "\n",
    "------------------------------------------------------------\n",
    "TASK 2 — Handle Cases with No Good Mechanistic Term\n",
    "------------------------------------------------------------\n",
    "If NONE of the candidate MeSH terms meaningfully represent the MOA:\n",
    "\n",
    "You MUST output:\n",
    "\n",
    "  \"chosen_mesh_term\": \"[none]\",\n",
    "  \"source_pmid\": null,\n",
    "  \"rationale\": \"Explain why no term fits.\"\n",
    "\n",
    "This is a VALID and EXPECTED outcome.\n",
    "\n",
    "------------------------------------------------------------\n",
    "OUTPUT FORMAT  (STRICT)\n",
    "------------------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object with EXACTLY these fields:\n",
    "\n",
    "{{\n",
    "  \"chosen_mesh_term\": \"<one exact candidate term OR '[none]'>\",\n",
    "  \"source_pmid\": \"<PMID you relied on OR null>\",\n",
    "  \"rationale\": \"One concise sentence explaining your decision.\"\n",
    "}}\n",
    "\n",
    "Constraints:\n",
    "- If you choose a MeSH term, it MUST MATCH EXACTLY one item from the candidate list.\n",
    "- If no suitable term exists, return \"[none]\".\n",
    "- JSON must be valid and parseable.\n",
    "\n",
    "------------------------------------------------------------\n",
    "MOA Payload (input data)\n",
    "------------------------------------------------------------\n",
    "{payload_json}\n",
    "\n",
    "------------------------------------------------------------\n",
    "Candidate MeSH Terms\n",
    "------------------------------------------------------------\n",
    "{mesh_json}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "master_moa_choices = load_master_moa_choices()\n",
    "master_moa_lock = threading.Lock()\n",
    "\n",
    "moa_counter = {\n",
    "    \"processed\": 0,\n",
    "    \"skipped_existing\": 0,\n",
    "    \"llm_error\": 0,\n",
    "    \"parse_error\": 0,\n",
    "    \"coverage_error\": 0,   # includes \"chosen term not in JSON-derived list\"\n",
    "    \"no_candidates\": 0,\n",
    "}\n",
    "moa_counter_lock = threading.Lock()\n",
    "\n",
    "\n",
    "def process_moa_file(fp: Path, idx: int, total: int) -> None:\n",
    "    \"\"\"Process a single MOA PubMed-search JSON file and select a MeSH term via LLM.\"\"\"\n",
    "    try:\n",
    "        payload = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}/{total}] Error reading {fp.name}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    moa_id = (\n",
    "        payload.get(\"moa_id\")\n",
    "        or payload.get(\"did\")   # backward-compat for did_* key\n",
    "        or fp.stem\n",
    "    )\n",
    "\n",
    "    mechanism = (\n",
    "        payload.get(\"mechanism\")        # if already present\n",
    "        or payload.get(\"mechanism_combined\")\n",
    "        or \"; \".join(payload.get(\"product_mechanisms\", []) or [])\n",
    "        or \"\"\n",
    "    )\n",
    "\n",
    "    if not moa_id:\n",
    "        print(f\"[{idx}/{total}] Missing moa_id in {fp.name}, skipping\")\n",
    "        return\n",
    "\n",
    "    out_fp = MOA_CHOICE_DIR / f\"{moa_id}.json\"\n",
    "    if out_fp.exists():\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"skipped_existing\"] += 1\n",
    "        return\n",
    "\n",
    "    match = payload.get(\"match\") or {}\n",
    "    records = match.get(\"records\") or {}\n",
    "    pmids = match.get(\"pmids\") or []\n",
    "\n",
    "    # Collect candidate MeSH terms (unique, in stable order) FROM THE JSON ONLY\n",
    "    candidate_terms = []\n",
    "    seen_terms = set()\n",
    "    for pmid, rec in records.items():\n",
    "        mesh_terms = rec.get(\"mesh_terms\") or []\n",
    "        for term in mesh_terms:\n",
    "            if term and term not in seen_terms:\n",
    "                seen_terms.add(term)\n",
    "                candidate_terms.append(term)\n",
    "\n",
    "    if not candidate_terms:\n",
    "        print(f\"[{idx}/{total}] No candidate MeSH terms for moa_id={moa_id}, skipping\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"no_candidates\"] += 1\n",
    "        return\n",
    "\n",
    "    # Condensed payload for the model (avoid full abstracts to save tokens)\n",
    "    condensed_records = {\n",
    "        pmid: {\n",
    "            \"title\": (rec.get(\"title\") or \"\"),\n",
    "            \"mesh_terms\": (rec.get(\"mesh_terms\") or []),\n",
    "        }\n",
    "        for pmid, rec in records.items()\n",
    "    }\n",
    "\n",
    "    moa_payload = {\n",
    "        \"moa_id\": moa_id,\n",
    "        \"mechanism\": mechanism,\n",
    "        \"tried_terms\": payload.get(\"tried_terms\") or [],\n",
    "        \"pmids\": pmids,\n",
    "        \"records\": condensed_records,\n",
    "    }\n",
    "\n",
    "    prompt = build_moa_mesh_prompt(moa_payload, candidate_terms)\n",
    "\n",
    "    token = moa_id\n",
    "    hash_id = moa_id\n",
    "\n",
    "    text_response = \"\"\n",
    "    raw_response = None\n",
    "    total_cost = 0.0\n",
    "    elapsed = 0.0\n",
    "\n",
    "    # Call LLM\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        res = client.query(prompt=prompt, model=MODEL)\n",
    "        elapsed = round(time.perf_counter() - t0, 2)\n",
    "\n",
    "        text_response = (res.get(\"text_response\") or \"\").strip()\n",
    "        raw_response = res.get(\"raw_response\")\n",
    "        total_cost = float(res.get(\"cost\") or 0.0)\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}/{total}] LLM error for moa_id={moa_id}: {e}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"llm_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Parse JSON\n",
    "    obj = extract_json_object(text_response)\n",
    "\n",
    "    if not isinstance(obj, dict) or not obj:\n",
    "        print(f\"[{idx}/{total}] JSON parse error moa_id={moa_id}, raw={text_response!r}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"parse_error\"] += 1\n",
    "        return\n",
    "\n",
    "    chosen_term = obj.get(\"chosen_mesh_term\")\n",
    "    source_pmid = obj.get(\"source_pmid\")\n",
    "    rationale = obj.get(\"rationale\")\n",
    "\n",
    "    # HARD CHECK: chosen term\n",
    "    if not chosen_term or not isinstance(chosen_term, str):\n",
    "        print(f\"[{idx}/{total}] Missing or invalid chosen_mesh_term for moa_id={moa_id}\")\n",
    "        with moa_counter_lock:\n",
    "            moa_counter[\"coverage_error\"] += 1\n",
    "        return\n",
    "\n",
    "    # Special allowed sentinel for \"no good term\"\n",
    "    if chosen_term == \"[none]\":\n",
    "        # Accept even though it's not in candidate_terms\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "    else:\n",
    "        # For any real term, it MUST come from the JSON-derived candidate list\n",
    "        if chosen_term not in candidate_terms:\n",
    "            # DNE in JSON (hallucinated or modified term) → reject, do NOT save\n",
    "            print(\n",
    "                f\"[{idx}/{total}] chosen_mesh_term not in JSON-derived candidate list \"\n",
    "                f\"for moa_id={moa_id}: {chosen_term!r}\"\n",
    "            )\n",
    "            with moa_counter_lock:\n",
    "                moa_counter[\"coverage_error\"] += 1\n",
    "            return\n",
    "\n",
    "        # Optional: source_pmid sanity check (must be one of pmids or None)\n",
    "        if source_pmid is not None and source_pmid not in pmids:\n",
    "            print(\n",
    "                f\"[{idx}/{total}] source_pmid {source_pmid!r} not in pmids for moa_id={moa_id}; \"\n",
    "                f\"still accepting chosen_mesh_term\"\n",
    "            )\n",
    "\n",
    "        mapped = {\n",
    "            \"moa_id\": moa_id,\n",
    "            \"mechanism\": mechanism,\n",
    "            \"candidate_mesh_terms\": candidate_terms,\n",
    "            \"chosen_mesh_term\": chosen_term,\n",
    "            \"source_pmid\": source_pmid,\n",
    "            \"rationale\": rationale,\n",
    "            \"source\": \"llm\",\n",
    "        }\n",
    "\n",
    "    # Save per-MOA JSON\n",
    "    out_fp.write_text(json.dumps(mapped, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Log entry\n",
    "    log_payload = {\n",
    "        \"token\": token,\n",
    "        \"hash_id\": hash_id,\n",
    "        \"model\": MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"structured_response\": json.dumps(mapped, ensure_ascii=False, indent=2),\n",
    "        \"raw_response\": repr(raw_response),\n",
    "        \"total_cost\": total_cost,\n",
    "        \"time_elapsed\": elapsed,\n",
    "    }\n",
    "    (MOA_CHOICE_LOG_DIR / f\"{hash_id}.json\").write_text(\n",
    "        json.dumps(log_payload, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # Update master\n",
    "    with master_moa_lock:\n",
    "        master_moa_choices[moa_id] = mapped\n",
    "        MASTER_MOA_CHOICES_PATH.write_text(\n",
    "            json.dumps(master_moa_choices, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "\n",
    "    with moa_counter_lock:\n",
    "        moa_counter[\"processed\"] += 1\n",
    "        if moa_counter[\"processed\"] % 50 == 0:\n",
    "            print(f\"Progress: processed {moa_counter['processed']} MOA entries...\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "moa_files = sorted(MOA_PUBMED_DIR.glob(\"*.json\"))\n",
    "total_moa = len(moa_files)\n",
    "print(f\"Loaded {total_moa} MOA PubMed-search files for MeSH-term selection.\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS_MOA) as ex:\n",
    "    futures = {\n",
    "        ex.submit(process_moa_file, fp, idx, total_moa): fp.name\n",
    "        for idx, fp in enumerate(moa_files, start=1)\n",
    "    }\n",
    "    for fut in as_completed(futures):\n",
    "        name = futures[fut]\n",
    "        try:\n",
    "            fut.result()\n",
    "        except Exception as e:\n",
    "            print(f\"Worker error (MOA MeSH selection) file={name}: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"MOA MeSH-term selection complete. \"\n",
    "    f\"processed={moa_counter['processed']}, \"\n",
    "    f\"skipped={moa_counter['skipped_existing']}, \"\n",
    "    f\"llm_error={moa_counter['llm_error']}, \"\n",
    "    f\"parse_error={moa_counter['parse_error']}, \"\n",
    "    f\"coverage_error={moa_counter['coverage_error']}, \"\n",
    "    f\"no_candidates={moa_counter['no_candidates']}\"\n",
    ")\n",
    "print(f\"Chosen MOA directory: {MOA_CHOICE_DIR}\")\n",
    "print(f\"Log directory:        {MOA_CHOICE_LOG_DIR}\")\n",
    "print(f\"Master choices:       {MASTER_MOA_CHOICES_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25b762f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trial breakdown: cache/task_1/trial_product_breakdown.csv, shape=(184, 23)\n",
      "did_to_mesh (non-[none]) count: 126\n",
      "trial_to_dids entries: 183\n",
      "did_to_names_norm entries: 126\n",
      "✔️ Wrote trial breakdown with chosen MeSH mechanisms → cache/task_3/trial_product_breakdown_w_chosen_mechanisms.csv\n",
      "| trial_hash                           | investigational_products                           | investigational_products_mechanism_mesh_terms                        | investigational_products_alternative_names                                                                                                                                                                                                          | investigational_products_molecular_target   | investigational_products_mechanism                                         | investigational_products_tt_drug_id   | investigational_products_bmt_drug_id   | active_comparators   | active_comparators_mechanism_mesh_terms   | active_comparators_alternative_names   | active_comparators_molecular_target   | active_comparators_mechanism   | active_comparators_tt_drug_id   | active_comparators_bmt_drug_id   | placebos   | placebos_alternative_names   | placebos_molecular_target   | placebos_mechanism   | standard_of_care   | standard_of_care_mechanism_mesh_terms   | standard_of_care_alternative_names   | standard_of_care_molecular_target   | standard_of_care_mechanism   | standard_of_care_tt_drug_id   | standard_of_care_bmt_drug_id   |\n",
      "|:-------------------------------------|:---------------------------------------------------|:---------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:---------------------------------------------------------------------------|:--------------------------------------|:---------------------------------------|:---------------------|:------------------------------------------|:---------------------------------------|:--------------------------------------|:-------------------------------|:--------------------------------|:---------------------------------|:-----------|:-----------------------------|:----------------------------|:---------------------|:-------------------|:----------------------------------------|:-------------------------------------|:------------------------------------|:-----------------------------|:------------------------------|:-------------------------------|\n",
      "| tid_0541995757b10e613a42173d6b8ddc09 | ['cinacalcet hydrochloride']                       | ['Receptors, Calcium-Sensing / agonists']                            | [['cinacalcet HCl', 'cinacalcet', 'cinacalcet HCl, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride tablet']] | ['Calcium-sensing receptor (CaSR)']         | ['Calcimimetic; positive allosteric modulator of CaSR']                    | ['194454']                            | ['']                                   | []                   | []                                        | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0d6e9b2f3f57c17c0e93610e28853f0c | ['Xenopax']                                        | ['Interleukin-2 Receptor alpha Subunit* / metabolism; immunology']   | [['daclizumab', 'daclizumab, Shanghai CP Guojian', 'transplantation MAb, Shanghai', 'recombinant anti-CD25 humanized monoclonal antibody injection']]                                                                                               | ['CD25 (IL2RA)']                            | ['Humanized monoclonal antibody; interleukin-2 receptor alpha antagonist'] | ['40098']                             | ['']                                   | []                   | []                                        | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0da20e863cfc5f3e369868462bff74e0 | ['recombinant erythropoiesis-stimulating protein'] | ['Receptors, Erythropoietin / agonists*']                            | [['NuPIAO', 'nupiao', 'rESP', 'recombinant erythropoietin stimulating protein', 'recombinant erythropoietin stimulating protein, 3SBio', 'recombinant erythropoiesis-stimulating protein injection (CHO cells)', 'SSS-06', 'SSS06', 'SSS 06']]      | ['Erythropoietin receptor (EPOR)']          | ['Erythropoietin receptor agonist']                                        | ['40640']                             | ['19694']                              | []                   | []                                        | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e698ee5065c49d23fcf57516957a273 | ['SB8']                                            | ['Vascular Endothelial Growth Factor A / antagonists & inhibitors*'] | [['SB 8', 'SB-8', 'Aybintio', 'Onbevzi', 'bevacizumab, Samsung Bioepis', '615', 'SB8 Anti-VEGF antibody']]                                                                                                                                          | ['VEGF-A']                                  | ['Anti-VEGF-A monoclonal antibody (angiogenesis inhibitor)']               | ['113881']                            | ['27338']                              | []                   | []                                        | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e8fa21079f928135dfc6164a15285f8 | ['SSS-17']                                         | ['Prolyl-Hydroxylase Inhibitors / pharmacology*']                    | [['SSS17', 'SSS 17', '[14C] SSS17', '[14C]-SSS17', '[14C]SSS17', 'HIF 117', 'HIF-117', 'HIF117', '[14C]HIF-117']]                                                                                                                                   | ['HIF']                                     | ['Hypoxia-inducible factor antagonist']                                    | ['130313']                            | ['']                                   | []                   | []                                        | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                   | []                                  | []                           | []                            | []                             |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attach chosen MeSH mechanism terms to the per-trial product breakdown table.\n",
    "\n",
    "What is a \"did\"?\n",
    "- A \"did\" is a hashed drug identifier string like \"did_2c2fb9efd4b8a1f837bf47004a49ce45\".\n",
    "- Each did represents one drug, aggregating all known names and metadata for that product.\n",
    "- did records are stored in:\n",
    "      cache/task_1/product_id_master_by_did.json\n",
    "  For example:\n",
    "      \"did_2c2fb9efd4b8a1f837bf47004a49ce45\": {\n",
    "        \"did\": \"did_2c2fb9efd4b8a1f837bf47004a49ce45\",\n",
    "        \"tt_drug_id\": \"9084\",\n",
    "        \"drug_names\": [\"ifosfamide\"],\n",
    "        \"alternative_names\": [\"Holoxan\", \"Ifex\", \"iphosphamide\", \"isophosphamide\"],\n",
    "        \"molecular_targets\": [],\n",
    "        \"product_mechanisms\": [\"Alkylating agent (DNA crosslinker)\"],\n",
    "        \"trial_hashes\": [\"tid_bb1e0571142dde8a49976632c349593c\"]\n",
    "      }\n",
    "\n",
    "Inputs:\n",
    "- cache/task_1/trial_product_breakdown.csv\n",
    "    Per-trial product lists, including:\n",
    "      • investigational_products\n",
    "      • active_comparators\n",
    "      • standard_of_care\n",
    "- cache/task_1/product_id_master_by_did.json\n",
    "    did → {\n",
    "      \"did\",\n",
    "      \"tt_drug_id\",\n",
    "      \"drug_names\",\n",
    "      \"alternative_names\",\n",
    "      \"product_mechanisms\",\n",
    "      \"trial_hashes\",\n",
    "      ...\n",
    "    }\n",
    "- cache/task_3/investigational_drug_moa_chosen_master.json\n",
    "    did → {\n",
    "      \"chosen_mesh_term\",\n",
    "      \"source_pmid\",\n",
    "      \"rationale\",\n",
    "      ...\n",
    "    }\n",
    "\n",
    "Process:\n",
    "- Build did → chosen_mesh_term (ignoring \"[none]\").\n",
    "- For each did:\n",
    "    • build a normalized name set from drug_names + alternative_names\n",
    "    • use trial_hashes to map dids to the trials where they appear\n",
    "- For each row of trial_product_breakdown.csv:\n",
    "    • look up which dids are associated with that trial_hash\n",
    "    • for each product name in each role column:\n",
    "        - normalize the name\n",
    "        - match against candidate did name sets\n",
    "        - collect 0, 1, or multiple MeSH terms per product\n",
    "- Create three new list-columns aligned with the existing product lists:\n",
    "    • investigational_products_mechanism_mesh_terms\n",
    "    • active_comparators_mechanism_mesh_terms\n",
    "    • standard_of_care_mechanism_mesh_terms\n",
    "\n",
    "Outputs:\n",
    "- cache/task_3/trial_product_breakdown_w_chosen_mechanisms.csv\n",
    "    Same rows as trial_product_breakdown.csv with the three\n",
    "    *_mechanism_mesh_terms columns added.\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------\n",
    "import ast\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "IN_BASE_DIR  = Path(\"cache/task_1\")\n",
    "MOA_BASE_DIR = Path(\"cache/task_3\")\n",
    "\n",
    "IN_BREAKDOWN_CSV      = IN_BASE_DIR  / \"trial_product_breakdown.csv\"\n",
    "PRODUCT_MASTER_BY_DID = IN_BASE_DIR  / \"product_id_master_by_did.json\"\n",
    "MOA_MASTER_PATH       = MOA_BASE_DIR / \"investigational_drug_moa_chosen_master.json\"\n",
    "OUT_BREAKDOWN_CSV     = MOA_BASE_DIR / \"trial_product_breakdown_w_chosen_mechanisms.csv\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse strings like \"['a','b']\" into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None:\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s or s in (\"[]\", \"[ ]\"):\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "        return [v]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "def insert_after(df: pd.DataFrame, col: str, newcol: str, values):\n",
    "    \"\"\"\n",
    "    Insert a new column `newcol` with `values` immediately after `col`.\n",
    "    If `col` is missing, append `newcol` at the end.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    if col not in cols:\n",
    "        df[newcol] = values\n",
    "        return\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "def is_none_term(s: str) -> bool:\n",
    "    \"\"\"Return True if the MeSH term is '[none]' or empty/None-like.\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    s2 = str(s).strip().lower()\n",
    "    return s2 in (\"[none]\", \"none\", \"\")\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    \"\"\"Normalize a product name for matching: lowercase and stripped.\"\"\"\n",
    "    return str(s).strip().lower()\n",
    "\n",
    "# ----------------------------------------\n",
    "# RUN\n",
    "# ----------------------------------------\n",
    "\n",
    "# Load main trial breakdown\n",
    "df = pd.read_csv(IN_BREAKDOWN_CSV, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trial breakdown: {IN_BREAKDOWN_CSV}, shape={df.shape}\")\n",
    "\n",
    "if not PRODUCT_MASTER_BY_DID.exists():\n",
    "    raise FileNotFoundError(f\"Missing product master: {PRODUCT_MASTER_BY_DID}\")\n",
    "\n",
    "if not MOA_MASTER_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing MOA master: {MOA_MASTER_PATH}\")\n",
    "\n",
    "product_by_did = json.loads(PRODUCT_MASTER_BY_DID.read_text(encoding=\"utf-8\"))\n",
    "moa_master     = json.loads(MOA_MASTER_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# did → chosen_mesh_term (only keep non-[none] terms)\n",
    "did_to_mesh: dict[str, str] = {}\n",
    "for did, rec in moa_master.items():\n",
    "    mesh = (rec.get(\"chosen_mesh_term\") or \"\").strip()\n",
    "    if mesh and not is_none_term(mesh):\n",
    "        did_to_mesh[did] = mesh\n",
    "\n",
    "print(f\"did_to_mesh (non-[none]) count: {len(did_to_mesh)}\")\n",
    "\n",
    "# Build:\n",
    "#   trial_hash → list[did]\n",
    "#   did → set(normalized names)\n",
    "trial_to_dids: dict[str, list[str]] = {}\n",
    "did_to_names_norm: dict[str, set[str]] = {}\n",
    "\n",
    "for did, rec in product_by_did.items():\n",
    "    # Only consider dids that actually have a chosen MeSH term\n",
    "    if did not in did_to_mesh:\n",
    "        continue\n",
    "\n",
    "    drug_names = rec.get(\"drug_names\", []) or []\n",
    "    alt_names  = rec.get(\"alternative_names\", []) or []\n",
    "    all_names  = set(drug_names) | set(alt_names)\n",
    "\n",
    "    names_norm = {norm_name(n) for n in all_names if str(n).strip()}\n",
    "    if not names_norm:\n",
    "        continue\n",
    "\n",
    "    did_to_names_norm[did] = names_norm\n",
    "\n",
    "    trial_hashes = rec.get(\"trial_hashes\", []) or []\n",
    "    for th in trial_hashes:\n",
    "        th_str = str(th).strip()\n",
    "        if not th_str:\n",
    "            continue\n",
    "        trial_to_dids.setdefault(th_str, []).append(did)\n",
    "\n",
    "print(f\"trial_to_dids entries: {len(trial_to_dids)}\")\n",
    "print(f\"did_to_names_norm entries: {len(did_to_names_norm)}\")\n",
    "\n",
    "# For each trial row, build MeSH lists per role\n",
    "ROLE_NAME_SPECS = [\n",
    "    (\"investigational_products\", \"investigational_products_mechanism_mesh_terms\"),\n",
    "    (\"active_comparators\",      \"active_comparators_mechanism_mesh_terms\"),\n",
    "    (\"standard_of_care\",        \"standard_of_care_mechanism_mesh_terms\"),\n",
    "]\n",
    "\n",
    "# Prepare containers for new columns\n",
    "new_cols = {spec[1]: [] for spec in ROLE_NAME_SPECS}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    trial_hash = str(row.get(\"trial_hash\", \"\")).strip()\n",
    "    candidate_dids = trial_to_dids.get(trial_hash, [])\n",
    "\n",
    "    # If no dids linked to this trial, all role columns get empty-string lists\n",
    "    if not candidate_dids:\n",
    "        for base_col, new_col in ROLE_NAME_SPECS:\n",
    "            names_list = parse_listish(row.get(base_col, \"\"))\n",
    "            mesh_list = [\"\" for _ in names_list]\n",
    "            new_cols[new_col].append(str(mesh_list))\n",
    "        continue\n",
    "\n",
    "    # Otherwise, match each product name to candidate dids by normalized name\n",
    "    for base_col, new_col in ROLE_NAME_SPECS:\n",
    "        names_list = parse_listish(row.get(base_col, \"\"))\n",
    "        mesh_list = []\n",
    "\n",
    "        for prod_name in names_list:\n",
    "            nn = norm_name(prod_name)\n",
    "            if not nn:\n",
    "                mesh_list.append(\"\")\n",
    "                continue\n",
    "\n",
    "            meshes_for_this = set()\n",
    "\n",
    "            for did in candidate_dids:\n",
    "                name_set = did_to_names_norm.get(did, set())\n",
    "                if nn in name_set:\n",
    "                    mesh = did_to_mesh.get(did, \"\")\n",
    "                    if mesh:\n",
    "                        meshes_for_this.add(mesh)\n",
    "\n",
    "            if not meshes_for_this:\n",
    "                mesh_list.append(\"\")\n",
    "            elif len(meshes_for_this) == 1:\n",
    "                mesh_list.append(next(iter(meshes_for_this)))\n",
    "            else:\n",
    "                # Multiple dids mapping to different MeSH terms\n",
    "                mesh_list.append(\"; \".join(sorted(meshes_for_this)))\n",
    "\n",
    "        new_cols[new_col].append(str(mesh_list))\n",
    "\n",
    "# Attach new columns next to their corresponding name columns\n",
    "for base_col, new_col in ROLE_NAME_SPECS:\n",
    "    insert_after(df, base_col, new_col, new_cols[new_col])\n",
    "\n",
    "# Save output\n",
    "OUT_BREAKDOWN_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT_BREAKDOWN_CSV, index=False)\n",
    "\n",
    "print(f\"✔️ Wrote trial breakdown with chosen MeSH mechanisms → {OUT_BREAKDOWN_CSV}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61cff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping desc2025.xml, already exists at cache/task_3/desc2025.xml.\n",
      "Skipping supp2025.xml, already exists at cache/task_3/supp2025.xml.\n",
      "Done fetching MeSH XML files for 2025.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Download MeSH descriptor & supplementary XML files for 2025.\n",
    "\n",
    "This utility fetches the core MeSH XML files directly from NLM and saves them\n",
    "locally so that downstream tasks can parse MeSH tree numbers, term names,\n",
    "synonyms, and supplementary concept records.\n",
    "\n",
    "Inputs (remote only):\n",
    "- Base URL:\n",
    "      https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh\n",
    "- Files downloaded:\n",
    "      • desc2025.xml  – main descriptor records (MeSH headings, tree numbers)\n",
    "      • supp2025.xml  – supplementary concept records\n",
    "\n",
    "Outputs (to cache/task_3/):\n",
    "- cache/task_3/desc2025.xml\n",
    "- cache/task_3/supp2025.xml\n",
    "\n",
    "Behavior:\n",
    "- If a file already exists locally and is non-empty, it is skipped.\n",
    "- Otherwise the file is fetched via HTTP GET and saved to disk.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://nlmpubs.nlm.nih.gov/projects/mesh/MESH_FILES/xmlmesh\"\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "OUT_DIR = BASE_DIR\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FILES = [\"desc2025.xml\", \"supp2025.xml\"]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def download_file(filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Download a single MeSH XML file by name into OUT_DIR.\n",
    "\n",
    "    Skips download if the file already exists and has non-zero size.\n",
    "    Raises an HTTPError if the request fails.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    out_path = OUT_DIR / filename\n",
    "\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        print(f\"Skipping {filename}, already exists at {out_path}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"⬇ Downloading {url} -> {out_path}\")\n",
    "    resp = requests.get(url, timeout=60)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    out_path.write_bytes(resp.content)\n",
    "    print(f\"Downloaded {filename} ({len(resp.content)} bytes).\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "for fname in FILES:\n",
    "    try:\n",
    "        download_file(fname)\n",
    "    except Exception as e:\n",
    "        print(f\"❗ Error downloading {fname}: {e}\")\n",
    "\n",
    "print(\"Done fetching MeSH XML files for 2025.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f42c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MeSH index terms: 992,888 unique normalized terms\n",
      "sotatercept → {'mesh_id': 'C542017', 'tree_numbers': ['D12.776.828.300'], 'scope_note': 'Recombinant proteins produced by the GENETIC TRANSLATION of fused genes formed by the combination of NUCLEIC ACID REGULATORY SEQUENCES of one or more genes with the protein coding sequences of one or more genes.'}\n",
      "ACE-011 → {'mesh_id': 'C542017', 'tree_numbers': ['D12.776.828.300'], 'scope_note': 'Recombinant proteins produced by the GENETIC TRANSLATION of fused genes formed by the combination of NUCLEIC ACID REGULATORY SEQUENCES of one or more genes with the protein coding sequences of one or more genes.'}\n",
      "winrevair → {'mesh_id': 'C542017', 'tree_numbers': ['D12.776.828.300'], 'scope_note': 'Recombinant proteins produced by the GENETIC TRANSLATION of fused genes formed by the combination of NUCLEIC ACID REGULATORY SEQUENCES of one or more genes with the protein coding sequences of one or more genes.'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build an in-memory MeSH term index from descriptor and supplementary XML.\n",
    "\n",
    "Inputs (from cache/task_3/):\n",
    "- desc2025.xml  – main MeSH descriptor records (headings, tree numbers, scope notes)\n",
    "- supp2025.xml  – supplementary concept records (SCRs), often mapped to descriptors\n",
    "\n",
    "Process:\n",
    "- Parse DescriptorRecords:\n",
    "    • Collect DescriptorUI, tree numbers, preferred heading, scope note.\n",
    "    • Collect all associated terms (preferred + synonyms) and normalize them.\n",
    "    • Map each normalized term → {mesh_id, tree_numbers, scope_note}.\n",
    "- Parse SupplementalRecords:\n",
    "    • Collect all names (record names, concept names, term strings).\n",
    "    • Use any direct SCR tree numbers.\n",
    "    • If no trees, follow HeadingMappedTo descriptors to inherit tree numbers\n",
    "      and scope notes from the mapped descriptors.\n",
    "    • Map each normalized SCR name → {mesh_id, tree_numbers, scope_note},\n",
    "      but do not overwrite an existing descriptor (D-UI) mapping.\n",
    "\n",
    "Outputs:\n",
    "- TREE_INDEX (in memory):\n",
    "    dict[normalized_term] = {\n",
    "        \"mesh_id\": <DescriptorUI or SupplementalRecordUI>,\n",
    "        \"tree_numbers\": [<tree number strings>],\n",
    "        \"scope_note\": <cleaned scope note or \"\">\n",
    "    }\n",
    "\n",
    "- Console summary:\n",
    "    • Number of unique normalized MeSH terms loaded.\n",
    "    • A few spot-check lookups (e.g., sotatercept, ACE-011, winrevair).\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "from pathlib import Path\n",
    "import html\n",
    "import unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "DESC_XML = BASE_DIR / \"desc2025.xml\"\n",
    "SUPP_XML = BASE_DIR / \"supp2025.xml\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "def norm(s: str) -> str:\n",
    "    \"\"\"Normalize a string for lookup: lowercase, unicode-clean, collapse spaces.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = (\n",
    "        t.replace(\"\\u2019\", \"'\")\n",
    "         .replace(\"\\u2018\", \"'\")\n",
    "         .replace(\"\\u2032\", \"'\")\n",
    "         .replace(\"\\u2033\", '\"')\n",
    "         .replace(\"\\u201C\", '\"')\n",
    "         .replace(\"\\u201D\", '\"')\n",
    "         .replace(\"\\u2010\", \"-\")\n",
    "         .replace(\"\\u2011\", \"-\")\n",
    "         .replace(\"\\u2012\", \"-\")\n",
    "         .replace(\"\\u2013\", \"-\")\n",
    "         .replace(\"\\u2014\", \"-\")\n",
    "    )\n",
    "    return \" \".join(t.strip().lower().split())\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Unicode-clean + collapse whitespace (preserve case).\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    t = html.unescape(s)\n",
    "    t = unicodedata.normalize(\"NFKC\", t)\n",
    "    t = (\n",
    "        t.replace(\"\\u2019\", \"'\")\n",
    "         .replace(\"\\u2018\", \"'\")\n",
    "         .replace(\"\\u2032\", \"'\")\n",
    "         .replace(\"\\u2033\", '\"')\n",
    "         .replace(\"\\u201C\", '\"')\n",
    "         .replace(\"\\u201D\", '\"')\n",
    "         .replace(\"\\u2010\", \"-\")\n",
    "         .replace(\"\\u2011\", \"-\")\n",
    "         .replace(\"\\u2012\", \"-\")\n",
    "         .replace(\"\\u2013\", \"-\")\n",
    "         .replace(\"\\u2014\", \"-\")\n",
    "    )\n",
    "    return \" \".join(t.strip().split())\n",
    "\n",
    "\n",
    "def _dedup(seq):\n",
    "    \"\"\"Preserve order while removing duplicates and empty entries.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _extract_scope_note_from_descriptor(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    Prefer the ScopeNote of the PreferredConcept (PreferredConceptYN='Y'),\n",
    "    else fall back to the first ScopeNote present under any Concept.\n",
    "    \"\"\"\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def _extract_scope_note_from_supp(rec: ET.Element) -> str:\n",
    "    \"\"\"\n",
    "    For SCRs, ScopeNote can also live under Concept.\n",
    "    Prefer the PreferredConcept (if flagged), else the first available.\n",
    "    \"\"\"\n",
    "    pref = rec.find(\".//ConceptList/Concept[@PreferredConceptYN='Y']/ScopeNote\")\n",
    "    if pref is not None and pref.text:\n",
    "        return clean_text(pref.text)\n",
    "\n",
    "    any_sn = rec.find(\".//ConceptList/Concept/ScopeNote\")\n",
    "    if any_sn is not None and any_sn.text:\n",
    "        return clean_text(any_sn.text)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def load_mesh_tree_and_id(\n",
    "    desc_xml_fp: Path,\n",
    "    supp_xml_fp: Path,\n",
    ") -> dict[str, dict[str, list[str] | str]]:\n",
    "    \"\"\"\n",
    "    Build a mapping:\n",
    "        normalized_term -> {\"mesh_id\", \"tree_numbers\", \"scope_note\"}\n",
    "    by combining MeSH Descriptor and Supplementary Concept XML.\n",
    "    \"\"\"\n",
    "    term_map: dict[str, dict[str, list[str] | str]] = {}\n",
    "\n",
    "    # Helper maps for SCR fallbacks\n",
    "    heading_to_tree: dict[str, list[str]] = {}\n",
    "    ui_to_tree: dict[str, list[str]] = {}\n",
    "    heading_to_scope: dict[str, str] = {}\n",
    "    ui_to_scope: dict[str, str] = {}\n",
    "\n",
    "    # --- Descriptors ---\n",
    "    if desc_xml_fp.exists():\n",
    "        root = ET.parse(desc_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//DescriptorRecord\"):\n",
    "            desc_ui = (rec.findtext(\"DescriptorUI\") or \"\").strip()\n",
    "            tree_numbers = _dedup(\n",
    "                [\n",
    "                    tn.text.strip()\n",
    "                    for tn in rec.findall(\".//TreeNumberList/TreeNumber\")\n",
    "                    if tn.text\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            heading_raw = rec.findtext(\"DescriptorName/String\")\n",
    "            heading_norm = norm(heading_raw) if heading_raw else \"\"\n",
    "            scope_note = _extract_scope_note_from_descriptor(rec)\n",
    "\n",
    "            if heading_norm:\n",
    "                heading_to_tree[heading_norm] = tree_numbers\n",
    "                heading_to_scope[heading_norm] = scope_note\n",
    "            if desc_ui:\n",
    "                ui_to_tree[desc_ui] = tree_numbers\n",
    "                ui_to_scope[desc_ui] = scope_note\n",
    "\n",
    "            # Collect all terms mapped to this descriptor\n",
    "            terms = set()\n",
    "            if heading_raw:\n",
    "                terms.add(heading_norm)\n",
    "            for concept in rec.findall(\".//Concept\"):\n",
    "                for term in concept.findall(\".//Term\"):\n",
    "                    s = term.findtext(\"String\")\n",
    "                    if s:\n",
    "                        terms.add(norm(s))\n",
    "\n",
    "            for term in terms:\n",
    "                term_map[term] = {\n",
    "                    \"mesh_id\": desc_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": scope_note,\n",
    "                }\n",
    "\n",
    "    # --- Supplementary (SCRs) ---\n",
    "    if supp_xml_fp.exists():\n",
    "        root = ET.parse(supp_xml_fp).getroot()\n",
    "        for rec in root.findall(\".//SupplementalRecord\"):\n",
    "            supp_ui = (rec.findtext(\"SupplementalRecordUI\") or \"\").strip()\n",
    "\n",
    "            # Collect ALL names for this SCR\n",
    "            names = set()\n",
    "\n",
    "            for s in rec.findall(\".//SupplementalRecordName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/ConceptName/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            for s in rec.findall(\".//ConceptList/Concept/TermList/Term/String\"):\n",
    "                if s is not None and s.text:\n",
    "                    names.add(norm(s.text))\n",
    "\n",
    "            # Direct trees (often none for SCRs)\n",
    "            tree_numbers = [\n",
    "                tn.text.strip()\n",
    "                for tn in rec.findall(\".//TreeNumberList/TreeNumber\")\n",
    "                if tn.text\n",
    "            ]\n",
    "\n",
    "            # SCR scope note (preferred concept first)\n",
    "            scr_scope_note = _extract_scope_note_from_supp(rec)\n",
    "\n",
    "            # Fallback via HeadingMappedTo (names and UIs)\n",
    "            mapped_scope_note = \"\"\n",
    "            if not tree_numbers:\n",
    "                # Try mapped names\n",
    "                mapped_names = [\n",
    "                    n.text.strip()\n",
    "                    for n in rec.findall(\n",
    "                        \".//HeadingMappedTo/DescriptorReferredTo/DescriptorName/String\"\n",
    "                    )\n",
    "                    if n is not None and n.text\n",
    "                ]\n",
    "                for m in mapped_names:\n",
    "                    m_norm = norm(m)\n",
    "                    tns = heading_to_tree.get(m_norm)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if (\n",
    "                        not mapped_scope_note\n",
    "                        and m_norm in heading_to_scope\n",
    "                        and heading_to_scope[m_norm]\n",
    "                    ):\n",
    "                        mapped_scope_note = heading_to_scope[m_norm]\n",
    "\n",
    "                # Try mapped UIs\n",
    "                mapped_uis = [\n",
    "                    u.text.strip().lstrip(\"*\")\n",
    "                    for u in rec.findall(\n",
    "                        \".//HeadingMappedTo/DescriptorReferredTo/DescriptorUI\"\n",
    "                    )\n",
    "                    if u is not None and u.text\n",
    "                ]\n",
    "                for mui in mapped_uis:\n",
    "                    tns = ui_to_tree.get(mui)\n",
    "                    if tns:\n",
    "                        tree_numbers.extend(tns)\n",
    "                    if (\n",
    "                        not mapped_scope_note\n",
    "                        and mui in ui_to_scope\n",
    "                        and ui_to_scope[mui]\n",
    "                    ):\n",
    "                        mapped_scope_note = ui_to_scope[mui]\n",
    "\n",
    "                tree_numbers = _dedup(tree_numbers)\n",
    "\n",
    "            final_scope_note = scr_scope_note or mapped_scope_note or \"\"\n",
    "\n",
    "            for name in names:\n",
    "                # Keep Descriptor mapping if already present for same term\n",
    "                if name in term_map and str(term_map[name].get(\"mesh_id\", \"\")).startswith(\n",
    "                    \"D\"\n",
    "                ):\n",
    "                    continue\n",
    "                term_map[name] = {\n",
    "                    \"mesh_id\": supp_ui,\n",
    "                    \"tree_numbers\": tree_numbers,\n",
    "                    \"scope_note\": final_scope_note,\n",
    "                }\n",
    "\n",
    "    return term_map\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "TREE_INDEX = load_mesh_tree_and_id(DESC_XML, SUPP_XML)\n",
    "print(f\"Loaded MeSH index terms: {len(TREE_INDEX):,} unique normalized terms\")\n",
    "\n",
    "# Quick checks\n",
    "for q in [\"sotatercept\", \"ACE-011\", \"winrevair\"]:\n",
    "    k = norm(q)\n",
    "    print(q, \"→\", TREE_INDEX.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fa5631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trials: cache/task_3/trial_product_breakdown_w_chosen_mechanisms.csv, shape=(184, 26)\n",
      "✅ Wrote: cache/task_3/trial_mechanism_mesh_mapping.csv\n",
      "| trial_hash                           | investigational_products                           | investigational_products_mechanism_mesh_terms                        | investigational_products_mechanism_tree_numbers                                 | investigational_products_mechanism_primary_tree_numbers   | investigational_products_alternative_names                                                                                                                                                                                                          | investigational_products_molecular_target   | investigational_products_mechanism                                         | investigational_products_tt_drug_id   | investigational_products_bmt_drug_id   | active_comparators   | active_comparators_mechanism_mesh_terms   | active_comparators_mechanism_tree_numbers   | active_comparators_mechanism_primary_tree_numbers   | active_comparators_alternative_names   | active_comparators_molecular_target   | active_comparators_mechanism   | active_comparators_tt_drug_id   | active_comparators_bmt_drug_id   | placebos   | placebos_alternative_names   | placebos_molecular_target   | placebos_mechanism   | standard_of_care   | standard_of_care_mechanism_mesh_terms   | standard_of_care_mechanism_tree_numbers   | standard_of_care_mechanism_primary_tree_numbers   | standard_of_care_alternative_names   | standard_of_care_molecular_target   | standard_of_care_mechanism   | standard_of_care_tt_drug_id   | standard_of_care_bmt_drug_id   |\n",
      "|:-------------------------------------|:---------------------------------------------------|:---------------------------------------------------------------------|:--------------------------------------------------------------------------------|:----------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:---------------------------------------------------------------------------|:--------------------------------------|:---------------------------------------|:---------------------|:------------------------------------------|:--------------------------------------------|:----------------------------------------------------|:---------------------------------------|:--------------------------------------|:-------------------------------|:--------------------------------|:---------------------------------|:-----------|:-----------------------------|:----------------------------|:---------------------|:-------------------|:----------------------------------------|:------------------------------------------|:--------------------------------------------------|:-------------------------------------|:------------------------------------|:-----------------------------|:------------------------------|:-------------------------------|\n",
      "| tid_0541995757b10e613a42173d6b8ddc09 | ['cinacalcet hydrochloride']                       | ['Receptors, Calcium-Sensing / agonists']                            | [['D12.776.543.750.695.115']]                                                   | ['D12.776.543.750.695.115']                               | [['cinacalcet HCl', 'cinacalcet', 'cinacalcet HCl, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride tablet']] | ['Calcium-sensing receptor (CaSR)']         | ['Calcimimetic; positive allosteric modulator of CaSR']                    | ['194454']                            | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0d6e9b2f3f57c17c0e93610e28853f0c | ['Xenopax']                                        | ['Interleukin-2 Receptor alpha Subunit* / metabolism; immunology']   | [['D12.776.543.750.705.852.420.320.500']]                                       | ['D12.776.543.750.705.852.420.320.500']                   | [['daclizumab', 'daclizumab, Shanghai CP Guojian', 'transplantation MAb, Shanghai', 'recombinant anti-CD25 humanized monoclonal antibody injection']]                                                                                               | ['CD25 (IL2RA)']                            | ['Humanized monoclonal antibody; interleukin-2 receptor alpha antagonist'] | ['40098']                             | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0da20e863cfc5f3e369868462bff74e0 | ['recombinant erythropoiesis-stimulating protein'] | ['Receptors, Erythropoietin / agonists*']                            | [['D12.776.543.750.705.852.150.200', 'D12.776.543.750.750.400.200.340']]        | ['D12.776.543.750.705.852.150.200']                       | [['NuPIAO', 'nupiao', 'rESP', 'recombinant erythropoietin stimulating protein', 'recombinant erythropoietin stimulating protein, 3SBio', 'recombinant erythropoiesis-stimulating protein injection (CHO cells)', 'SSS-06', 'SSS06', 'SSS 06']]      | ['Erythropoietin receptor (EPOR)']          | ['Erythropoietin receptor agonist']                                        | ['40640']                             | ['19694']                              | []                   | []                                        | []                                          | []                                                  | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e698ee5065c49d23fcf57516957a273 | ['SB8']                                            | ['Vascular Endothelial Growth Factor A / antagonists & inhibitors*'] | [['D12.644.276.100.800.200', 'D12.776.467.100.800.200', 'D23.529.100.800.200']] | ['D12.644.276.100.800.200']                               | [['SB 8', 'SB-8', 'Aybintio', 'Onbevzi', 'bevacizumab, Samsung Bioepis', '615', 'SB8 Anti-VEGF antibody']]                                                                                                                                          | ['VEGF-A']                                  | ['Anti-VEGF-A monoclonal antibody (angiogenesis inhibitor)']               | ['113881']                            | ['27338']                              | []                   | []                                        | []                                          | []                                                  | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e8fa21079f928135dfc6164a15285f8 | ['SSS-17']                                         | ['Prolyl-Hydroxylase Inhibitors / pharmacology*']                    | [['D27.505.519.389.740']]                                                       | ['D27.505.519.389.740']                                   | [['SSS17', 'SSS 17', '[14C] SSS17', '[14C]-SSS17', '[14C]SSS17', 'HIF 117', 'HIF-117', 'HIF117', '[14C]HIF-117']]                                                                                                                                   | ['HIF']                                     | ['Hypoxia-inducible factor antagonist']                                    | ['130313']                            | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                   | []                                  | []                           | []                            | []                             |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Map chosen MeSH mechanism terms to MeSH tree numbers for each trial and role.\n",
    "\n",
    "Inputs (from cache/task_3/):\n",
    "- trial_product_breakdown_w_chosen_mechanisms.csv\n",
    "    One row per trial, including:\n",
    "        • trial_hash\n",
    "        • investigational_products_mechanism_mesh_terms\n",
    "        • active_comparators_mechanism_mesh_terms\n",
    "        • standard_of_care_mechanism_mesh_terms\n",
    "    Each of the *_mechanism_mesh_terms columns is a list-like structure of\n",
    "    MeSH headings selected in the previous step (e.g., [\"Alkylating Agents\", ...]).\n",
    "\n",
    "- In-memory MeSH index from the MeSH loader cell:\n",
    "    • TREE_INDEX: dict[normalized_term] = {\"mesh_id\", \"tree_numbers\", \"scope_note\"}\n",
    "    • norm(): helper used to normalize MeSH headings for lookup\n",
    "\n",
    "Process:\n",
    "1. For each MeSH heading in the *_mechanism_mesh_terms columns:\n",
    "    - Look up all MeSH tree numbers via TREE_INDEX (mesh_heading_to_tree_numbers).\n",
    "2. For each list of tree numbers belonging to a single MeSH term:\n",
    "    - Pick one \"primary\" tree number using a simple heuristic that prefers\n",
    "      pharmacologically relevant branches (D12, D27, D02, etc.).\n",
    "3. Build six new columns:\n",
    "    • investigational_products_mechanism_tree_numbers\n",
    "    • investigational_products_mechanism_primary_tree_numbers\n",
    "    • active_comparators_mechanism_tree_numbers\n",
    "    • active_comparators_mechanism_primary_tree_numbers\n",
    "    • standard_of_care_mechanism_tree_numbers\n",
    "    • standard_of_care_mechanism_primary_tree_numbers\n",
    "\n",
    "Outputs (to cache/task_3/):\n",
    "- trial_mechanism_mesh_mapping.csv\n",
    "    Same rows as input, plus the six new tree-number columns inserted\n",
    "    next to the corresponding *_mechanism_mesh_terms columns.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "TRIALS_IN_PATH = BASE_DIR / \"trial_product_breakdown_w_chosen_mechanisms.csv\"\n",
    "TRIALS_OUT_PATH = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# Sanity: TREE_INDEX and norm must already be loaded\n",
    "# -----------------------------------------\n",
    "try:\n",
    "    TREE_INDEX\n",
    "except NameError:\n",
    "    raise RuntimeError(\"TREE_INDEX is not defined — run the MeSH loader cell first.\")\n",
    "\n",
    "try:\n",
    "    norm\n",
    "except NameError:\n",
    "    raise RuntimeError(\"norm() is not defined — ensure it is defined in the MeSH loader cell.\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# HELPERS\n",
    "# -----------------------------------------\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a stringified Python list (or empty) into a real list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def mesh_heading_to_tree_numbers(chosen_mesh_term: str):\n",
    "    \"\"\"\n",
    "    Return *all* tree numbers for the chosen MeSH heading using TREE_INDEX.\n",
    "    \"\"\"\n",
    "    if not chosen_mesh_term or chosen_mesh_term == \"[none]\":\n",
    "        return []\n",
    "    base = chosen_mesh_term.split(\" / \")[0].replace(\"*\", \"\").strip()\n",
    "    key = norm(base)\n",
    "    info = TREE_INDEX.get(key)\n",
    "    if not info:\n",
    "        return []\n",
    "    return info.get(\"tree_numbers\", []) or []\n",
    "\n",
    "\n",
    "# Clinical-pharmacology-ish heuristic for ONE primary tree number\n",
    "PRIORITY_PREFIXES = [\n",
    "    \"D12.\",  # Proteins: receptors, enzymes, cytokines, antibodies (biologics / targets)\n",
    "    \"D27.\",  # Chemical Actions and Uses: classic pharmacologic classes\n",
    "    \"D02.\",  # Organic Chemicals: small-molecule drugs\n",
    "    \"D09.\",  # Carbohydrates\n",
    "    \"D23.\",  # Immunologic Factors\n",
    "    \"D26.\",  # Biological Factors\n",
    "]\n",
    "\n",
    "def _depth(tn: str) -> int:\n",
    "    \"\"\"Depth proxy for a tree number: more segments = deeper (more specific).\"\"\"\n",
    "    return len(tn.split(\".\"))\n",
    "\n",
    "def choose_primary_tree(tree_numbers):\n",
    "    \"\"\"\n",
    "    Given a list of tree numbers for ONE MeSH term,\n",
    "    choose a single 'primary' tree that best reflects the\n",
    "    pharmacologic / target-level concept.\n",
    "    \"\"\"\n",
    "    if not tree_numbers:\n",
    "        return \"\"\n",
    "\n",
    "    tns = [t.strip() for t in tree_numbers if isinstance(t, str) and t.strip()]\n",
    "    if not tns:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Prefer specific high-value branches (by prefix)\n",
    "    for prefix in PRIORITY_PREFIXES:\n",
    "        candidates = [t for t in tns if t.startswith(prefix)]\n",
    "        if candidates:\n",
    "            # choose the highest-level (shortest depth) node in that branch\n",
    "            return max(candidates, key=_depth)\n",
    "\n",
    "    # 2) Else prefer any Chemicals & Drugs branch (D*)\n",
    "    d_candidates = [t for t in tns if t.startswith(\"D\")]\n",
    "    if d_candidates:\n",
    "        return max(d_candidates, key=_depth)\n",
    "\n",
    "    # 3) Fallback: shortest overall\n",
    "    return max(tns, key=_depth)\n",
    "\n",
    "\n",
    "def trees_for_mesh_term_list(mesh_term_list):\n",
    "    \"\"\"\n",
    "    Given a list of MeSH headings (already mapped mechanism terms),\n",
    "    return:\n",
    "      all_tree_lists : list of [list-of-tree-numbers] per term\n",
    "      primary_trees  : list of ONE chosen tree number per term (\"\" if none)\n",
    "    \"\"\"\n",
    "    all_tree_lists = []\n",
    "    primary_trees = []\n",
    "\n",
    "    for term in mesh_term_list:\n",
    "        term_str = (term or \"\").strip()\n",
    "        if not term_str:\n",
    "            all_tree_lists.append([])\n",
    "            primary_trees.append(\"\")\n",
    "            continue\n",
    "\n",
    "        all_trees = mesh_heading_to_tree_numbers(term_str)\n",
    "        all_tree_lists.append(all_trees)\n",
    "\n",
    "        primary = choose_primary_tree(all_trees)\n",
    "        primary_trees.append(primary)\n",
    "\n",
    "    return all_tree_lists, primary_trees\n",
    "\n",
    "\n",
    "def insert_after(df, col, newcol, values):\n",
    "    \"\"\"\n",
    "    Insert a new column immediately after `col` if it exists,\n",
    "    otherwise append it to the end of the DataFrame.\n",
    "    \"\"\"\n",
    "    cols = list(df.columns)\n",
    "    if col not in cols:\n",
    "        df[newcol] = values\n",
    "        return\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "# -----------------------------------------\n",
    "# RUN\n",
    "# -----------------------------------------\n",
    "# Load trial dataset\n",
    "df = pd.read_csv(TRIALS_IN_PATH, dtype=str).fillna(\"\")\n",
    "print(f\"Loaded trials: {TRIALS_IN_PATH}, shape={df.shape}\")\n",
    "\n",
    "# Compute tree-number columns row-wise\n",
    "inv_trees_all = []\n",
    "inv_trees_primary = []\n",
    "\n",
    "ac_trees_all = []\n",
    "ac_trees_primary = []\n",
    "\n",
    "soc_trees_all = []\n",
    "soc_trees_primary = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inv_mesh_terms = parse_listish(row.get(\"investigational_products_mechanism_mesh_terms\"))\n",
    "    ac_mesh_terms = parse_listish(row.get(\"active_comparators_mechanism_mesh_terms\"))\n",
    "    soc_mesh_terms = parse_listish(row.get(\"standard_of_care_mechanism_mesh_terms\"))\n",
    "\n",
    "    inv_all_t, inv_primary_t = trees_for_mesh_term_list(inv_mesh_terms)\n",
    "    ac_all_t, ac_primary_t = trees_for_mesh_term_list(ac_mesh_terms)\n",
    "    soc_all_t, soc_primary_t = trees_for_mesh_term_list(soc_mesh_terms)\n",
    "\n",
    "    inv_trees_all.append(inv_all_t)\n",
    "    inv_trees_primary.append(inv_primary_t)\n",
    "\n",
    "    ac_trees_all.append(ac_all_t)\n",
    "    ac_trees_primary.append(ac_primary_t)\n",
    "\n",
    "    soc_trees_all.append(soc_all_t)\n",
    "    soc_trees_primary.append(soc_primary_t)\n",
    "\n",
    "# Insert columns next to the mechanism_mesh_terms columns\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_mesh_terms\",\n",
    "    \"investigational_products_mechanism_tree_numbers\",\n",
    "    inv_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_tree_numbers\",\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    inv_trees_primary,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_mesh_terms\",\n",
    "    \"active_comparators_mechanism_tree_numbers\",\n",
    "    ac_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_tree_numbers\",\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    ac_trees_primary,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_mesh_terms\",\n",
    "    \"standard_of_care_mechanism_tree_numbers\",\n",
    "    soc_trees_all,\n",
    ")\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "    soc_trees_primary,\n",
    ")\n",
    "\n",
    "# Save output\n",
    "TRIALS_OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(TRIALS_OUT_PATH, index=False)\n",
    "print(f\"✅ Wrote: {TRIALS_OUT_PATH}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e09af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tree number + term breakdown → cache/task_3/trial_mechanism_mesh_tree_number_counts.csv\n",
      "Top 20 combinations:\n",
      "| mesh_term                                                                    | tree_number                             |   count |\n",
      "|:-----------------------------------------------------------------------------|:----------------------------------------|--------:|\n",
      "| Receptors, Erythropoietin / agonists*                                        | D12.776.543.750.705.852.150.200         |      27 |\n",
      "| Cross-Linking Reagents                                                       | D27.720.470.410.210                     |      13 |\n",
      "| Interleukin-4 Receptor alpha Subunit / antagonists & inhibitors; immunology* | D12.776.543.750.705.852.420.360.300.200 |      13 |\n",
      "| Receptors, Thrombopoietin / agonists                                         | D12.776.543.750.705.852.610             |      13 |\n",
      "| ErbB Receptors / antagonists & inhibitors*; metabolism                       | D12.776.543.750.750.400.074             |      10 |\n",
      "| Tubulin Modulators / pharmacology*                                           | D27.505.519.593.249.500                 |      10 |\n",
      "| Receptor, ErbB-2 / antagonists & inhibitors*                                 | D12.776.543.750.750.400.074.400         |       9 |\n",
      "| Vascular Endothelial Growth Factor A / antagonists & inhibitors              | D12.644.276.100.800.200                 |       9 |\n",
      "| Tumor Necrosis Factor Inhibitors                                             | D27.505.954.158.757                     |       8 |\n",
      "| Programmed Cell Death 1 Receptor* / antagonists & inhibitors                 | D12.776.543.750.705.222.875             |       7 |\n",
      "| Folic Acid Antagonists / pharmacology*                                       | D27.505.519.389.350                     |       7 |\n",
      "| Programmed Cell Death 1 Receptor / antagonists & inhibitors                  | D12.776.543.750.705.222.875             |       7 |\n",
      "| Thymidylate Synthase / antagonists & inhibitors*                             | D08.811.913.555.500.862                 |       7 |\n",
      "| Urate Oxidase / pharmacology                                                 | D08.811.682.943                         |       6 |\n",
      "| Interleukin-1beta / antagonists & inhibitors*                                | D12.644.276.374.465.010.600             |       6 |\n",
      "| Programmed Cell Death 1 Receptor / antagonists & inhibitors*                 | D12.776.543.750.705.222.875             |       6 |\n",
      "| Prolyl-Hydroxylase Inhibitors / pharmacology*                                | D27.505.519.389.740                     |       6 |\n",
      "| Interleukin-17 / antagonists & inhibitors*                                   | D12.644.276.374.465.517                 |       6 |\n",
      "| Interleukin-5 / antagonists & inhibitors                                     | D12.644.276.374.465.202                 |       5 |\n",
      "| Tumor Necrosis Factor-alpha / antagonists & inhibitors*                      | D12.644.276.374.500.800                 |       5 |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Summarize MeSH mechanism terms by primary tree number across all trial mechanisms.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_3/trial_mechanism_mesh_mapping.csv\n",
    "    One row per trial, with per-role lists of:\n",
    "      • MeSH mechanism terms\n",
    "      • Primary MeSH tree numbers (one primary tree per mechanism term)\n",
    "\n",
    "Process:\n",
    "- For each of three mechanism roles:\n",
    "      • investigational_products\n",
    "      • active_comparators\n",
    "      • standard_of_care\n",
    "  parse list-like columns for:\n",
    "      • mechanism MeSH terms\n",
    "      • primary tree numbers\n",
    "- Iterate over aligned (mesh_term, primary_tree_number) pairs.\n",
    "- Drop empty / '[none]' mesh terms and empty tree numbers.\n",
    "- Count how often each (mesh_term, primary_tree_number) pair occurs across trials.\n",
    "\n",
    "Outputs:\n",
    "- Table of counts per (mesh_term, primary_tree_number), sorted by descending count:\n",
    "      cache/task_3/trial_mechanism_mesh_tree_number_counts.csv\n",
    "- Console preview of the top 20 (mesh_term, tree_number) combinations.\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "\n",
    "# Columns with MeSH mechanism terms (already mapped)\n",
    "MESH_TERM_COLS = [\n",
    "    \"investigational_products_mechanism_mesh_terms\",\n",
    "    \"active_comparators_mechanism_mesh_terms\",\n",
    "    \"standard_of_care_mechanism_mesh_terms\",\n",
    "]\n",
    "\n",
    "# Columns with *primary* tree numbers (one tree per mechanism)\n",
    "TREE_COLS = [\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        return val if isinstance(val, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def is_none_term(s: str) -> bool:\n",
    "    \"\"\"Treat '[none]' / 'none' / empty as unusable.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return True\n",
    "    t = s.strip().lower()\n",
    "    return t in (\"\", \"[none]\", \"none\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "tree_to_mesh_terms = defaultdict(list)\n",
    "pair_counter = Counter()   # (mesh_term, primary_tree_number) → count\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # Parse lists for each mechanism category\n",
    "    mesh_term_lists   = [parse_listish(row.get(c, \"[]\")) for c in MESH_TERM_COLS]\n",
    "    tree_number_lists = [parse_listish(row.get(c, \"[]\")) for c in TREE_COLS]\n",
    "\n",
    "    # Iterate over the three mechanism categories in parallel\n",
    "    for mesh_terms, tree_nums in zip(mesh_term_lists, tree_number_lists):\n",
    "        for mesh_term, primary_tn in zip(mesh_terms, tree_nums):\n",
    "            if is_none_term(mesh_term):\n",
    "                continue\n",
    "            if not isinstance(primary_tn, str) or not primary_tn.strip():\n",
    "                continue\n",
    "\n",
    "            tn = primary_tn.strip()\n",
    "            mt = mesh_term.strip()\n",
    "\n",
    "            pair_counter[(mt, tn)] += 1\n",
    "            tree_to_mesh_terms[tn].append(mt)\n",
    "\n",
    "# Convert to DataFrame\n",
    "out_rows = [\n",
    "    {\n",
    "        \"mesh_term\": mesh_term,\n",
    "        \"tree_number\": tree_num,\n",
    "        \"count\": count,\n",
    "    }\n",
    "    for (mesh_term, tree_num), count in pair_counter.items()\n",
    "]\n",
    "\n",
    "out_df = pd.DataFrame(out_rows).sort_values(\"count\", ascending=False)\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved tree number + term breakdown → {OUTPUT_PATH}\")\n",
    "print(\"Top 20 combinations:\")\n",
    "print(out_df.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd96708b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved mechanism super-group mapping → cache/task_3/trial_mechanism_super_group_mapping.csv\n",
      "| mesh_term                                                                              | tree_number                             |   count | mechanism_super_group                     |\n",
      "|:---------------------------------------------------------------------------------------|:----------------------------------------|--------:|:------------------------------------------|\n",
      "| Antibodies, Bispecific*                                                                | D12.776.124.486.485.114.125             |       4 | biologic_antibodies_biologics             |\n",
      "| Antibodies, Bispecific* / pharmacology; therapeutic use                                | D12.776.124.486.485.114.125             |       3 | biologic_antibodies_biologics             |\n",
      "| Antibodies, Monoclonal, Humanized*                                                     | D12.776.124.486.485.114.224.060         |       1 | biologic_antibodies_biologics             |\n",
      "| Antibodies, Monoclonal, Humanized                                                      | D12.776.124.486.485.114.224.060         |       1 | biologic_antibodies_biologics             |\n",
      "| Antibodies, Monoclonal / economics*; therapeutic use*                                  | D12.776.124.486.485.114.224             |       1 | biologic_antibodies_biologics             |\n",
      "| Receptors, Chimeric Antigen* / immunology                                              | D12.776.543.750.705.816.824.150         |       1 | biologic_antibodies_biologics             |\n",
      "| Rituximab                                                                              | D12.776.124.486.485.114.224.075.785     |       1 | biologic_antibodies_biologics             |\n",
      "| Tubulin Modulators / pharmacology*                                                     | D27.505.519.593.249.500                 |      10 | classical_cytotoxic_chemotherapy          |\n",
      "| Antineoplastic Agents, Alkylating* / pharmacology                                      | D27.505.519.124.035                     |       3 | classical_cytotoxic_chemotherapy          |\n",
      "| Tubulin Modulators* / pharmacology; chemistry                                          | D27.505.519.593.249.500                 |       2 | classical_cytotoxic_chemotherapy          |\n",
      "| Antimetabolites, Antineoplastic / pharmacology*                                        | D27.505.519.186.144                     |       2 | classical_cytotoxic_chemotherapy          |\n",
      "| Tubulin Modulators / chemistry; pharmacology*; therapeutic use                         | D27.505.519.593.249.500                 |       2 | classical_cytotoxic_chemotherapy          |\n",
      "| Topoisomerase II Inhibitors                                                            | D27.505.519.389.892.750                 |       2 | classical_cytotoxic_chemotherapy          |\n",
      "| Topoisomerase I Inhibitors*                                                            | D27.505.519.389.892.500                 |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Tubulin Modulators / pharmacology*; therapeutic use                                    | D27.505.519.593.249.500                 |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Tubulin Modulators / pharmacology                                                      | D27.505.519.593.249.500                 |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Taxoids / pharmacology*                                                                | D02.455.426.392.368.242.888             |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Antimetabolites, Antineoplastic / chemical synthesis; metabolism; pharmacology*        | D27.505.519.186.144                     |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Topoisomerase I Inhibitors / pharmacology*                                             | D27.505.519.389.892.500                 |       1 | classical_cytotoxic_chemotherapy          |\n",
      "| Receptors, Erythropoietin / agonists*                                                  | D12.776.543.750.705.852.150.200         |      27 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Thrombopoietin / agonists                                                   | D12.776.543.750.705.852.610             |      13 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Thrombopoietin / agonists*                                                  | D12.776.543.750.705.852.610             |       4 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Glucocorticoid* / agonists                                                  | D12.776.826.750.430                     |       2 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Glucocorticoid / agonists*                                                  | D12.776.826.750.430                     |       2 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Interleukin-2* / agonists                                                   | D12.776.543.750.705.852.420.320         |       2 | cytokine_hormone_receptor_modulators      |\n",
      "| Receptors, Calcium-Sensing / agonists                                                  | D12.776.543.750.695.115                 |       1 | cytokine_hormone_receptor_modulators      |\n",
      "| Interleukin-2 Receptor alpha Subunit* / metabolism; immunology                         | D12.776.543.750.705.852.420.320.500     |       1 | cytokine_hormone_receptor_modulators      |\n",
      "| Programmed Cell Death 1 Receptor* / antagonists & inhibitors                           | D12.776.543.750.705.222.875             |       7 | immune_checkpoint_immune_modulation       |\n",
      "| Programmed Cell Death 1 Receptor / antagonists & inhibitors                            | D12.776.543.750.705.222.875             |       7 | immune_checkpoint_immune_modulation       |\n",
      "| Programmed Cell Death 1 Receptor / antagonists & inhibitors*                           | D12.776.543.750.705.222.875             |       6 | immune_checkpoint_immune_modulation       |\n",
      "| Tumor Necrosis Factor-alpha / antagonists & inhibitors*                                | D12.644.276.374.500.800                 |       5 | immune_checkpoint_immune_modulation       |\n",
      "| Immune Checkpoint Inhibitors* / therapeutic use; pharmacology                          | D27.505.954.248.384.500                 |       4 | immune_checkpoint_immune_modulation       |\n",
      "| Tumor Necrosis Factor-alpha / antagonists & inhibitors*; pharmacology*                 | D12.644.276.374.500.800                 |       3 | immune_checkpoint_immune_modulation       |\n",
      "| Lectins, C-Type / antagonists & inhibitors*; immunology                                | D12.776.503.280                         |       2 | immune_checkpoint_immune_modulation       |\n",
      "| Immune Checkpoint Inhibitors / pharmacology                                            | D27.505.954.248.384.500                 |       2 | immune_checkpoint_immune_modulation       |\n",
      "| Programmed Cell Death 1 Receptor* / antagonists & inhibitors; immunology               | D12.776.543.750.705.222.875             |       1 | immune_checkpoint_immune_modulation       |\n",
      "| CD47 Antigen / antagonists & inhibitors*; immunology; metabolism                       | D12.776.395.550.014                     |       1 | immune_checkpoint_immune_modulation       |\n",
      "| Interleukin 1 Receptor Antagonist Protein*                                             | D12.644.276.374.460                     |       1 | immune_checkpoint_immune_modulation       |\n",
      "| Interleukin 1 Receptor Antagonist Protein                                              | D12.644.276.374.460                     |       1 | immune_checkpoint_immune_modulation       |\n",
      "| Interleukin-33* / antagonists & inhibitors; immunology                                 | D12.644.276.374.465.850                 |       1 | immune_checkpoint_immune_modulation       |\n",
      "| Tumor Necrosis Factor Ligand Superfamily Member 15* / antagonists & inhibitors         | D12.644.276.374.750.720                 |       1 | immune_checkpoint_immune_modulation       |\n",
      "| Tumor Necrosis Factor Inhibitors                                                       | D27.505.954.158.757                     |       8 | metabolic_pathway_modulators              |\n",
      "| Folic Acid Antagonists / pharmacology*                                                 | D27.505.519.389.350                     |       7 | metabolic_pathway_modulators              |\n",
      "| Thymidylate Synthase / antagonists & inhibitors*                                       | D08.811.913.555.500.862                 |       7 | metabolic_pathway_modulators              |\n",
      "| Prolyl-Hydroxylase Inhibitors / pharmacology*                                          | D27.505.519.389.740                     |       6 | metabolic_pathway_modulators              |\n",
      "| Gluconeogenesis / drug effects*                                                        | G02.111.158.500                         |       2 | metabolic_pathway_modulators              |\n",
      "| IMP Dehydrogenase / antagonists & inhibitors*                                          | D08.811.682.047.820.450                 |       1 | metabolic_pathway_modulators              |\n",
      "| Histamine H1 Antagonists* / pharmacology                                               | D27.505.519.625.375.425.400             |       3 | small_molecule_immunomod_antiinflammatory |\n",
      "| Phosphodiesterase 4 Inhibitors* / pharmacology                                         | D27.505.519.389.735.374                 |       3 | small_molecule_immunomod_antiinflammatory |\n",
      "| Calcineurin Inhibitors                                                                 | D27.505.519.389.174                     |       3 | small_molecule_immunomod_antiinflammatory |\n",
      "| Glucocorticoids                                                                        | D27.505.696.399.472.488                 |       2 | small_molecule_immunomod_antiinflammatory |\n",
      "| Glucocorticoids* / administration & dosage; therapeutic use                            | D27.505.696.399.472.488                 |       1 | small_molecule_immunomod_antiinflammatory |\n",
      "| Immunosuppressive Agents*                                                              | D27.505.696.477.656                     |       1 | small_molecule_immunomod_antiinflammatory |\n",
      "| Antimalarials* / pharmacology                                                          | D27.505.954.122.250.100.085             |       1 | small_molecule_immunomod_antiinflammatory |\n",
      "| Calcineurin Inhibitors*                                                                | D27.505.519.389.174                     |       1 | small_molecule_immunomod_antiinflammatory |\n",
      "| Cross-Linking Reagents                                                                 | D27.720.470.410.210                     |      13 | supportive_adjunctive_agents              |\n",
      "| Interleukin-4 Receptor alpha Subunit / antagonists & inhibitors; immunology*           | D12.776.543.750.705.852.420.360.300.200 |      13 | supportive_adjunctive_agents              |\n",
      "| ErbB Receptors / antagonists & inhibitors*; metabolism                                 | D12.776.543.750.750.400.074             |      10 | supportive_adjunctive_agents              |\n",
      "| Urate Oxidase / pharmacology                                                           | D08.811.682.943                         |       6 | supportive_adjunctive_agents              |\n",
      "| Interleukin-1beta / antagonists & inhibitors*                                          | D12.644.276.374.465.010.600             |       6 | supportive_adjunctive_agents              |\n",
      "| Interleukin-17 / antagonists & inhibitors*                                             | D12.644.276.374.465.517                 |       6 | supportive_adjunctive_agents              |\n",
      "| Interleukin-5 / antagonists & inhibitors                                               | D12.644.276.374.465.202                 |       5 | supportive_adjunctive_agents              |\n",
      "| Complement C3b / antagonists & inhibitors                                              | D12.776.124.486.274.250.260             |       2 | supportive_adjunctive_agents              |\n",
      "| Antigens, CD20                                                                         | D23.050.301.264.035.120                 |       2 | supportive_adjunctive_agents              |\n",
      "| Nerve Growth Factor / antagonists & inhibitors*                                        | D12.644.276.860.437                     |       2 | supportive_adjunctive_agents              |\n",
      "| Tetrahydrofolates / pharmacology*                                                      | D03.633.100.733.631.400.800             |       1 | supportive_adjunctive_agents              |\n",
      "| Proto-Oncogene Proteins c-bcl-2 / antagonists & inhibitors*                            | D12.776.624.664.700.169                 |       1 | supportive_adjunctive_agents              |\n",
      "| Receptors, Opioid, kappa / agonists                                                    | D12.776.543.750.720.600.610.400         |       1 | supportive_adjunctive_agents              |\n",
      "| Factor Xa Inhibitors                                                                   | D27.505.519.389.745.800.449.500         |       1 | supportive_adjunctive_agents              |\n",
      "| Ion Exchange Resins*                                                                   | D27.720.470.420                         |       1 | supportive_adjunctive_agents              |\n",
      "| Receptors, Opioid, kappa / agonists*                                                   | D12.776.543.750.720.600.610.400         |       1 | supportive_adjunctive_agents              |\n",
      "| Folic Acid / analogs & derivatives; metabolism                                         | D03.633.100.733.631.400                 |       1 | supportive_adjunctive_agents              |\n",
      "| Receptor, ErbB-2 / antagonists & inhibitors*                                           | D12.776.543.750.750.400.074.400         |       9 | targeted_pathway_inhibitors               |\n",
      "| Vascular Endothelial Growth Factor A / antagonists & inhibitors                        | D12.644.276.100.800.200                 |       9 | targeted_pathway_inhibitors               |\n",
      "| Receptor, ErbB-2* / antagonists & inhibitors; metabolism                               | D12.776.543.750.750.400.074.400         |       5 | targeted_pathway_inhibitors               |\n",
      "| Vascular Endothelial Growth Factor A / antagonists & inhibitors; genetics; metabolism  | D12.644.276.100.800.200                 |       4 | targeted_pathway_inhibitors               |\n",
      "| Vascular Endothelial Growth Factor A / antagonists & inhibitors*                       | D12.644.276.100.800.200                 |       3 | targeted_pathway_inhibitors               |\n",
      "| Janus Kinase Inhibitors*                                                               | D27.505.519.389.755.500                 |       2 | targeted_pathway_inhibitors               |\n",
      "| Vascular Endothelial Growth Factor A* / antagonists & inhibitors; genetics; physiology | D12.644.276.100.800.200                 |       1 | targeted_pathway_inhibitors               |\n",
      "| MTOR Inhibitors*                                                                       | D27.505.519.389.755.750                 |       1 | targeted_pathway_inhibitors               |\n",
      "| Cancer Vaccines* / immunology; therapeutic use                                         | D20.215.894.200                         |       1 | vaccines_immune_biologics                 |\n",
      "| Recombinant Fusion Proteins* / pharmacology; therapeutic use                           | D12.776.828.300                         |       1 | vaccines_immune_biologics                 |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classify MeSH mechanism terms into 9 high-level MOA super-groups.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_3/trial_mechanism_mesh_tree_number_counts.csv\n",
    "    Contains one row per (mesh_term, tree_number) pair with a \"count\" column:\n",
    "        • mesh_term   : MeSH mechanism term chosen for a drug mechanism.\n",
    "        • tree_number : Primary MeSH tree number used for that mechanism.\n",
    "        • count       : Frequency of this (mesh_term, tree_number) across trials.\n",
    "\n",
    "Process:\n",
    "- For each row, apply a deterministic heuristic classifier that:\n",
    "    • Uses both mesh_term text and tree_number branches.\n",
    "    • Maps each mechanism into one of 9 super-groups:\n",
    "        G1: cytokine_hormone_receptor_modulators\n",
    "        G2: immune_checkpoint_immune_modulation\n",
    "        G3: targeted_pathway_inhibitors\n",
    "        G4: classical_cytotoxic_chemotherapy\n",
    "        G5: biologic_antibodies_biologics\n",
    "        G6: small_molecule_immunomod_antiinflammatory\n",
    "        G7: metabolic_pathway_modulators\n",
    "        G8: vaccines_immune_biologics\n",
    "        G9: supportive_adjunctive_agents\n",
    "\n",
    "Outputs:\n",
    "- CSV with one row per (mesh_term, tree_number) and its assigned super-group:\n",
    "      cache/task_3/trial_mechanism_super_group_mapping.csv\n",
    "\n",
    "  Columns:\n",
    "      mesh_term\n",
    "      tree_number\n",
    "      count\n",
    "      mechanism_super_group\n",
    "\n",
    "- Console preview of the full mapping table (markdown).\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_mesh_tree_number_counts.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# SUPER-GROUP LABELS\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Define super-group labels (9 buckets)\n",
    "G1 = \"cytokine_hormone_receptor_modulators\"\n",
    "G2 = \"immune_checkpoint_immune_modulation\"\n",
    "G3 = \"targeted_pathway_inhibitors\"\n",
    "G4 = \"classical_cytotoxic_chemotherapy\"\n",
    "G5 = \"biologic_antibodies_biologics\"\n",
    "G6 = \"small_molecule_immunomod_antiinflammatory\"\n",
    "G7 = \"metabolic_pathway_modulators\"\n",
    "G8 = \"vaccines_immune_biologics\"\n",
    "G9 = \"supportive_adjunctive_agents\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "\n",
    "def classify_super_group(mesh_term: str, tree_number: str) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic mapping of mesh_term + tree_number to one of 9 MOA super-groups.\n",
    "    Think like a clinical pharmacologist, but keep it deterministic and simple.\n",
    "    \"\"\"\n",
    "    # Robust string coercion\n",
    "    t = str(mesh_term or \"\").lower()\n",
    "    tn = str(tree_number or \"\").strip()\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 8: Vaccines & immune biologics (non-mAb)\n",
    "    # -----------------------\n",
    "    if \"vaccine\" in t:\n",
    "        return G8\n",
    "    if \"recombinant fusion proteins\" in t:\n",
    "        return G8\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 5: Biologic antibodies (mono / bispecific / CAR / fusion)\n",
    "    # -----------------------\n",
    "    if (\n",
    "        \"antibod\" in t\n",
    "        or \"immunoconjugate\" in t\n",
    "        or \"chimeric antigen\" in t\n",
    "    ):\n",
    "        return G5\n",
    "    # Core monoclonal antibody/fusion protein MeSH branches\n",
    "    if tn.startswith(\"D12.776.124.486.485.114\"):  # Antibodies, Monoclonal*\n",
    "        return G5\n",
    "    if tn.startswith(\"D12.776.124.790.651.114\"):  # Therapeutic mAbs under Immunologic Factors\n",
    "        return G5\n",
    "    if tn.startswith(\"D12.776.828.300\"):  # Recombinant Fusion Proteins\n",
    "        return G5\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 1: Cytokine & hormone receptor modulators\n",
    "    # (EPO-R, TPO-R, IL-2R, glucocorticoid receptor, Ca-sensing receptor, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"receptors, erythropoietin\",\n",
    "        \"erythropoietin\",\n",
    "        \"receptors, thrombopoietin\",\n",
    "        \"thrombopoietin\",\n",
    "        \"receptors, interleukin-2\",\n",
    "        \"interleukin-2 receptor alpha subunit\",\n",
    "        \"receptors, glucocorticoid\",\n",
    "        \"receptors, calcium-sensing\",\n",
    "    ]):\n",
    "        return G1\n",
    "    # Hematopoietic / cytokine receptor MeSH branches seen in your table\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.150\"):  # EPO-R agonists\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.610\"):  # TPO-R agonists\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.705.852.420.320\"):  # IL-2R\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.826.750.430\"):  # Glucocorticoid receptor\n",
    "        return G1\n",
    "    if tn.startswith(\"D12.776.543.750.695.115\"):  # Ca-sensing receptor\n",
    "        return G1\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 2: Immune checkpoint & immune modulation\n",
    "    # (Immune checkpoint inhibitors, PD-1/PD-L1, TNF, CD47, IL-33, IL-1RA, lectins, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"immune checkpoint inhibitors\",\n",
    "        \"immune checkpoint inhibitor\",\n",
    "        \"immune checkpoint\",  # generic catch-all\n",
    "        \"programmed cell death 1 receptor\", \"pd-1\", \"pd1\",\n",
    "        \"pd-l1\", \"pdl1\",\n",
    "        \"tumor necrosis factor-alpha\",\n",
    "        \"tnf\",\n",
    "        \"tumor necrosis factor ligand superfamily member\",\n",
    "        \"cd47 antigen\",\n",
    "        \"lectins, c-type\",\n",
    "        \"interleukin-33\",\n",
    "        \"interleukin 1 receptor antagonist protein\",\n",
    "    ]):\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.543.750.705.222.875\"):  # PD-1 receptor\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.500.800\"):  # TNF-alpha\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.395.550.014\"):  # CD47 antigen\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.776.503.280\"):  # C-type lectins\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.750.720\"):  # TNF ligand superfamily member 15\n",
    "        return G2\n",
    "    if tn.startswith(\"D12.644.276.374.465.850\"):  # IL-33\n",
    "        return G2\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 3: Targeted pathway inhibitors (RTK / JAK-STAT / mTOR, VEGF, HER2)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"vascular endothelial growth factor a\",\n",
    "        \"receptor, erbb-2\",\n",
    "        \"erbb2\",\n",
    "        \"vegf\",\n",
    "        \"janus kinase inhibitors\",\n",
    "        \"jak inhibitor\",\n",
    "        \"mtor inhibitors\",\n",
    "        \"tor serine-threonine kinases\",\n",
    "    ]):\n",
    "        return G3\n",
    "    # VEGF-A branch\n",
    "    if tn.startswith(\"D12.644.276.100.800.200\"):\n",
    "        return G3\n",
    "    # HER2 / ErbB-2 receptor branch\n",
    "    if tn.startswith(\"D12.776.543.750.750.400.074.400\"):\n",
    "        return G3\n",
    "    # JAK / mTOR live under D27.505.519.* but we key by text above.\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 4: Classical cytotoxic chemotherapy (antimetabolite / alkylator / tubulin / topo)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"antimetabolites, antineoplastic\",\n",
    "        \"antineoplastic agents, alkylating\",\n",
    "        \"topoisomerase i inhibitors\",\n",
    "        \"topoisomerase ii inhibitors\",\n",
    "        \"vinca alkaloids\",\n",
    "        \"vinblastine\",\n",
    "        \"taxoids\",\n",
    "        \"paclitaxel\",\n",
    "        \"tubulin modulators\",\n",
    "    ]):\n",
    "        return G4\n",
    "    # Classical chemo branches\n",
    "    if tn.startswith(\"D27.505.519.186\"):  # antimetabolites, antineoplastic\n",
    "        return G4\n",
    "    if tn.startswith(\"D27.505.519.124\"):  # alkylating agents\n",
    "        return G4\n",
    "    if tn.startswith(\"D27.505.519.593.249.500\"):  # tubulin modulators\n",
    "        return G4\n",
    "    # You can add explicit topo/anthracycline branches here if you see them later.\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 6: Small-molecule immunomodulators & anti-inflammatories\n",
    "    # (PDE4 inhibitors, COX inhibitors, glucocorticoids as drugs, antimalarials)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"phosphodiesterase 4 inhibitors\",\n",
    "        \"cyclooxygenase inhibitors\",\n",
    "        \"histamine h1 antagonists\",\n",
    "        \"glucocorticoids* / metabolism; pharmacology\",\n",
    "        \"glucocorticoids\",  # as a drug class\n",
    "        \"antimalarials\",\n",
    "        \"immunosuppressive agents\",\n",
    "        \"calcineurin inhibitors\",\n",
    "    ]):\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.519.625.375.425.400\"):  # H1 antagonists\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.696.663.850.014.040.500.500\"):  # COX inhibitors\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.519.389.735.374\"):  # PDE4 inhibitors\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.696.477.656\"):  # Immunosuppressive Agents*\n",
    "        return G6\n",
    "    if tn.startswith(\"D27.505.954.122.250.100.085\"):  # Antimalarials\n",
    "        return G6\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 7: Metabolic pathway modulators\n",
    "    # (gluconeogenesis, metabolic enzymes, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"gluconeogenesis / drug effects\",\n",
    "        \"gluconeogenesis\",\n",
    "        \"biguanides\",\n",
    "        \"imp dehydrogenase\",\n",
    "        \"thymidylate synthase\",\n",
    "        \"thymidine phosphorylase\",\n",
    "    ]):\n",
    "        return G7\n",
    "    if tn.startswith(\"G02.111.158.500\"):  # Gluconeogenesis / drug effects*\n",
    "        return G7\n",
    "    if tn.startswith(\"D08.811.682.047.820.450\"):  # IMP dehydrogenase\n",
    "        return G7\n",
    "\n",
    "    # -----------------------\n",
    "    # GROUP 9: Supportive / adjunctive agents\n",
    "    # (anion exchange resins, leucovorin, antithrombins, etc.)\n",
    "    # -----------------------\n",
    "    if any(kw in t for kw in [\n",
    "        \"anion exchange resins\",\n",
    "        \"antithrombins\",\n",
    "        \"leucovorin\",\n",
    "    ]):\n",
    "        return G9\n",
    "    if tn.startswith(\"D27.720.470.420.050\"):  # Anion exchange resins\n",
    "        return G9\n",
    "    if tn.startswith(\"D27.505.519.389.745.800.449\"):  # Antithrombins / agonists\n",
    "        return G9\n",
    "    if \"leucovorin\" in t:\n",
    "        return G9\n",
    "\n",
    "    # -----------------------\n",
    "    # Fallbacks:\n",
    "    # - If D27.505.* and not otherwise classified → treat as metabolic/chemical other\n",
    "    # -----------------------\n",
    "    if tn.startswith(\"D27.505.\"):\n",
    "        return G7  # generic chemical/metabolic \"other\" rather than immuno\n",
    "\n",
    "    # Absolute default: call it supportive/other\n",
    "    return G9\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Load counts per (mesh_term, tree_number)\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Start from full input and add group label\n",
    "out_df = df[[\"mesh_term\", \"tree_number\", \"count\"]].copy()\n",
    "out_df[\"mechanism_super_group\"] = [\n",
    "    classify_super_group(m, tn) for m, tn in zip(out_df[\"mesh_term\"], out_df[\"tree_number\"])\n",
    "]\n",
    "\n",
    "# Optional: sort for readability\n",
    "out_df = out_df.sort_values([\"mechanism_super_group\", \"count\"], ascending=[True, False])\n",
    "\n",
    "# Save\n",
    "out_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved mechanism super-group mapping → {OUTPUT_PATH}\")\n",
    "print(out_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4f3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59 tree_number → super_group mappings\n",
      "Wrote final trial-level file with super-groups → cache/task_3/trial_mechanism_with_super_groups.csv\n",
      "| trial_hash                           | investigational_products                           | investigational_products_mechanism_mesh_terms                        | investigational_products_mechanism_tree_numbers                                 | investigational_products_mechanism_primary_tree_numbers   | investigational_products_mechanism_super_group   | investigational_products_alternative_names                                                                                                                                                                                                          | investigational_products_molecular_target   | investigational_products_mechanism                                         | investigational_products_tt_drug_id   | investigational_products_bmt_drug_id   | active_comparators   | active_comparators_mechanism_mesh_terms   | active_comparators_mechanism_tree_numbers   | active_comparators_mechanism_primary_tree_numbers   | active_comparators_mechanism_super_group   | active_comparators_alternative_names   | active_comparators_molecular_target   | active_comparators_mechanism   | active_comparators_tt_drug_id   | active_comparators_bmt_drug_id   | placebos   | placebos_alternative_names   | placebos_molecular_target   | placebos_mechanism   | standard_of_care   | standard_of_care_mechanism_mesh_terms   | standard_of_care_mechanism_tree_numbers   | standard_of_care_mechanism_primary_tree_numbers   | standard_of_care_mechanism_super_group   | standard_of_care_alternative_names   | standard_of_care_molecular_target   | standard_of_care_mechanism   | standard_of_care_tt_drug_id   | standard_of_care_bmt_drug_id   |\n",
      "|:-------------------------------------|:---------------------------------------------------|:---------------------------------------------------------------------|:--------------------------------------------------------------------------------|:----------------------------------------------------------|:-------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------|:---------------------------------------------------------------------------|:--------------------------------------|:---------------------------------------|:---------------------|:------------------------------------------|:--------------------------------------------|:----------------------------------------------------|:-------------------------------------------|:---------------------------------------|:--------------------------------------|:-------------------------------|:--------------------------------|:---------------------------------|:-----------|:-----------------------------|:----------------------------|:---------------------|:-------------------|:----------------------------------------|:------------------------------------------|:--------------------------------------------------|:-----------------------------------------|:-------------------------------------|:------------------------------------|:-----------------------------|:------------------------------|:-------------------------------|\n",
      "| tid_0541995757b10e613a42173d6b8ddc09 | ['cinacalcet hydrochloride']                       | ['Receptors, Calcium-Sensing / agonists']                            | [['D12.776.543.750.695.115']]                                                   | ['D12.776.543.750.695.115']                               | ['cytokine_hormone_receptor_modulators']         | [['cinacalcet HCl', 'cinacalcet', 'cinacalcet HCl, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet, Zhejiang Wansheng Pharmaceutical Co.', 'cinacalcet hydrochloride tablet']] | ['Calcium-sensing receptor (CaSR)']         | ['Calcimimetic; positive allosteric modulator of CaSR']                    | ['194454']                            | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                         | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                       | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0d6e9b2f3f57c17c0e93610e28853f0c | ['Xenopax']                                        | ['Interleukin-2 Receptor alpha Subunit* / metabolism; immunology']   | [['D12.776.543.750.705.852.420.320.500']]                                       | ['D12.776.543.750.705.852.420.320.500']                   | ['cytokine_hormone_receptor_modulators']         | [['daclizumab', 'daclizumab, Shanghai CP Guojian', 'transplantation MAb, Shanghai', 'recombinant anti-CD25 humanized monoclonal antibody injection']]                                                                                               | ['CD25 (IL2RA)']                            | ['Humanized monoclonal antibody; interleukin-2 receptor alpha antagonist'] | ['40098']                             | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                         | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                       | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0da20e863cfc5f3e369868462bff74e0 | ['recombinant erythropoiesis-stimulating protein'] | ['Receptors, Erythropoietin / agonists*']                            | [['D12.776.543.750.705.852.150.200', 'D12.776.543.750.750.400.200.340']]        | ['D12.776.543.750.705.852.150.200']                       | ['cytokine_hormone_receptor_modulators']         | [['NuPIAO', 'nupiao', 'rESP', 'recombinant erythropoietin stimulating protein', 'recombinant erythropoietin stimulating protein, 3SBio', 'recombinant erythropoiesis-stimulating protein injection (CHO cells)', 'SSS-06', 'SSS06', 'SSS 06']]      | ['Erythropoietin receptor (EPOR)']          | ['Erythropoietin receptor agonist']                                        | ['40640']                             | ['19694']                              | []                   | []                                        | []                                          | []                                                  | []                                         | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                       | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e698ee5065c49d23fcf57516957a273 | ['SB8']                                            | ['Vascular Endothelial Growth Factor A / antagonists & inhibitors*'] | [['D12.644.276.100.800.200', 'D12.776.467.100.800.200', 'D23.529.100.800.200']] | ['D12.644.276.100.800.200']                               | ['targeted_pathway_inhibitors']                  | [['SB 8', 'SB-8', 'Aybintio', 'Onbevzi', 'bevacizumab, Samsung Bioepis', '615', 'SB8 Anti-VEGF antibody']]                                                                                                                                          | ['VEGF-A']                                  | ['Anti-VEGF-A monoclonal antibody (angiogenesis inhibitor)']               | ['113881']                            | ['27338']                              | []                   | []                                        | []                                          | []                                                  | []                                         | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                       | []                                   | []                                  | []                           | []                            | []                             |\n",
      "| tid_0e8fa21079f928135dfc6164a15285f8 | ['SSS-17']                                         | ['Prolyl-Hydroxylase Inhibitors / pharmacology*']                    | [['D27.505.519.389.740']]                                                       | ['D27.505.519.389.740']                                   | ['metabolic_pathway_modulators']                 | [['SSS17', 'SSS 17', '[14C] SSS17', '[14C]-SSS17', '[14C]SSS17', 'HIF 117', 'HIF-117', 'HIF117', '[14C]HIF-117']]                                                                                                                                   | ['HIF']                                     | ['Hypoxia-inducible factor antagonist']                                    | ['130313']                            | ['']                                   | []                   | []                                        | []                                          | []                                                  | []                                         | []                                     | []                                    | []                             | []                              | []                               | []         | []                           | []                          | []                   | []                 | []                                      | []                                        | []                                                | []                                       | []                                   | []                                  | []                           | []                            | []                             |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Attach 9-bucket MOA super-group labels to each mechanism role at the trial level.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_3/trial_mechanism_super_group_mapping.csv\n",
    "    One row per (mesh_term, tree_number) with columns:\n",
    "        • mesh_term\n",
    "        • tree_number\n",
    "        • count\n",
    "        • mechanism_super_group\n",
    "\n",
    "- CSV: cache/task_3/trial_mechanism_mesh_mapping.csv\n",
    "    Trial-level table with, for each role:\n",
    "        • investigational_products_mechanism_mesh_terms\n",
    "        • investigational_products_mechanism_primary_tree_numbers\n",
    "        • investigational_products_mapped\n",
    "        • investigational_products_primary_tree_numbers\n",
    "        • active_comparators_mechanism_mesh_terms\n",
    "        • active_comparators_mechanism_primary_tree_numbers\n",
    "        • standard_of_care_mechanism_mesh_terms\n",
    "        • standard_of_care_mechanism_primary_tree_numbers\n",
    "\n",
    "Process:\n",
    "- Build a mapping: primary tree_number → mechanism_super_group\n",
    "  from trial_mechanism_super_group_mapping.csv.\n",
    "- For each trial:\n",
    "    • INVESTIGATIONAL PRODUCTS:\n",
    "        - Primary: use mechanism-based MeSH (mechanism_mesh_terms / mechanism_primary_tree_numbers).\n",
    "        - Fallback: if mechanism term is missing / '[none]' / empty, use\n",
    "          investigational product MeSH mapping (investigational_products_mapped /\n",
    "          investigational_products_primary_tree_numbers).\n",
    "        - Map the chosen tree_number to mechanism_super_group.\n",
    "    • ACTIVE COMPARATORS / STANDARD OF CARE:\n",
    "        - Use mechanism-based tree_numbers only (no fallback).\n",
    "- Insert three new list-valued columns:\n",
    "    • investigational_products_mechanism_super_group\n",
    "    • active_comparators_mechanism_super_group\n",
    "    • standard_of_care_mechanism_super_group\n",
    "  immediately to the right of their respective *_primary_tree_numbers columns.\n",
    "\n",
    "Outputs:\n",
    "- CSV: cache/task_3/trial_mechanism_with_super_groups.csv\n",
    "    Same as input trial_mechanism_mesh_mapping.csv plus the super-group columns.\n",
    "- Console preview of the first 5 rows (markdown).\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "\n",
    "MAP_PATH   = BASE_DIR / \"trial_mechanism_super_group_mapping.csv\"\n",
    "TRIALS_IN  = BASE_DIR / \"trial_mechanism_mesh_mapping.csv\"\n",
    "TRIALS_OUT = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# LOAD MAPPING: tree_number → super_group\n",
    "# ---------------------------------------------------\n",
    "\n",
    "map_df = pd.read_csv(MAP_PATH)\n",
    "\n",
    "# Normalize tree_number a bit (strip whitespace)\n",
    "map_df[\"tree_number\"] = map_df[\"tree_number\"].astype(str).str.strip()\n",
    "\n",
    "# If there are duplicates, we just keep the first (should all agree anyway)\n",
    "mapping = {}\n",
    "for _, row in map_df.iterrows():\n",
    "    tn = str(row[\"tree_number\"]).strip()\n",
    "    sg = row[\"mechanism_super_group\"]\n",
    "    if tn and tn not in mapping:\n",
    "        mapping[tn] = sg\n",
    "\n",
    "print(f\"Loaded {len(mapping):,} tree_number → super_group mappings\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse list-like strings (e.g. \"['a','b']\") into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        if isinstance(v, list):\n",
    "            return v\n",
    "    except Exception:\n",
    "        return []\n",
    "    # If it's a single scalar, wrap in list\n",
    "    return [s]\n",
    "\n",
    "\n",
    "def build_super_group_list(mech_terms, mech_trees):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      mech_terms : list of MeSH headings (unused except for length)\n",
    "      mech_trees : list of primary tree numbers (strings)\n",
    "    Return:\n",
    "      list of mechanism_super_group strings (same length).\n",
    "    \"\"\"\n",
    "    # Ensure lists\n",
    "    mech_trees = mech_trees or []\n",
    "    mech_terms = mech_terms or []\n",
    "\n",
    "    n = max(len(mech_terms), len(mech_trees))\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        tn = (mech_trees[i] if i < len(mech_trees) else \"\") or \"\"\n",
    "        tn = tn.strip()\n",
    "        if not tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        out.append(mapping.get(tn, \"\"))  # \"\" if not found\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_super_group_list_with_fallback(\n",
    "    mech_terms,\n",
    "    mech_trees,\n",
    "    fallback_terms,\n",
    "    fallback_trees,\n",
    "):\n",
    "    \"\"\"\n",
    "    For investigational products:\n",
    "    - First try mechanism-based MeSH mapping (mech_terms/mech_trees).\n",
    "    - If the mechanism term is missing / '[none]' / empty, fall back to\n",
    "      the investigational product MeSH mapping (fallback_terms/fallback_trees).\n",
    "\n",
    "    Mapping itself is done ONLY on tree_number.\n",
    "    \"\"\"\n",
    "    # Ensure all are lists\n",
    "    mech_terms     = mech_terms or []\n",
    "    mech_trees     = mech_trees or []\n",
    "    fallback_terms = fallback_terms or []\n",
    "    fallback_trees = fallback_trees or []\n",
    "\n",
    "    n = max(len(mech_terms), len(mech_trees), len(fallback_terms), len(fallback_trees))\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Primary (mechanism-based)\n",
    "        term_mech = (mech_terms[i] if i < len(mech_terms) else \"\") or \"\"\n",
    "        tn_mech   = (mech_trees[i] if i < len(mech_trees) else \"\") or \"\"\n",
    "\n",
    "        term_mech = term_mech.strip()\n",
    "        tn_mech   = tn_mech.strip()\n",
    "\n",
    "        # Fallback (drug-based)\n",
    "        term_fb = (fallback_terms[i] if i < len(fallback_terms) else \"\") or \"\"\n",
    "        tn_fb   = (fallback_trees[i] if i < len(fallback_trees) else \"\") or \"\"\n",
    "\n",
    "        term_fb = term_fb.strip()\n",
    "        tn_fb   = tn_fb.strip()\n",
    "\n",
    "        # Decide which tree number to use\n",
    "        chosen_tn = \"\"\n",
    "        # 1) Use mechanism term if present and not [none]\n",
    "        if term_mech and term_mech != \"[none]\" and tn_mech:\n",
    "            chosen_tn = tn_mech\n",
    "        # 2) Else fall back to investigational product MeSH term\n",
    "        elif term_fb and term_fb != \"[none]\" and tn_fb:\n",
    "            chosen_tn = tn_fb\n",
    "\n",
    "        chosen_tn = chosen_tn.strip()\n",
    "        if not chosen_tn:\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "\n",
    "        out.append(mapping.get(chosen_tn, \"\"))  # \"\" if no mapping\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def insert_after(df, col, newcol, values):\n",
    "    \"\"\"Insert a new column immediately after `col`.\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    idx = cols.index(col)\n",
    "    df.insert(idx + 1, newcol, values)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# RUN\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Load trial-level mechanism mapping\n",
    "df = pd.read_csv(TRIALS_IN)\n",
    "\n",
    "inv_sg_list = []\n",
    "ac_sg_list  = []\n",
    "soc_sg_list = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    # -----------------------------\n",
    "    # INVESTIGATIONAL PRODUCTS\n",
    "    # -----------------------------\n",
    "    # Mechanism-based mapping\n",
    "    inv_mech_terms = parse_listish(row.get(\"investigational_products_mechanism_mesh_terms\"))\n",
    "    inv_mech_tn    = parse_listish(row.get(\"investigational_products_mechanism_primary_tree_numbers\"))\n",
    "\n",
    "    # Fallback: investigational product MeSH mapping\n",
    "    inv_prod_terms = parse_listish(row.get(\"investigational_products_mapped\"))\n",
    "    inv_prod_tn    = parse_listish(row.get(\"investigational_products_primary_tree_numbers\"))\n",
    "\n",
    "    inv_sg_list.append(\n",
    "        build_super_group_list_with_fallback(\n",
    "            inv_mech_terms,\n",
    "            inv_mech_tn,\n",
    "            inv_prod_terms,\n",
    "            inv_prod_tn,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # ACTIVE COMPARATORS\n",
    "    # (no fallback requested)\n",
    "    # -----------------------------\n",
    "    ac_terms = parse_listish(row.get(\"active_comparators_mechanism_mesh_terms\"))\n",
    "    ac_tn    = parse_listish(row.get(\"active_comparators_mechanism_primary_tree_numbers\"))\n",
    "    ac_sg_list.append(build_super_group_list(ac_terms, ac_tn))\n",
    "\n",
    "    # -----------------------------\n",
    "    # STANDARD OF CARE\n",
    "    # (no fallback requested)\n",
    "    # -----------------------------\n",
    "    soc_terms = parse_listish(row.get(\"standard_of_care_mechanism_mesh_terms\"))\n",
    "    soc_tn    = parse_listish(row.get(\"standard_of_care_mechanism_primary_tree_numbers\"))\n",
    "    soc_sg_list.append(build_super_group_list(soc_terms, soc_tn))\n",
    "\n",
    "\n",
    "# Insert new columns next to the primary_tree_numbers\n",
    "insert_after(\n",
    "    df,\n",
    "    \"investigational_products_mechanism_primary_tree_numbers\",\n",
    "    \"investigational_products_mechanism_super_group\",\n",
    "    inv_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"active_comparators_mechanism_primary_tree_numbers\",\n",
    "    \"active_comparators_mechanism_super_group\",\n",
    "    ac_sg_list,\n",
    ")\n",
    "\n",
    "insert_after(\n",
    "    df,\n",
    "    \"standard_of_care_mechanism_primary_tree_numbers\",\n",
    "    \"standard_of_care_mechanism_super_group\",\n",
    "    soc_sg_list,\n",
    ")\n",
    "\n",
    "# Save final CSV\n",
    "df.to_csv(TRIALS_OUT, index=False)\n",
    "print(f\"Wrote final trial-level file with super-groups → {TRIALS_OUT}\")\n",
    "print(df.head(5).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71d17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved distribution → cache/task_3/trial_super_group_distribution.csv\n",
      "Top categories:\n",
      "\n",
      "| mechanism_super_group                     |   count |\n",
      "|:------------------------------------------|--------:|\n",
      "| supportive_adjunctive_agents              |      53 |\n",
      "| cytokine_hormone_receptor_modulators      |      39 |\n",
      "| immune_checkpoint_immune_modulation       |      32 |\n",
      "| targeted_pathway_inhibitors               |      20 |\n",
      "| metabolic_pathway_modulators              |      15 |\n",
      "| biologic_antibodies_biologics             |      10 |\n",
      "| small_molecule_immunomod_antiinflammatory |       7 |\n",
      "| classical_cytotoxic_chemotherapy          |       5 |\n",
      "| vaccines_immune_biologics                 |       2 |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Derive a single 9-bucket MOA super-group label per trial and summarize distribution.\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_3/trial_mechanism_with_super_groups.csv\n",
    "    Trial-level table with list-valued columns:\n",
    "        • investigational_products_mechanism_super_group\n",
    "        • active_comparators_mechanism_super_group\n",
    "        • standard_of_care_mechanism_super_group\n",
    "\n",
    "Process:\n",
    "- For each trial:\n",
    "    1. Parse the list-valued super-group columns from strings into Python lists.\n",
    "    2. Choose a single trial-level super-group with the following priority:\n",
    "         • Priority 1: first non-empty investigational_products_mechanism_super_group\n",
    "         • Priority 2: if none, first non-empty active_comparators_mechanism_super_group\n",
    "         • Priority 3: if none, first non-empty standard_of_care_mechanism_super_group\n",
    "    3. Attach the chosen label in a new column:\n",
    "         • trial_mechanism_super_group\n",
    "- Aggregate a distribution (count per mechanism_super_group), excluding empty labels.\n",
    "\n",
    "Outputs:\n",
    "- CSV: cache/task_3/trial_super_group_distribution.csv\n",
    "    Columns:\n",
    "        • mechanism_super_group\n",
    "        • count\n",
    "- Console preview of the top categories (markdown).\n",
    "\"\"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = Path(\"cache/task_3\")\n",
    "INPUT_PATH  = BASE_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "OUTPUT_PATH = BASE_DIR / \"trial_super_group_distribution.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# HELPERS\n",
    "# -------------------------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse list-like strings safely into Python lists.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def first_non_empty_str(lst):\n",
    "    \"\"\"Return the first non-empty string from a list, or '' if none.\"\"\"\n",
    "    if not isinstance(lst, list):\n",
    "        return \"\"\n",
    "    for v in lst:\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip()\n",
    "    return \"\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# RUN\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "chosen_super_groups = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inv_sg_list = parse_listish(row.get(\"investigational_products_mechanism_super_group\", \"[]\"))\n",
    "    ac_sg_list  = parse_listish(row.get(\"active_comparators_mechanism_super_group\", \"[]\"))\n",
    "    soc_sg_list = parse_listish(row.get(\"standard_of_care_mechanism_super_group\", \"[]\"))\n",
    "\n",
    "    # Priority 1 — investigational product super-group\n",
    "    chosen = first_non_empty_str(inv_sg_list)\n",
    "\n",
    "    # Priority 2 — active comparator super-group\n",
    "    if not chosen:\n",
    "        chosen = first_non_empty_str(ac_sg_list)\n",
    "\n",
    "    # Priority 3 — fallback to SOC\n",
    "    if not chosen:\n",
    "        chosen = first_non_empty_str(soc_sg_list)\n",
    "\n",
    "    chosen_super_groups.append(chosen)\n",
    "\n",
    "# Add per-trial chosen super-group (optional but useful)\n",
    "df[\"trial_mechanism_super_group\"] = chosen_super_groups\n",
    "\n",
    "# Count distribution (exclude empty)\n",
    "dist = Counter(sg for sg in chosen_super_groups if sg)\n",
    "\n",
    "dist_df = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            {\"mechanism_super_group\": sg, \"count\": count}\n",
    "            for sg, count in dist.items()\n",
    "        ]\n",
    "    )\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Save distribution\n",
    "dist_df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved distribution → {OUTPUT_PATH}\")\n",
    "print(\"Top categories:\\n\")\n",
    "print(dist_df.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deae989",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e05ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results table → output/trial_results_table.csv\n",
      "| trial_title                                                                                                                                                                                                                                 | drug_name                                         | moa                                  | innovation_generic_biosimilar   | category                             |\n",
      "|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------|:-------------------------------------|:--------------------------------|:-------------------------------------|\n",
      "| Random, single oral administration, double cycle, double cross, bioequivalence test of Cinacalcet Hydrochloride Tablets in healthy subjects under fasting and postprandial state                                                            | cinacalcet hydrochloride                          | Receptors, Calcium-Sensing           | Generic                         | cytokine_hormone_receptor_modulators |\n",
      "| Study of Xenopax for the treatment of rejection in kidney transplant.                                                                                                                                                                       | Xenopax                                           | Interleukin-2 Receptor alpha Subunit | Biosimilar                      | cytokine_hormone_receptor_modulators |\n",
      "| A single intravenous administration of recombinant erythropoiesis-stimulating protein injection (CHO cells) to explore the tolerability, safety and pharmacokinetic characteristics of healthy subjects                                     | recombinant erythropoiesis-stimulating protein    | Receptors, Erythropoietin            | Innovative                      | cytokine_hormone_receptor_modulators |\n",
      "| A Pivotal Study of SB8 Anti-VEGF Antibody in Patients with Metastatic Colon Cancer and Non-Small Cell Lung Cancer.                                                                                                                          | SB8                                               | Vascular Endothelial Growth Factor A | Biosimilar                      | targeted_pathway_inhibitors          |\n",
      "| A Mass Balance Study of [14C]SSS17 in Healthy Chinese Male Subjects                                                                                                                                                                         | SSS-17                                            | Prolyl-Hydroxylase Inhibitors        | Innovative                      | metabolic_pathway_modulators         |\n",
      "| A randomized, open label, crossover, bioequivalence, bioavailability, safety study of metformin hydrochloride sustained-release tablets in healthy volunteers under fasting and postprandial administration                                 | Metformin hydrochloride sustained-release tablets | Gluconeogenesis                      | Generic                         | metabolic_pathway_modulators         |\n",
      "| A Phase III study of RD001 for the treatment of Anemia                                                                                                                                                                                      | RD-01                                             | Receptors, Erythropoietin            | Innovative                      | cytokine_hormone_receptor_modulators |\n",
      "| A Single-center, Single-arm, Open-label, Single-dose Phase I Clinical Study to Evaluate Pharmacokinetics, Pharmacodynamics, Safety and Tolerability of rhTPO in Healthy Caucasian Participants. Approximately 22 Subjects Will be Enrolled. | Recombinant human thrombopoietin                  | Receptors, Thrombopoietin            | Innovative                      | cytokine_hormone_receptor_modulators |\n",
      "| A Phase II Study to Evaluate the Safety and Efficacy of SSGJ-705 Monotherapy and Combination Therapy in Patients With Advanced HER2-Expressing Solid Tumors                                                                                 | SSGJ-705                                          | Antibodies, Bispecific               | Innovative                      | biologic_antibodies_biologics        |\n",
      "| A Phase III Pivotal Trial to Evaluate 602 in Patients with Colorectal Cancer                                                                                                                                                                | 602                                               | ErbB Receptors                       | Innovative                      | supportive_adjunctive_agents         |\n",
      "| A phase II/III clinical trial of 601A in Age-Related Macular Degeneration.                                                                                                                                                                  | 601A                                              | Vascular Endothelial Growth Factor A | Innovative                      | targeted_pathway_inhibitors          |\n",
      "| A Phase I, Multicenter, Open-Label, First-in-Human Clinical Trial to Evaluate the Safety, Tolerability, Pharmacokinetics and Potential Anti-tumor Effects of SSGJ-705 in Patients With Advanced or Metastatic HER2-expressing Solid Tumors  | SSGJ-705                                          | Antibodies, Bispecific               | Innovative                      | biologic_antibodies_biologics        |\n",
      "| A Multicenter, Randomized, Double-blind, Placebo-controlled Phase II, Efficacy and Safety Study of Recombinant Anti-IL-5 Humanized Monoclonal Antibody Therapy in Adult Subjects With Severe Eosinophilic Asthma                            | SSGJ-610                                          | Interleukin-5                        | Innovative                      | supportive_adjunctive_agents         |\n",
      "| A Multicenter, Single-Arm, Open Label Study Evaluating the Efficacy and Safety of Maintenance Treatment With Recombinant Human Thrombopoietin in Thrombocytopenic Subjects With Immune Thrombocytopenia                                     | recombinant human thrombopoietin                  | Receptors, Thrombopoietin            | Innovative                      | cytokine_hormone_receptor_modulators |\n",
      "| A Multicenter, Randomized, Double-blind, Placebo-controlled Phase IIb, Effcacy and Safety Study of Recombinant Anti-IL-5 Humanized Monoclonal Antibody Therapy in Adult Subjects With Severe Asthma                                         | SSGJ-610                                          | Interleukin-5                        | Innovative                      | supportive_adjunctive_agents         |\n",
      "| A Phase II Study of SSGJ-707 Monotherapy in First-line PD-L1 Positive Advanced NSCLC Patients                                                                                                                                               | SSGJ-707                                          | Programmed Cell Death 1 Receptor     | Innovative                      | immune_checkpoint_immune_modulation  |\n",
      "| Study on the tolerability, pharmacokinetics and pharmacodynamics of a single subcutaneous injection of recombinant human thrombopoietic factor injection (rh-TPO) in healthy adults                                                         | Recombinant human thrombopoietin                  | Receptors, Thrombopoietin            | Biosimilar                      | cytokine_hormone_receptor_modulators |\n",
      "| rESP Medication With a  to Explore the Tolerability ,Safety and Pharmacokinetic Characteristics for Healthy Subjects in the Phase I Clinical Study                                                                                          | Recombinant erythropoietin stimulating protein    | Receptors, Erythropoietin            | Innovative                      | cytokine_hormone_receptor_modulators |\n",
      "| A Phase IV study of Yisaipu for rheumatoid arthritis                                                                                                                                                                                        | Yisaipu                                           | Tumor Necrosis Factor Inhibitors     | Biosimilar                      | metabolic_pathway_modulators         |\n",
      "| Effect of Food on the Pharmacokinetics of SSS17 Capsules: A Study in Chinese Healthy Subjects                                                                                                                                               | SSS-17                                            | Prolyl-Hydroxylase Inhibitors        | Innovative                      | metabolic_pathway_modulators         |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build a final, human-readable trial results table summarizing:\n",
    "- Trial title\n",
    "- Combined investigational (or SOC) drug name(s)\n",
    "- Cleaned mechanism-of-action (MeSH-based, lightly normalized)\n",
    "- Innovation status (Innovative / Generic / Biosimilar)\n",
    "- High-level MOA super-group category\n",
    "\n",
    "Inputs:\n",
    "- CSV: cache/task_2/trial_investigational_drugs_classifications.csv\n",
    "    • trial_hash\n",
    "    • investigational_products                  (list-like)\n",
    "    • investigational_products_classifications  (list-like; Innovative/Generic/Biosimilar/etc.)\n",
    "\n",
    "- CSV: cache/task_3/trial_mechanism_with_super_groups.csv\n",
    "    • trial_hash\n",
    "    • investigational_products, standard_of_care (list-like drug names)\n",
    "    • *_mechanism_mesh_terms, *_mechanism_super_group (list-like MeSH MOA & 9-bucket super-groups)\n",
    "\n",
    "- CSV: cache/data_preprocess/raw_trials_with_hash.csv\n",
    "    • trial_hash\n",
    "    • title  (main trial title)\n",
    "\n",
    "Process:\n",
    "1. From the classification table:\n",
    "   - Parse list-like investigational products & classifications.\n",
    "   - Join product names and innovation flags with '+' per trial.\n",
    "2. From the mechanism table:\n",
    "   - Build per-trial '+'-joined strings for:\n",
    "       • inv_drug_name_joined\n",
    "       • inv_moa_joined\n",
    "       • inv_category_joined (super-groups, unique)\n",
    "       • soc_drug_name_joined\n",
    "       • soc_moa_joined\n",
    "       • soc_category_joined\n",
    "3. Merge mechanism info with classification info on trial_hash.\n",
    "4. For each trial, choose a single row:\n",
    "   - If investigational product(s) exist:\n",
    "       • drug_name  = inv_drug_name_joined\n",
    "       • moa        = inv_moa_joined\n",
    "       • innovation = innovation_joined\n",
    "       • category   = inv_category_joined (fallback to SOC category if missing)\n",
    "   - Else if only SOC exists:\n",
    "       • drug_name  = soc_drug_name_joined\n",
    "       • moa        = soc_moa_joined\n",
    "       • category   = soc_category_joined\n",
    "       • innovation = \"Generic\"\n",
    "   - Else: leave fields empty.\n",
    "5. Attach trial_title from raw_trials_with_hash; if missing, fall back to trial_hash.\n",
    "6. Clean up:\n",
    "   - Remove parenthetical text from drug_name (e.g. \"Drug X (Company Y)\" → \"Drug X\").\n",
    "   - Normalize MOA:\n",
    "       • split on '+'\n",
    "       • drop text after '/' for each component\n",
    "       • remove '*' and trim\n",
    "       • rejoin with '+'\n",
    "   - Normalize innovation flags to canonical title-case\n",
    "     (Innovative, Generic, Biosimilar), preserving '+' separators.\n",
    "\n",
    "Outputs:\n",
    "- CSV: output/trial_results_table.csv\n",
    "    Columns:\n",
    "        trial_title\n",
    "        drug_name\n",
    "        moa\n",
    "        innovation_generic_biosimilar\n",
    "        category\n",
    "- Console preview of the first 20 rows (markdown).\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------\n",
    "\n",
    "import ast\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TASK2_DIR = Path(\"cache/task_2\")\n",
    "TASK3_DIR = Path(\"cache/task_3\")\n",
    "DATA_DIR  = Path(\"cache/data_preprocess\")\n",
    "OUT_DIR   = Path(\"output\")\n",
    "\n",
    "CLASS_PATH  = TASK2_DIR / \"trial_investigational_drugs_classifications.csv\"\n",
    "MECH_PATH   = TASK3_DIR / \"trial_mechanism_with_super_groups.csv\"\n",
    "TRIALS_PATH = DATA_DIR / \"raw_trials_with_hash.csv\"\n",
    "OUTPUT_PATH = OUT_DIR / \"trial_results_table.csv\"\n",
    "\n",
    "# ---------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------\n",
    "\n",
    "def parse_listish(x):\n",
    "    \"\"\"Parse a list-like string into a Python list.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        v = ast.literal_eval(s)\n",
    "        return v if isinstance(v, list) else []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def join_plus(lst):\n",
    "    \"\"\"Join non-empty strings with '+'.\"\"\"\n",
    "    cleaned = [str(x).strip() for x in lst if str(x).strip()]\n",
    "    return \"+\".join(cleaned)\n",
    "\n",
    "def join_unique_plus(lst):\n",
    "    \"\"\"Join unique, non-empty strings with '+' (order-preserving).\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in lst:\n",
    "        s = str(x).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s)\n",
    "            out.append(s)\n",
    "    return \"+\".join(out)\n",
    "\n",
    "def normalize_innovation(val: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize to canonical title-case:\n",
    "      - Innovative\n",
    "      - Generic\n",
    "      - Biosimilar\n",
    "    Anything else is title-cased as a fallback.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    v = str(val).strip()\n",
    "    if not v:\n",
    "        return \"\"\n",
    "    low = v.lower()\n",
    "    if \"innov\" in low:\n",
    "        return \"Innovative\"\n",
    "    if \"bio\" in low:\n",
    "        return \"Biosimilar\"\n",
    "    if \"gener\" in low:\n",
    "        return \"Generic\"\n",
    "    # Fallback: just title-case whatever it is.\n",
    "    return v.title()\n",
    "\n",
    "def strip_parentheses(s: str) -> str:\n",
    "    \"\"\"Remove all parenthetical segments from a drug name.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return re.sub(r\"\\s*\\([^)]*\\)\", \"\", s).strip()\n",
    "\n",
    "def clean_moa(moa: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean MOA string:\n",
    "      - split by '+'\n",
    "      - for each piece, drop everything after '/'\n",
    "      - remove '*'\n",
    "      - strip whitespace\n",
    "      - rejoin by '+'\n",
    "    \"\"\"\n",
    "    if not isinstance(moa, str):\n",
    "        return \"\"\n",
    "    parts = moa.split(\"+\")\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        s = part.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        # keep only text before first '/'\n",
    "        if \"/\" in s:\n",
    "            s = s.split(\"/\", 1)[0]\n",
    "        # remove '*' and strip again\n",
    "        s = s.replace(\"*\", \"\").strip()\n",
    "        if s:\n",
    "            cleaned_parts.append(s)\n",
    "    return \"+\".join(cleaned_parts)\n",
    "\n",
    "# ---------------------------------\n",
    "# RUN\n",
    "# ---------------------------------\n",
    "\n",
    "# Load inputs\n",
    "df_class  = pd.read_csv(CLASS_PATH)\n",
    "df_mech   = pd.read_csv(MECH_PATH)\n",
    "df_titles = pd.read_csv(TRIALS_PATH, dtype=str)[[\"trial_hash\", \"title\"]]\n",
    "\n",
    "# Prepare classification info\n",
    "df_class[\"drug_name_list\"] = df_class[\"investigational_products\"].apply(parse_listish)\n",
    "df_class[\"innovation_list\"] = df_class[\"investigational_products_classifications\"].apply(parse_listish)\n",
    "\n",
    "df_class[\"drug_name_joined\"] = df_class[\"drug_name_list\"].apply(join_plus)\n",
    "df_class[\"innovation_joined\"] = df_class[\"innovation_list\"].apply(\n",
    "    lambda lst: join_plus([normalize_innovation(v) for v in lst])\n",
    ")\n",
    "\n",
    "df_class_slim = df_class[[\"trial_hash\", \"drug_name_joined\", \"innovation_joined\"]].copy()\n",
    "\n",
    "# Prepare mechanism / category info\n",
    "df_mech[\"inv_drug_name_joined\"] = df_mech[\"investigational_products\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"inv_moa_joined\"] = df_mech[\"investigational_products_mechanism_mesh_terms\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"inv_category_joined\"] = df_mech[\"investigational_products_mechanism_super_group\"].apply(\n",
    "    lambda x: join_unique_plus(parse_listish(x))\n",
    ")\n",
    "\n",
    "df_mech[\"soc_drug_name_joined\"] = df_mech[\"standard_of_care\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"soc_moa_joined\"] = df_mech[\"standard_of_care_mechanism_mesh_terms\"].apply(\n",
    "    lambda x: join_plus(parse_listish(x))\n",
    ")\n",
    "df_mech[\"soc_category_joined\"] = df_mech[\"standard_of_care_mechanism_super_group\"].apply(\n",
    "    lambda x: join_unique_plus(parse_listish(x))\n",
    ")\n",
    "\n",
    "df_mech_slim = df_mech[\n",
    "    [\n",
    "        \"trial_hash\",\n",
    "        \"inv_drug_name_joined\",\n",
    "        \"inv_moa_joined\",\n",
    "        \"inv_category_joined\",\n",
    "        \"soc_drug_name_joined\",\n",
    "        \"soc_moa_joined\",\n",
    "        \"soc_category_joined\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# Merge on trial_hash\n",
    "merged = pd.merge(\n",
    "    df_mech_slim,\n",
    "    df_class_slim,\n",
    "    on=\"trial_hash\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Build final table\n",
    "def build_final_row(row):\n",
    "    inv_drug = (row.get(\"inv_drug_name_joined\") or \"\").strip()\n",
    "    soc_drug = (row.get(\"soc_drug_name_joined\") or \"\").strip()\n",
    "\n",
    "    if inv_drug:\n",
    "        drug_name  = inv_drug\n",
    "        moa        = (row.get(\"inv_moa_joined\") or \"\").strip()\n",
    "        innovation = (row.get(\"innovation_joined\") or \"\").strip()\n",
    "        category   = (row.get(\"inv_category_joined\") or \"\").strip()\n",
    "        if not category:\n",
    "            category = (row.get(\"soc_category_joined\") or \"\").strip()\n",
    "\n",
    "    elif soc_drug:\n",
    "        drug_name  = soc_drug\n",
    "        moa        = (row.get(\"soc_moa_joined\") or \"\").strip()\n",
    "        category   = (row.get(\"soc_category_joined\") or \"\").strip()\n",
    "        # Standard-of-care only: treat as Generic (canonical title-case)\n",
    "        innovation = \"Generic\"\n",
    "\n",
    "    else:\n",
    "        drug_name = \"\"\n",
    "        moa = \"\"\n",
    "        innovation = \"\"\n",
    "        category = \"\"\n",
    "\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"drug_name\": drug_name,\n",
    "            \"moa\": moa,\n",
    "            \"innovation_generic_biosimilar\": innovation,\n",
    "            \"category\": category,\n",
    "        }\n",
    "    )\n",
    "\n",
    "final_cols = merged.apply(build_final_row, axis=1)\n",
    "final = pd.concat([merged[[\"trial_hash\"]], final_cols], axis=1)\n",
    "\n",
    "# Attach titles\n",
    "final = final.merge(df_titles, on=\"trial_hash\", how=\"left\")\n",
    "final[\"trial_title\"] = final[\"title\"].fillna(final[\"trial_hash\"])\n",
    "final.drop(columns=[\"title\"], inplace=True)\n",
    "\n",
    "# REMOVE PARENTHETICAL TEXT FROM drug_name\n",
    "final[\"drug_name\"] = final[\"drug_name\"].apply(strip_parentheses)\n",
    "\n",
    "# CLEAN MOA FIELD\n",
    "final[\"moa\"] = final[\"moa\"].apply(clean_moa)\n",
    "\n",
    "# Ensure innovation is always canonical (in case anything slipped through)\n",
    "final[\"innovation_generic_biosimilar\"] = final[\"innovation_generic_biosimilar\"].apply(\n",
    "    lambda v: \"+\".join(\n",
    "        normalize_innovation(part)\n",
    "        for part in str(v).split(\"+\")\n",
    "        if str(part).strip()\n",
    "    ) if pd.notna(v) and str(v).strip() else \"\"\n",
    ")\n",
    "\n",
    "results = final[\n",
    "    [\"trial_title\", \"drug_name\", \"moa\", \"innovation_generic_biosimilar\", \"category\"]\n",
    "].copy()\n",
    "\n",
    "# Save\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved results table → {OUTPUT_PATH}\")\n",
    "print(results.head(20).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db1a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 rows with at least one missing value.\n",
      "\n",
      "Rows with missing values:\n",
      "\n",
      "| trial_title                                                 | drug_name   |   moa | innovation_generic_biosimilar   |   category |\n",
      "|:------------------------------------------------------------|:------------|------:|:--------------------------------|-----------:|\n",
      "| A Phase I Study of SSS24 in Patients With Colorectal Cancer | SSS-24      |   nan | Innovative                      |        nan |\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inspect the final trial results table for rows with missing key fields.\n",
    "\n",
    "Inputs:\n",
    "- CSV: output/trial_results_table.csv\n",
    "    Columns (at minimum):\n",
    "        • trial_title\n",
    "        • drug_name\n",
    "        • moa\n",
    "        • innovation_generic_biosimilar\n",
    "        • category\n",
    "\n",
    "Process:\n",
    "- Load the final trial results table.\n",
    "- Define a helper to treat NaN and empty strings (\"\") as missing.\n",
    "- Identify rows where ANY of the key columns are missing.\n",
    "- Print:\n",
    "    • The total number of rows with at least one missing value.\n",
    "    • A markdown preview of all such rows.\n",
    "\n",
    "Outputs:\n",
    "- Console-only diagnostics; no files are written.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = Path(\"output\")\n",
    "INPUT_PATH = OUT_DIR / \"trial_results_table.csv\"\n",
    "\n",
    "# Columns to inspect for missing values\n",
    "COLS_TO_CHECK = [\n",
    "    \"trial_title\",\n",
    "    \"drug_name\",\n",
    "    \"moa\",\n",
    "    \"innovation_generic_biosimilar\",\n",
    "    \"category\",\n",
    "]\n",
    "\n",
    "# ---------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------\n",
    "\n",
    "def is_missing(x):\n",
    "    \"\"\"Return True if value is NaN or an empty string (after stripping).\"\"\"\n",
    "    return pd.isna(x) or (str(x).strip() == \"\")\n",
    "\n",
    "# ---------------------------------\n",
    "# RUN\n",
    "# ---------------------------------\n",
    "\n",
    "# Load results table\n",
    "results = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "# Ensure all required columns exist\n",
    "missing_cols = [c for c in COLS_TO_CHECK if c not in results.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing expected columns in input: {missing_cols}\")\n",
    "\n",
    "# Treat \"\" as missing for easier filtering\n",
    "mask_missing = results[COLS_TO_CHECK].map(is_missing).any(axis=1)\n",
    "\n",
    "missing_rows = results[mask_missing].copy()\n",
    "\n",
    "print(f\"Found {len(missing_rows)} rows with at least one missing value.\\n\")\n",
    "\n",
    "if len(missing_rows) > 0:\n",
    "    print(\"Rows with missing values:\\n\")\n",
    "    print(missing_rows.to_markdown(index=False))\n",
    "else:\n",
    "    print(\"No rows with missing values in the selected columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036c24e",
   "metadata": {},
   "source": [
    "discovered for two drugs \"601\" and \"Inetetamab\" citline mapped them to the wrong drug / drug_id\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "That is inotuzumab ozogamicin (Besponsa):\n",
    "- Target: CD22\n",
    "- Indication: B-cell ALL, etc.\n",
    "- Mechanism: antibody–drug conjugate / DNA damaging.\n",
    "\n",
    "This has nothing to do with:\n",
    "- HER2\n",
    "- breast cancer neoadjuvant\n",
    "- Inetetamab / Inituzumab / Ceputin\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "AER-601 (Aerami / Dance Biopharm GLP-1 analogue)\n",
    "- Target: GLP-1 receptor\n",
    "- Indications: Type 2 diabetes, obesity, appetite/weight control\n",
    "- Mechanism: GLP-1 receptor agonist, incretin mimetic, insulin secretagogue\n",
    "\n",
    "This has nothing to do with:\n",
    "- VEGF-A or VEGF receptors\n",
    "- Intravitreal ophthalmic anti-VEGF biologics\n",
    "- Pathological myopic choroidal neovascularization (pmCNV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
